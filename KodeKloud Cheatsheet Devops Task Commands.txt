Task 1 - Task 36 : Kodekloud Cheatsheat SysAdmin Task Commands.txt
--------------------------------------------------------------------------------------------------------------------------------------------
Task 37 : 14/Feb/2022
Create Replicaset in Kubernetes Cluster

The Nautilus DevOps team is going to deploy some applications on kubernetes cluster as they are planning to migrate some of their existing applications there. Recently one of the team members has been assigned a task to write a template as per details mentioned below:

    Create a ReplicaSet using httpd image with latest tag only and remember to mention tag i.e httpd:latest and name it as httpd-replicaset.

    Labels app should be httpd_app, labels type should be front-end. The container should be named as httpd-container; also make sure replicas counts are 4.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

		1.
		thor@jump_host ~$ kubectl get deploy
			No resources found in default namespace.
		 
		2.
		thor@jump_host ~$ kubectl get services
			NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
			kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   57m
		
		3. 
		thor@jump_host ~$ vi /tmp/httpd.yaml
			apiVersion: apps/v1
			kind: ReplicaSet
			metadata:
			  name: httpd-replicaset
			  labels:
			    app: httpd_app
			    type: front-end
			spec:
			  replicas: 4
			  selector:
			    matchLabels:
			      app: httpd_app
			  template:
			    metadata:
			      labels:
			        app: httpd_app
			        type: front-end
			    spec:
			      containers:
			        - name: httpd-container
			          image: httpd:latest

		4.	          
		thor@jump_host ~$ cat /tmp/httpd.yaml
			apiVersion: apps/v1
				kind: ReplicaSet
				metadata:
				  name: httpd-replicaset
				  labels:
				    app: httpd_app
				    type: front-end
				spec:
				  replicas: 4
				  selector:
				    matchLabels:
				      app: httpd_app
				  template:
				    metadata:
				      labels:
				        app: httpd_app
				        type: front-end
				    spec:
				      containers:
				        - name: httpd-container
				          image: httpd:latest

		5.
		thor@jump_host ~$ kubectl create -f /tmp/httpd.yaml 
			replicaset.apps/httpd-replicaset created
		 
		6.
		thor@jump_host ~$ kubectl get pods
			NAME                     READY   STATUS              RESTARTS   AGE
			httpd-replicaset-9ddvb   0/1     ContainerCreating   0          13s
			httpd-replicaset-ndtzd   0/1     ContainerCreating   0          13s
			httpd-replicaset-p8vdv   0/1     ContainerCreating   0          13s
			httpd-replicaset-w4nmd   0/1     ContainerCreating   0          13s

--------------------------------------------------------------------------------------------------------------------------------------------
Task 38: 15/Feb/2022
Git Clone Repositories

DevOps team created a new Git repository last week; however, as of now no team is using it. The Nautilus application development team recently asked for a copy of that repo on Storage server in Stratos DC. Please clone the repo as per details shared below:

    The repo that needs to be cloned is /opt/news.git

    Clone this git repository under /usr/src/kodekloudrepos directory. Please do not try to make any changes in repo.

		1. Login to Storage server an switch to root
		ssh natasha@ststor01

		sudo su -

		2. Go to location where you want to clone the repository / destination and verify its empty
		# cd /usr/src/kodekloudrepos/
		# ll
			total 0

		3. Clone the repository using given location/source and verify the contents
		# git clone /opt/news.git/
			Cloning into 'news'...
			warning: You appear to have cloned an empty repository.
			done.

		# ll
			total 4
			drwxr-xr-x 3 root root 4096 Feb 15 17:32 news

		# cd news

		# ll
			total 0

--------------------------------------------------------------------------------------------------------------------------------------------
Task 39 : 17/Feb/2022
Setup Puppet Certs

The Nautilus DevOps team has set up a puppet master and an agent node in Stratos Datacenter. Puppet master is running on jump host itself (also note that Puppet master node is also running as Puppet CA server) and Puppet agent is running on App Server 2. Since it is a fresh set up, the team wants to sign certificates for puppet master as well as puppet agent nodes so that they can proceed with the next steps of set up. You can find more details about the task below:

Puppet server and agent nodes are already have required packages, but you may need to start puppetserver (on master) and puppet service on both nodes.

    Assign and sign certificates for both master and agent node.


	On Jump Host/PuppetMaster server :

		1. Switch to root user 
		sudo su -

		2. Make changes in /etc/hosts to include alias for puppet master
		# vi /etc/hosts
			172.16.238.3    jump_host.stratos.xfusioncorp.com jump_host puppet
			172.16.239.5    jump_host.stratos.xfusioncorp.com jump_host puppet
			172.17.0.4      jump_host.stratos.xfusioncorp.com jump_host puppet

		# cat /etc/hosts
			127.0.0.1       localhost
			::1     localhost ip6-localhost ip6-loopback
			fe00::0 ip6-localnet
			ff00::0 ip6-mcastprefix
			ff02::1 ip6-allnodes
			ff02::2 ip6-allrouters
			172.16.238.10   stapp01.stratos.xfusioncorp.com
			172.16.238.11   stapp02.stratos.xfusioncorp.com
			172.16.238.12   stapp03.stratos.xfusioncorp.com
			172.16.238.3    jump_host.stratos.xfusioncorp.com jump_host puppet
			172.16.239.5    jump_host.stratos.xfusioncorp.com jump_host puppet
			172.17.0.4      jump_host.stratos.xfusioncorp.com jump_host puppet

		3.Check puppetserver status
		# systemctl status puppetserver
		● puppetserver.service - puppetserver Service
		   Loaded: loaded (/usr/lib/systemd/system/puppetserver.service; disabled; vendor preset: disabled)
		   Active: active (running) since Thu 2022-02-17 04:34:20 UTC; 1min 11s ago
		  Process: 13195 ExecStart=/opt/puppetlabs/server/apps/puppetserver/bin/puppetserver start (code=exited, status=0/SUCCESS)
		 Main PID: 13258 (java)
		    Tasks: 90 (limit: 4915)
		   CGroup: /docker/9463a0d98965db0a54bc2af297eb82a9ef1abcbe227e26805fd5cf3cd02bdb53/system.slice/puppetserver.service
		           └─13258 /usr/bin/java -Xms512m -Xmx512m -Djruby.logger.class=com.puppetlabs.jruby_utils.jruby.Slf4jLogger -XX:OnOutOfMemoryErro...

		4. Check certificates on puppetserver
		# puppetserver ca  list --all
		Signed Certificates:
		    964369dd618d.c.argo-prod-us-east1.internal       (SHA256)  8C:19:47:FD:59:97:CE:0C:1D:DF:52:92:A3:BA:41:22:F2:3D:95:D4:38:D9:EF:85:65:9A:7B:B5:59:D5:CA:E6       alt names: ["DNS:puppet", "DNS:964369dd618d.c.argo-prod-us-east1.internal"]     authorization extensions: [pp_cli_auth: true]
		    jump_host.stratos.xfusioncorp.com                (SHA256)  F1:1F:B0:FE:F6:4A:20:52:C4:C7:D6:9E:1B:FC:2E:32:21:8C:53:5E:E3:61:9F:A1:47:B9:A9:78:2F:E5:3D:66       alt names: ["DNS:puppet", "DNS:jump_host.stratos.xfusioncorp.com"]      authorization extensions: [pp_cli_auth: true]

	On App server/PuppeetClient :

		5. Login to App server and switch to root
		ssh steve@stapp02

		sudo su -

		6. Make the PuppetServer Master entries in /etc/hosts file
		# vi /etc/hosts
		 	172.16.238.3    jump_host.stratos.xfusioncorp.com jump_host puppet
			172.16.239.5    jump_host.stratos.xfusioncorp.com jump_host puppet
			172.17.0.4      jump_host.stratos.xfusioncorp.com jump_host puppet

		7. Verify hosts entries via ping 
		# ping puppet
		PING jump_host.stratos.xfusioncorp.com (172.16.238.3) 56(84) bytes of data.
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=1 ttl=64 time=0.068 ms
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=2 ttl=64 time=0.076 ms
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=3 ttl=64 time=0.071 ms
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=4 ttl=64 time=0.072 ms
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=5 ttl=64 time=0.090 ms
		^C
		--- jump_host.stratos.xfusioncorp.com ping statistics ---
		5 packets transmitted, 5 received, 0% packet loss, time 4081ms
		rtt min/avg/max/mdev = 0.068/0.075/0.090/0.011 ms

		8. Check puppet client status and check if errors
		# systemctl status puppet
		● puppet.service - Puppet agent
		   Loaded: loaded (/usr/lib/systemd/system/puppet.service; disabled; vendor preset: disabled)
		   Active: active (running) since Thu 2022-02-17 04:31:58 UTC; 6min ago
		 Main PID: 487 (puppet)
		   CGroup: /docker/a819efbba5ee93fe68e7e03589eeef739764d9d47a85c8db98909ab600c248bc/system.slice/puppet.service
		           └─487 /opt/puppetlabs/puppet/bin/ruby /opt/puppetlabs/puppet/bin/puppet agent --no-daemonize

		Feb 17 04:31:58 stapp02.stratos.xfusioncorp.com systemd[1]: About to execute: /opt/puppetlabs/puppet/bin/puppet agent $PUPPET_EXTRA_...monize
		Feb 17 04:31:58 stapp02.stratos.xfusioncorp.com systemd[1]: Forked /opt/puppetlabs/puppet/bin/puppet as 487
		Feb 17 04:31:58 stapp02.stratos.xfusioncorp.com systemd[1]: puppet.service changed dead -> running
		Feb 17 04:31:58 stapp02.stratos.xfusioncorp.com systemd[1]: Job puppet.service/start finished, result=done
		Feb 17 04:31:58 stapp02.stratos.xfusioncorp.com systemd[1]: Started Puppet agent.
		Feb 17 04:31:58 stapp02.stratos.xfusioncorp.com systemd[487]: Executing: /opt/puppetlabs/puppet/bin/puppet agent --no-daemonize
		Feb 17 04:34:20 stapp02.stratos.xfusioncorp.com puppet-agent[487]: Request to https://puppet:8140/puppet-ca/v1 timed out connect oper...conds
		Feb 17 04:34:20 stapp02.stratos.xfusioncorp.com puppet-agent[487]: Wrapped exception:
		Feb 17 04:34:20 stapp02.stratos.xfusioncorp.com puppet-agent[487]: execution expired
		Feb 17 04:34:20 stapp02.stratos.xfusioncorp.com puppet-agent[487]: No more routes to ca
		Hint: Some lines were ellipsized, use -l to show in full.

		9. Restart puppet client and check status
		# systemctl restart puppet
			
		# systemctl status puppet
		● puppet.service - Puppet agent
		   Loaded: loaded (/usr/lib/systemd/system/puppet.service; disabled; vendor preset: disabled)
		   Active: active (running) since Thu 2022-02-17 04:38:12 UTC; 3s ago
		 Main PID: 596 (puppet)
		   CGroup: /docker/a819efbba5ee93fe68e7e03589eeef739764d9d47a85c8db98909ab600c248bc/system.slice/puppet.service
		           └─596 /opt/puppetlabs/puppet/bin/ruby /opt/puppetlabs/puppet/bin/puppet agent --no-daemonize

		Feb 17 04:38:12 stapp02.stratos.xfusioncorp.com systemd[1]: About to execute: /opt/puppetlabs/puppet/bin/puppet agent $PUPPET_EXTRA_...monize
		Feb 17 04:38:12 stapp02.stratos.xfusioncorp.com systemd[1]: Forked /opt/puppetlabs/puppet/bin/puppet as 596
		Feb 17 04:38:12 stapp02.stratos.xfusioncorp.com systemd[1]: puppet.service changed dead -> running
		Feb 17 04:38:12 stapp02.stratos.xfusioncorp.com systemd[1]: Job puppet.service/start finished, result=done
		Feb 17 04:38:12 stapp02.stratos.xfusioncorp.com systemd[1]: Started Puppet agent.
		Feb 17 04:38:12 stapp02.stratos.xfusioncorp.com systemd[596]: Executing: /opt/puppetlabs/puppet/bin/puppet agent --no-daemonize
		Hint: Some lines were ellipsized, use -l to show in full.

	On Jump Host/PuppetMaster Server :

		10. Check certificates on puppetserver and look for stapp02 certificate request 
		# puppetserver ca  list --all
		Requested Certificates:
		    stapp02.stratos.xfusioncorp.com       (SHA256)  8F:E9:B3:29:D6:8C:83:7D:47:3F:03:0C:F0:F2:06:18:E1:60:00:47:5F:C3:D0:00:68:4B:B9:6A:BC:99:5F:DB
		Signed Certificates:
		    964369dd618d.c.argo-prod-us-east1.internal       (SHA256)  8C:19:47:FD:59:97:CE:0C:1D:DF:52:92:A3:BA:41:22:F2:3D:95:D4:38:D9:EF:85:65:9A:7B:B5:59:D5:CA:E6       alt names: ["DNS:puppet", "DNS:964369dd618d.c.argo-prod-us-east1.internal"]     authorization extensions: [pp_cli_auth: true]
		    jump_host.stratos.xfusioncorp.com                (SHA256)  F1:1F:B0:FE:F6:4A:20:52:C4:C7:D6:9E:1B:FC:2E:32:21:8C:53:5E:E3:61:9F:A1:47:B9:A9:78:2F:E5:3D:66       alt names: ["DNS:puppet", "DNS:jump_host.stratos.xfusioncorp.com"]      authorization extensions: [pp_cli_auth: true]

		11. Sign all certificates
		# puppetserver ca sign --all
		Successfully signed certificate request for stapp02.stratos.xfusioncorp.com	

	On App server/PuppetClient :

		12. Validate puppet agent sign on
		# puppet agent -t
		Info: csr_attributes file loading from /etc/puppetlabs/puppet/csr_attributes.yaml
		Info: Creating a new SSL certificate request for stapp02.stratos.xfusioncorp.com
		Info: Certificate Request fingerprint (SHA256): 8F:E9:B3:29:D6:8C:83:7D:47:3F:03:0C:F0:F2:06:18:E1:60:00:47:5F:C3:D0:00:68:4B:B9:6A:BC:99:5F:DB
		Info: Downloaded certificate for stapp02.stratos.xfusioncorp.com from https://puppet:8140/puppet-ca/v1
		Info: Using configured environment 'production'
		Info: Retrieving pluginfacts
		Info: Retrieving plugin
		Info: Retrieving locales
		Info: Caching catalog for stapp02.stratos.xfusioncorp.com
		Info: Applying configuration version '1645072754'
		Info: Creating state file /opt/puppetlabs/puppet/cache/state/state.yaml
		Notice: Applied catalog in 0.01 seconds

--------------------------------------------------------------------------------------------------------------------------------------------
Task 40 : 24/Feb/2022
Ansible Setup Httpd and PHP
Nautilus Application development team wants to test the Apache and PHP setup on one of the app servers in Stratos Datacenter. They want the DevOps team to prepare an Ansible playbook to accomplish this task. Below you can find more details about the task.

There is an inventory file ~/playbooks/inventory on jump host.

Create a playbook ~/playbooks/httpd.yml on jump host and perform the following tasks on App Server 1.

a. Install httpd and php packages (whatever default version is available in yum repo).

b. Change default document root of Apache to /var/www/html/myroot in default Apache config /etc/httpd/conf/httpd.conf. Make sure /var/www/html/myroot path exists (if not please create the same).

c. There is a template ~/playbooks/templates/phpinfo.php.j2 on jump host. Copy this template to the Apache document root you created as phpinfo.php file and make sure user owner and the group owner for this file is apache user.

d. Start and enable httpd service.

Note: Validation will try to run the playbook using command ansible-playbook -i inventory httpd.yml, so please make sure the playbook works this way without passing any extra arguments.


thor@jump_host ~$ cd playbooks/
thor@jump_host ~/playbooks$ ll -a
total 20
drwxr-xr-x 3 thor thor 4096 Feb 24 05:10 .
drwxr----- 1 thor thor 4096 Feb 24 05:10 ..
-rw-r--r-- 1 thor thor   36 Feb 24 05:10 ansible.cfg
-rw-r--r-- 1 thor thor  237 Feb 24 05:10 inventory
drwxr-xr-x 2 thor thor 4096 Feb 24 05:10 templates
thor@jump_host ~/playbooks$ cd inventory 
bash: cd: inventory: Not a directory
thor@jump_host ~/playbooks$ ll -a
total 20
drwxr-xr-x 3 thor thor 4096 Feb 24 05:10 .
drwxr----- 1 thor thor 4096 Feb 24 05:10 ..
-rw-r--r-- 1 thor thor   36 Feb 24 05:10 ansible.cfg
-rw-r--r-- 1 thor thor  237 Feb 24 05:10 inventory
drwxr-xr-x 2 thor thor 4096 Feb 24 05:10 templates
thor@jump_host ~/playbooks$ vi httpd.yml
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ cat httpd.yml 
- name: Setup Httpd and PHP

  hosts: stapp01

  become: yes

  tasks:

    - name: Install latest version of httpd and php

      package:

        name:

          - httpd

          - php

        state: latest

    - name: Replace default DocumentRoot in httpd.conf

      replace:

        path: /etc/httpd/conf/httpd.conf

        regexp: DocumentRoot \"\/var\/www\/html\"

        replace: DocumentRoot "/var/www/html/myroot"

    - name: Create the new DocumentRoot directory if it does not exist

      file:

        path: /var/www/html/myroot

        state: directory

        owner: apache

        group: apache

    - name: Use Jinja2 template to generate phpinfo.php

      template:

        src: /home/thor/playbooks/templates/phpinfo.php.j2

        dest: /var/www/html/myroot/phpinfo.php

        owner: apache

        group: apache

    - name: Start and enable service httpd

      service:

        name: httpd

        state: started

        enabled: yes 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ cd templates/
thor@jump_host ~/playbooks/templates$  ll-a
bash: ll-a: command not found
thor@jump_host ~/playbooks/templates$  ll -a
total 12
drwxr-xr-x 2 thor thor 4096 Feb 24 05:10 .
drwxr-xr-x 3 thor thor 4096 Feb 24 05:13 ..
-rw-r--r-- 1 thor thor   19 Feb 24 05:10 phpinfo.php.j2
thor@jump_host ~/playbooks/templates$ cd ../
thor@jump_host ~/playbooks$ ansible-playbook -i inventory httpd.yml 

PLAY [Setup Httpd and PHP] ******************************************************************************************************************

TASK [Gathering Facts] **********************************************************************************************************************
ok: [stapp01]

TASK [Install latest version of httpd and php] **********************************************************************************************
changed: [stapp01]

TASK [Replace default DocumentRoot in httpd.conf] *******************************************************************************************
changed: [stapp01]

TASK [Create the new DocumentRoot directory if it does not exist] ***************************************************************************
changed: [stapp01]

TASK [Use Jinja2 template to generate phpinfo.php] ******************************************************************************************
changed: [stapp01]

TASK [Start and enable service httpd] *******************************************************************************************************
changed: [stapp01]

PLAY RECAP **********************************************************************************************************************************
stapp01                    : ok=6    changed=5    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

thor@jump_host ~/playbooks$ 

Last login: Thu Feb 24 05:10:23 UTC 2022 on pts/0
thor@jump_host ~$ ssh tony@stapp01
The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
ECDSA key fingerprint is SHA256:HRK4JT6fwc3O4QN4Kd4nKBfOFgvwjBs/XBj+WP8m56Y.
ECDSA key fingerprint is MD5:46:21:7e:34:9a:50:9c:45:12:9f:ba:d6:de:ac:ed:db.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'stapp01' (ECDSA) to the list of known hosts.
tony@stapp01's password: 
Last login: Thu Feb 24 05:15:26 2022 from jump_host.devops-ansible-httpd-php-v2_app_net
[tony@stapp01 ~]$ 
[tony@stapp01 ~]$ 
[tony@stapp01 ~]$ sudo su -
[root@stapp01 ~]# Ir0nM@n
-bash: Ir0nM@n: command not found
[root@stapp01 ~]# 
[root@stapp01 ~]# 
[root@stapp01 ~]# 
[root@stapp01 ~]# 
[root@stapp01 ~]# 
[root@stapp01 ~]# 
[root@stapp01 ~]# rpm -qa |grep httpd
httpd-tools-2.4.6-97.el7.centos.4.x86_64
httpd-2.4.6-97.el7.centos.4.x86_64
[root@stapp01 ~]# rpm -qa |grep php
php-cli-5.4.16-48.el7.x86_64
php-common-5.4.16-48.el7.x86_64
php-5.4.16-48.el7.x86_64
[root@stapp01 ~]# 
[root@stapp01 ~]# 
[root@stapp01 ~]# cd /var/www/html/
[root@stapp01 html]# ls -ahl
total 12K
drwxr-xr-x 3 root   root   4.0K Feb 24 05:15 .
drwxr-xr-x 4 root   root   4.0K Feb 24 05:15 ..
drwxr-xr-x 2 apache apache 4.0K Feb 24 05:15 myroot
[root@stapp01 html]# 

--------------------------------------------------------------------------------------------------------------------------------------------

Task 41 : 25/Feb/2022
Ansible File Module
The Nautilus DevOps team is working to test several Ansible modules on servers in Stratos DC. Recently they wanted to test the file creation on remote hosts using Ansible. Find below more details about the task:

a. Create an inventory file ~/playbook/inventory on jump host and add all app servers in it.

b. Create a playbook ~/playbook/playbook.yml to create a blank file /opt/nfsshare.txt on all app servers.

c. The /opt/nfsshare.txt file permission must be 0755.

d. The user/group owner of file /opt/nfsshare.txt must be tony on app server 1, steve on app server 2 and banner on app server 3.

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml, so please make sure the playbook works this way without passing any extra arguments.


		1. Got to playbook directory 
		cd playbook/

		pwd
			/home/thor/playbook

		2. Create the inventory file as per given requirements and verify
		ll -a
			total 8
			drwxr-xr-x 2 thor thor 4096 Feb 25 09:14 .
			drwxr----- 1 thor thor 4096 Feb 25 09:14 ..

		vi inventory
			stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n  ansible_user=tony
			stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@  ansible_user=steve
			stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n  ansible_user=banner

		cat inventory 
			stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n  ansible_user=tony
			stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@  ansible_user=steve
			stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n  ansible_user=banner

		3. Verify if ansible is able to connect as per inventory file
		ansible all -a "ls -ahl /opt/nfsshare.txt" -i inventory
			stapp03 | FAILED | rc=2 >>
			ls: cannot access /opt/nfsshare.txt: No such file or directorynon-zero return code
			stapp02 | FAILED | rc=2 >>
			ls: cannot access /opt/nfsshare.txt: No such file or directorynon-zero return code
			stapp01 | FAILED | rc=2 >>
			ls: cannot access /opt/nfsshare.txt: No such file or directorynon-zero return code

		4. Create playbook.yml file as per requirements and verify
		vi playbook.yml
			- name: Create file in appservers
		  	  hosts: stapp01, stapp02, stapp03
		  	  become: yes
		      tasks:
			    - name: Create the file and set properties
			      file:
			        path: /opt/nfsshare.txt
			        owner: "{{ ansible_user }}"
			        group: "{{ ansible_user }}"
			        mode: "0755"
			        state: touch

		cat playbook.yml 
			- name: Create file in appservers

			  hosts: stapp01, stapp02, stapp03

			  become: yes

			  tasks:

			    - name: Create the file and set properties

			      file:

			        path: /opt/nfsshare.txt

			        owner: "{{ ansible_user }}"

			        group: "{{ ansible_user }}"

			        mode: "0755"

			        state: touch

		5. Run the playbook
		ansible-playbook -i inventory playbook.yml 

			PLAY [Create file in appservers] ************************************************************************************************************

			TASK [Gathering Facts] **********************************************************************************************************************
			ok: [stapp03]
			ok: [stapp02]
			ok: [stapp01]

			TASK [Create the file and set properties] ***************************************************************************************************
			changed: [stapp01]
			changed: [stapp03]
			changed: [stapp02]

			PLAY RECAP **********************************************************************************************************************************
			stapp01                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
			stapp02                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
			stapp03                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

		6. Verify the files created on App servers
		ansible all -a "ls -ahl /opt/nfsshare.txt" -i inventory
			stapp01 | CHANGED | rc=0 >>
			-rwxr-xr-x 1 tony tony 0 Feb 25 09:20 /opt/nfsshare.txt
			stapp03 | CHANGED | rc=0 >>
			-rwxr-xr-x 1 banner banner 0 Feb 25 09:20 /opt/nfsshare.txt
			stapp02 | CHANGED | rc=0 >>
			-rwxr-xr-x 1 steve steve 0 Feb 25 09:20 /opt/nfsshare.txt

--------------------------------------------------------------------------------------------------------------------------------------------

Task 42: 4/Mar/2022
Troubleshoot Deployment issues in Kubernetes

Last week, the Nautilus DevOps team deployed a redis app on Kubernetes cluster, which was working fine so far. This morning one of the team members was making some changes in this existing setup, but he made some mistakes and the app went down. We need to fix this as soon as possible. Please take a look.

The deployment name is redis-deployment. The pods are not in running state right now, so please look into the issue and fix the same.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

thor@jump_host ~$ kubectl get-deployment
Error: unknown command "get-deployment" for "kubectl"
Run 'kubectl --help' for usage.
thor@jump_host ~$ kubectl get deployment
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
redis-deployment   0/1     1            0           71s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
NAME                               READY   STATUS              RESTARTS   AGE
redis-deployment-8bdf985f7-65g4j   0/1     ContainerCreating   0          106s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get configmap
NAME               DATA   AGE
kube-root-ca.crt   1      20m
redis-config       2      2m4s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl describe deployment
Name:                   redis-deployment
Namespace:              default
CreationTimestamp:      Fri, 04 Mar 2022 07:59:44 +0000
Labels:                 app=redis
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=redis
Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=redis
  Containers:
   redis-container:
    Image:      redis:alpin
    Port:       6379/TCP
    Host Port:  0/TCP
    Requests:
      cpu:        300m
    Environment:  <none>
    Mounts:
      /redis-master from config (rw)
      /redis-master-data from data (rw)
  Volumes:
   data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
   config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      redis-conig
    Optional:  false
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      False   MinimumReplicasUnavailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  redis-deployment-8bdf985f7 (1/1 replicas created)
NewReplicaSet:   <none>
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  2m45s  deployment-controller  Scaled up replica set redis-deployment-8bdf985f7 to 1
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl describe pods
Name:           redis-deployment-8bdf985f7-65g4j
Namespace:      default
Priority:       0
Node:           kodekloud-control-plane/172.17.0.2
Start Time:     Fri, 04 Mar 2022 07:59:44 +0000
Labels:         app=redis
                pod-template-hash=8bdf985f7
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/redis-deployment-8bdf985f7
Containers:
  redis-container:
    Container ID:   
    Image:          redis:alpin
    Image ID:       
    Port:           6379/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        300m
    Environment:  <none>
    Mounts:
      /redis-master from config (rw)
      /redis-master-data from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-mktdc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  data:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      redis-conig
    Optional:  false
  default-token-mktdc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-mktdc
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age                  From               Message
  ----     ------       ----                 ----               -------
  Normal   Scheduled    3m38s                default-scheduler  Successfully assigned default/redis-deployment-8bdf985f7-65g4j to kodekloud-control-plane
  Warning  FailedMount  95s                  kubelet            Unable to attach or mount volumes: unmounted volumes=[config], unattached volumes=[data config default-token-mktdc]: timed out waiting for the condition
  Warning  FailedMount  90s (x9 over 3m37s)  kubelet            MountVolume.SetUp failed for volume "config" : configmap "redis-conig" not found
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl describe configmap
Name:         kube-root-ca.crt
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
ca.crt:
----
-----BEGIN CERTIFICATE-----
MIIC5zCCAc+gAwIBAgIBADANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwprdWJl
cm5ldGVzMB4XDTIyMDMwNDA3MzkxMVoXDTMyMDMwMTA3MzkxMVowFTETMBEGA1UE
AxMKa3ViZXJuZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALme
RU36Jbasf+ky0j2DuqdVD+2Vw1trKQnA4N5n/gFzvKFUu4d6ZW0lWNyd73f/0W4U
y9joi+Pp2WZX5wEbwyTbJZ9RYopm8rgRRg1V1+GU4mC5aBUrat4pjQZX4OQNyBMd
ZBz/Cll/c/S6Y1vTDeqOm4SYz+2YSyVOtdLtBnUDGxH6m13gfqrGN/n/RhlDJO9T
722YgASeOFvPXpJMzStK/lZjUcsW4EYNoQgKewn+yB5k2LTvhs3ovj5Mc0gbWMBn
azJgCen+EJAV2yFxuFf41ppi1MvY7CeytGuOnv3uNi609TJrlp7NEuSoMCeIt1gT
IcaNpsFGFJ8CiOFWjgsCAwEAAaNCMEAwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB
/wQFMAMBAf8wHQYDVR0OBBYEFJ6FKF3kj6rXegz1wkfzx09gxROjMA0GCSqGSIb3
DQEBCwUAA4IBAQCuOE/yWOW7NAZAgYCavM8gd+px/onSM9o43wcl7YSvFM6/TVDq
ksbpbdG60GmgV5Fggui6p+zaaAYrtY33qRIYKwvQ0rpEx04llQRETrvXyXCtAe83
zYteCjlbY0rlpjWhpxxXS0RJLilMojtbi4Vr05zLEyYa5cQ1NN5YNgzGdFBHYY+D
bPvGVXFyVrzDdf8iUt9VcMOsrD03s7sUitnoIJSE+PkaYvdxcTuwENRccQTZoj6E
vfWAmEqPTVTYnkv6Z1lHejwXUoWd/DaVnmajUKB8YS2TgoUjwpEJbS0lRAikHeSs
zZiAqUXdZmZQXDWK6mlQELlg0u48IhGrHrF6
-----END CERTIFICATE-----

Events:  <none>


Name:         redis-config
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
maxmemory:
----
2mb
maxmemory-policy:
----
allkeys-lru
Events:  <none>
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl edit deployment
deployment.apps/redis-deployment edited
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get deploy
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
redis-deployment   1/1     1            1           9m18s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
redis-deployment-5bb6dd57fd-l4l56   1/1     Running   0          46s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get deploy
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
redis-deployment   1/1     1            1           9m33s
thor@jump_host ~$ 


--------------------------------------------------------------------------------------------------------------------------------------------
Task 43 : 12/Mar/2022
Deploy Grafana on Kubernetes Cluster

The Nautilus DevOps teams is planning to set up a Grafana tool to collect and analyze analytics from some applications. They are planning to deploy it on Kubernetes cluster. Below you can find more details.

1.) Create a deployment named grafana-deployment-nautilus using any grafana image for Grafana app. Set other parameters as per your choice.

2.) Create NodePort type service with nodePort 32000 to expose the app.

You need not to make any configuration changes inside the Grafana app once deployed, just make sure you are able to access the Grafana login page.

Note: The kubeclt on jump_host has been configured to work with kubernetes cluster.


thor@jump_host ~$ kubectl get pods
No resources found in default namespace.
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get services
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   3h16m
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ vi /tmp/grafana.yaml
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ cat /tmp/grafana.yaml 
apiVersion: v1

kind: Service

metadata:

  name: grafana-service-nautilus

spec:

  type: NodePort

  selector:

    app: grafana

  ports:

    - port: 3000

      targetPort: 3000

      nodePort: 32000

---

apiVersion: apps/v1

kind: Deployment

metadata:

  name: grafana-deployment-nautilus

spec:

  selector:

    matchLabels:

      app: grafana

  template:

    metadata:

      labels:

        app: grafana

    spec:

      containers:

        - name: grafana-container-nautilus

          image: grafana/grafana:latest

          ports:

            - containerPort: 3000
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl create -f /tmp/grafana.yaml 
service/grafana-service-nautilus created
deployment.apps/grafana-deployment-nautilus created
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get services
NAME                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
grafana-service-nautilus   NodePort    10.96.233.148   <none>        3000:32000/TCP   15s
kubernetes                 ClusterIP   10.96.0.1       <none>        443/TCP          3h18m
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
NAME                                           READY   STATUS    RESTARTS   AGE
grafana-deployment-nautilus-66bb8cdd8b-xkrwz   1/1     Running   0          22s

--------------------------------------------------------------------------------------------------------------------------------------------
Task 44 : 16/Mar/2022

Create Cronjobs in Kubernetes
There are some jobs/tasks that need to be run regularly on different schedules. Currently the Nautilus DevOps team is working on developing some scripts that will be executed on different schedules, but for the time being the team is creating some cron jobs in Kubernetes cluster with some dummy commands (which will be replaced by original scripts later). Create a cronjob as per details given below:

    Create a cronjob named datacenter.

    Set schedule to */8 * * * *.

    Container name should be cron-datacenter.

    Use nginx image with latest tag only and remember to mention the tag i.e nginx:latest.

    Run a dummy command echo Welcome to xfusioncorp!.

    Ensure restart policy is OnFailure.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

thor@jump_host ~$ kubectl get pods
No resources found in default namespace.
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get cronjobs
No resources found in default namespace.
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get cronjob
No resources found in default namespace.
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ vi /tmp/cron.yml
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ cat /tmp/cron.yml 
apiVersion: batch/v1beta1

kind: CronJob

metadata:

  name: devops

spec:

  schedule: "*/5 * * * *"

  jobTemplate:

    spec:

      template:

        spec:

          containers:

            - name: cron-devops

              image: nginx:latest

              command:

                - /bin/sh

                - -c

                - echo Welcome to xfusioncorp!

          restartPolicy: OnFailure
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl create -f /tmp/cron.yml 
cronjob.batch/devops created
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
No resources found in default namespace.
thor@jump_host ~$ kubectl get pod
No resources found in default namespace.
thor@jump_host ~$ kubectl get cronjob
NAME     SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
devops   */5 * * * *   False     0        <none>          35s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pod
No resources found in default namespace.
thor@jump_host ~$ kubectl get pod
No resources found in default namespace.
thor@jump_host ~$ kubectl get pod
No resources found in default namespace.
thor@jump_host ~$ kubectl get cronjob
NAME     SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
devops   */5 * * * *   False     0        <none>          61s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get cronjob
NAME     SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
devops   */5 * * * *   False     0        <none>          67s
thor@jump_host ~$ kubectl get cronjob
NAME     SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
devops   */5 * * * *   False     0        <none>          70s
thor@jump_host ~$ kubectl get cronjob
NAME     SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
devops   */5 * * * *   False     1        15s             101s
thor@jump_host ~$ kubectl get pod
NAME                      READY   STATUS              RESTARTS   AGE
devops-1647415500-7jhh7   0/1     ContainerCreating   0          16s
thor@jump_host ~$ kubectl logs devops-1647415500-7jhh7

--------------------------------------------------------------------------------------------------------------------------------------------

Task 45 : 18/Mar/2022
Git Manage Remotes

The xFusionCorp development team added updates to the project that is maintained under /opt/beta.git repo and cloned under /usr/src/kodekloudrepos/beta. Recently some changes were made on Git server that is hosted on Storage server in Stratos DC. The DevOps team added some new Git remotes, so we need to update remote on /usr/src/kodekloudrepos/beta repository as per details mentioned below:

a. In /usr/src/kodekloudrepos/beta repo add a new remote dev_beta and point it to /opt/xfusioncorp_beta.git repository.

b. There is a file /tmp/index.html on same server; copy this file to the repo and add/commit to master branch.

c. Finally push master branch to this new remote origin.

thor@jump_host ~$ ssh natasha@ststor01
The authenticity of host 'ststor01 (172.16.238.15)' can't be established.
ECDSA key fingerprint is SHA256:ZYks87RZDpXlv4gMTVaz+GZFGeGKd3ziPSVrZQSr8vI.
ECDSA key fingerprint is MD5:bf:52:2a:cd:74:f5:9f:ad:8a:28:a4:d7:09:3e:e9:d1.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'ststor01,172.16.238.15' (ECDSA) to the list of known hosts.
natasha@ststor01's password: 
[natasha@ststor01 ~]$ 
[natasha@ststor01 ~]$ 
[natasha@ststor01 ~]$ 
[natasha@ststor01 ~]$ sudo su -

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for natasha: 
[root@ststor01 ~]# 
[root@ststor01 ~]# 
[root@ststor01 ~]# 
[root@ststor01 ~]# cd /usr/src/kodekloudrepos/
[root@ststor01 kodekloudrepos]# ls -ahl
total 12K
drwxr-xr-x 3 root root 4.0K Mar 17 18:35 .
drwxr-xr-x 1 root root 4.0K Mar 17 18:35 ..
drwxr-xr-x 3 root root 4.0K Mar 17 18:35 beta
[root@ststor01 kodekloudrepos]# cd beta/
[root@ststor01 beta]# 
[root@ststor01 beta]# ls -ahl
total 16K
drwxr-xr-x 3 root root 4.0K Mar 17 18:35 .
drwxr-xr-x 3 root root 4.0K Mar 17 18:35 ..
drwxr-xr-x 8 root root 4.0K Mar 17 18:35 .git
-rw-r--r-- 1 root root   34 Mar 17 18:35 info.txt
[root@ststor01 beta]# git remote add dev_beta /opt/xfusioncorp_beta.git
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# cp /tmp/index.html ./
[root@ststor01 beta]# ls -ahl
total 20K
drwxr-xr-x 3 root root 4.0K Mar 17 18:39 .
drwxr-xr-x 3 root root 4.0K Mar 17 18:35 ..
drwxr-xr-x 8 root root 4.0K Mar 17 18:38 .git
-rw-r--r-- 1 root root  120 Mar 17 18:39 index.html
-rw-r--r-- 1 root root   34 Mar 17 18:35 info.txt
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# git init
Reinitialized existing Git repository in /usr/src/kodekloudrepos/beta/.git/
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# git add index.html 
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# git commit -m "Added index.html"
[master 36e84d4] Added index.html
 1 file changed, 10 insertions(+)
 create mode 100644 index.html
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# git push -u dev_beta master
Counting objects: 6, done.
Delta compression using up to 36 threads.
Compressing objects: 100% (4/4), done.
Writing objects: 100% (6/6), 584 bytes | 0 bytes/s, done.
Total 6 (delta 0), reused 0 (delta 0)
To /opt/xfusioncorp_beta.git
 * [new branch]      master -> master
Branch master set up to track remote branch master from dev_beta.
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# 

--------------------------------------------------------------------------------------------------------------------------------------------
Task 46: 19/Mar/2022
Docker EXEC Operations

One of the Nautilus DevOps team members was working to configure services on a kkloud container that is running on App Server 1 in Stratos Datacenter. Due to some personal work he is on PTO for the rest of the week, but we need to finish his pending work ASAP. Please complete the remaining work as per details given below:

a. Install apache2 in kkloud container using apt that is running on App Server 1 in Stratos Datacenter.

b. Configure Apache to listen on port 5001 instead of default http port. Do not bind it to listen on specific IP or hostname only, i.e it should listen on localhost, 127.0.0.1, container ip, etc.

c. Make sure Apache service is up and running inside the container. Keep the container in running state at the end.

thor@jump_host ~$ ssh tony@stapp01
The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
ECDSA key fingerprint is SHA256:4yH914g0tXmYd6hoQ6Lf3EOXLvLr0mE6XM97FGBFex0.
ECDSA key fingerprint is MD5:98:c0:60:13:6b:23:d6:a3:d9:02:dc:28:f6:2b:14:d5.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'stapp01,172.16.238.10' (ECDSA) to the list of known hosts.
tony@stapp01's password: 
[tony@stapp01 ~]$ 
[tony@stapp01 ~]$ 
[tony@stapp01 ~]$ 
[tony@stapp01 ~]$ sudo su - 

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for tony: 
[root@stapp01 ~]# 
[root@stapp01 ~]# 
[root@stapp01 ~]# 
[root@stapp01 ~]# docker ps
CONTAINER ID   IMAGE           COMMAND   CREATED              STATUS              PORTS     NAMES
b71b6813474d   ubuntu:latest   "bash"    About a minute ago   Up About a minute             kkloud
[root@stapp01 ~]# 
[root@stapp01 ~]# 
[root@stapp01 ~]# docker exec -it kkloud /bin/sh
# 
# 
# apt install apache2 -y
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following additional packages will be installed:
  apache2-bin apache2-data apache2-utils file libapr1 libaprutil1 libaprutil1-dbd-sqlite3 libaprutil1-ldap libexpat1 libgdbm-compat4
  libgdbm6 libicu66 libjansson4 liblua5.2-0 libmagic-mgc libmagic1 libperl5.30 libxml2 mime-support netbase perl perl-modules-5.30 ssl-cert
  tzdata xz-utils
Suggested packages:
  apache2-doc apache2-suexec-pristine | apache2-suexec-custom www-browser ufw gdbm-l10n perl-doc libterm-readline-gnu-perl
  | libterm-readline-perl-perl make libb-debug-perl liblocale-codes-perl openssl-blacklist
The following NEW packages will be installed:
  apache2 apache2-bin apache2-data apache2-utils file libapr1 libaprutil1 libaprutil1-dbd-sqlite3 libaprutil1-ldap libexpat1
  libgdbm-compat4 libgdbm6 libicu66 libjansson4 liblua5.2-0 libmagic-mgc libmagic1 libperl5.30 libxml2 mime-support netbase perl
  perl-modules-5.30 ssl-cert tzdata xz-utils
0 upgraded, 26 newly installed, 0 to remove and 0 not upgraded.
Need to get 18.8 MB of archives.
After this operation, 101 MB of additional disk space will be used.
Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 perl-modules-5.30 all 5.30.0-9ubuntu0.2 [2738 kB]
Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libgdbm6 amd64 1.18.1-5 [27.4 kB]
Get:3 http://archive.ubuntu.com/ubuntu focal/main amd64 libgdbm-compat4 amd64 1.18.1-5 [6244 B]
Get:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libperl5.30 amd64 5.30.0-9ubuntu0.2 [3952 kB]
Get:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 perl amd64 5.30.0-9ubuntu0.2 [224 kB]
Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 libapr1 amd64 1.6.5-1ubuntu1 [91.4 kB]
Get:7 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libexpat1 amd64 2.2.9-1ubuntu0.4 [74.4 kB]
Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 libaprutil1 amd64 1.6.1-4ubuntu2 [84.7 kB]
Get:9 http://archive.ubuntu.com/ubuntu focal/main amd64 libaprutil1-dbd-sqlite3 amd64 1.6.1-4ubuntu2 [10.5 kB]
Get:10 http://archive.ubuntu.com/ubuntu focal/main amd64 libaprutil1-ldap amd64 1.6.1-4ubuntu2 [8736 B]
Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 libjansson4 amd64 2.12-1build1 [28.9 kB]
Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 liblua5.2-0 amd64 5.2.4-1.1build3 [106 kB]
Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 tzdata all 2021e-0ubuntu0.20.04 [295 kB]
Get:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libicu66 amd64 66.1-2ubuntu2.1 [8515 kB]
Get:15 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libxml2 amd64 2.9.10+dfsg-5ubuntu0.20.04.2 [640 kB]
Get:16 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 apache2-bin amd64 2.4.41-4ubuntu3.10 [1181 kB]
Get:17 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 apache2-data all 2.4.41-4ubuntu3.10 [158 kB]
Get:18 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 apache2-utils amd64 2.4.41-4ubuntu3.10 [84.5 kB]
Get:19 http://archive.ubuntu.com/ubuntu focal/main amd64 mime-support all 3.64ubuntu1 [30.6 kB]
Get:20 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 apache2 amd64 2.4.41-4ubuntu3.10 [95.5 kB]
Get:21 http://archive.ubuntu.com/ubuntu focal/main amd64 libmagic-mgc amd64 1:5.38-4 [218 kB]
Get:22 http://archive.ubuntu.com/ubuntu focal/main amd64 libmagic1 amd64 1:5.38-4 [75.9 kB]
Get:23 http://archive.ubuntu.com/ubuntu focal/main amd64 file amd64 1:5.38-4 [23.3 kB]
Get:24 http://archive.ubuntu.com/ubuntu focal/main amd64 netbase all 6.1 [13.1 kB]
Get:25 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 xz-utils amd64 5.2.4-1ubuntu1 [82.5 kB]
Get:26 http://archive.ubuntu.com/ubuntu focal/main amd64 ssl-cert all 1.0.39 [17.0 kB]
Fetched 18.8 MB in 1s (30.2 MB/s)     
debconf: delaying package configuration, since apt-utils is not installed
Selecting previously unselected package perl-modules-5.30.
(Reading database ... 4660 files and directories currently installed.)
Preparing to unpack .../00-perl-modules-5.30_5.30.0-9ubuntu0.2_all.deb ...
Unpacking perl-modules-5.30 (5.30.0-9ubuntu0.2) ...
Selecting previously unselected package libgdbm6:amd64.
Preparing to unpack .../01-libgdbm6_1.18.1-5_amd64.deb ...
Unpacking libgdbm6:amd64 (1.18.1-5) ...
Selecting previously unselected package libgdbm-compat4:amd64.
Preparing to unpack .../02-libgdbm-compat4_1.18.1-5_amd64.deb ...
Unpacking libgdbm-compat4:amd64 (1.18.1-5) ...
Selecting previously unselected package libperl5.30:amd64.
Preparing to unpack .../03-libperl5.30_5.30.0-9ubuntu0.2_amd64.deb ...
Unpacking libperl5.30:amd64 (5.30.0-9ubuntu0.2) ...
Selecting previously unselected package perl.
Preparing to unpack .../04-perl_5.30.0-9ubuntu0.2_amd64.deb ...
Unpacking perl (5.30.0-9ubuntu0.2) ...
Selecting previously unselected package libapr1:amd64.
Preparing to unpack .../05-libapr1_1.6.5-1ubuntu1_amd64.deb ...
Unpacking libapr1:amd64 (1.6.5-1ubuntu1) ...
Selecting previously unselected package libexpat1:amd64.
Preparing to unpack .../06-libexpat1_2.2.9-1ubuntu0.4_amd64.deb ...
Unpacking libexpat1:amd64 (2.2.9-1ubuntu0.4) ...
Selecting previously unselected package libaprutil1:amd64.
Preparing to unpack .../07-libaprutil1_1.6.1-4ubuntu2_amd64.deb ...
Unpacking libaprutil1:amd64 (1.6.1-4ubuntu2) ...
Selecting previously unselected package libaprutil1-dbd-sqlite3:amd64.
Preparing to unpack .../08-libaprutil1-dbd-sqlite3_1.6.1-4ubuntu2_amd64.deb ...
Unpacking libaprutil1-dbd-sqlite3:amd64 (1.6.1-4ubuntu2) ...
Selecting previously unselected package libaprutil1-ldap:amd64.
Preparing to unpack .../09-libaprutil1-ldap_1.6.1-4ubuntu2_amd64.deb ...
Unpacking libaprutil1-ldap:amd64 (1.6.1-4ubuntu2) ...
Selecting previously unselected package libjansson4:amd64.
Preparing to unpack .../10-libjansson4_2.12-1build1_amd64.deb ...
Unpacking libjansson4:amd64 (2.12-1build1) ...
Selecting previously unselected package liblua5.2-0:amd64.
Preparing to unpack .../11-liblua5.2-0_5.2.4-1.1build3_amd64.deb ...
Unpacking liblua5.2-0:amd64 (5.2.4-1.1build3) ...
Selecting previously unselected package tzdata.
Preparing to unpack .../12-tzdata_2021e-0ubuntu0.20.04_all.deb ...
Unpacking tzdata (2021e-0ubuntu0.20.04) ...
Selecting previously unselected package libicu66:amd64.
Preparing to unpack .../13-libicu66_66.1-2ubuntu2.1_amd64.deb ...
Unpacking libicu66:amd64 (66.1-2ubuntu2.1) ...
Selecting previously unselected package libxml2:amd64.
Preparing to unpack .../14-libxml2_2.9.10+dfsg-5ubuntu0.20.04.2_amd64.deb ...
Unpacking libxml2:amd64 (2.9.10+dfsg-5ubuntu0.20.04.2) ...
Selecting previously unselected package apache2-bin.
Preparing to unpack .../15-apache2-bin_2.4.41-4ubuntu3.10_amd64.deb ...
Unpacking apache2-bin (2.4.41-4ubuntu3.10) ...
Selecting previously unselected package apache2-data.
Preparing to unpack .../16-apache2-data_2.4.41-4ubuntu3.10_all.deb ...
Unpacking apache2-data (2.4.41-4ubuntu3.10) ...
Selecting previously unselected package apache2-utils.
Preparing to unpack .../17-apache2-utils_2.4.41-4ubuntu3.10_amd64.deb ...
Unpacking apache2-utils (2.4.41-4ubuntu3.10) ...
Selecting previously unselected package mime-support.
Preparing to unpack .../18-mime-support_3.64ubuntu1_all.deb ...
Unpacking mime-support (3.64ubuntu1) ...
Selecting previously unselected package apache2.
Preparing to unpack .../19-apache2_2.4.41-4ubuntu3.10_amd64.deb ...
Unpacking apache2 (2.4.41-4ubuntu3.10) ...
Selecting previously unselected package libmagic-mgc.
Preparing to unpack .../20-libmagic-mgc_1%3a5.38-4_amd64.deb ...
Unpacking libmagic-mgc (1:5.38-4) ...
Selecting previously unselected package libmagic1:amd64.
Preparing to unpack .../21-libmagic1_1%3a5.38-4_amd64.deb ...
Unpacking libmagic1:amd64 (1:5.38-4) ...
Selecting previously unselected package file.
Preparing to unpack .../22-file_1%3a5.38-4_amd64.deb ...
Unpacking file (1:5.38-4) ...
Selecting previously unselected package netbase.
Preparing to unpack .../23-netbase_6.1_all.deb ...
Unpacking netbase (6.1) ...
Selecting previously unselected package xz-utils.
Preparing to unpack .../24-xz-utils_5.2.4-1ubuntu1_amd64.deb ...
Unpacking xz-utils (5.2.4-1ubuntu1) ...
Selecting previously unselected package ssl-cert.
Preparing to unpack .../25-ssl-cert_1.0.39_all.deb ...
Unpacking ssl-cert (1.0.39) ...
Setting up libexpat1:amd64 (2.2.9-1ubuntu0.4) ...
Setting up perl-modules-5.30 (5.30.0-9ubuntu0.2) ...
Setting up mime-support (3.64ubuntu1) ...
Setting up libmagic-mgc (1:5.38-4) ...
Setting up libmagic1:amd64 (1:5.38-4) ...
Setting up libapr1:amd64 (1.6.5-1ubuntu1) ...
Setting up file (1:5.38-4) ...
Setting up libjansson4:amd64 (2.12-1build1) ...
Setting up tzdata (2021e-0ubuntu0.20.04) ...
debconf: unable to initialize frontend: Dialog
debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)
debconf: falling back to frontend: Readline
Configuring tzdata
------------------

Please select the geographic area in which you live. Subsequent configuration questions will narrow this down by presenting a list of
cities, representing the time zones in which they are located.

  1. Africa   3. Antarctica  5. Arctic  7. Atlantic  9. Indian    11. SystemV  13. Etc
  2. America  4. Australia   6. Asia    8. Europe    10. Pacific  12. US
Geographic area: 12

Please select the city or region corresponding to your time zone.

  1. Alaska    3. Arizona  5. Eastern  7. Indiana-Starke  9. Mountain  11. Samoa
  2. Aleutian  4. Central  6. Hawaii   8. Michigan        10. Pacific
Time zone: 1


Current default time zone: 'US/Alaska'
Local time is now:      Fri Mar 18 20:46:13 AKDT 2022.
Universal Time is now:  Sat Mar 19 04:46:13 UTC 2022.
Run 'dpkg-reconfigure tzdata' if you wish to change it.

Setting up ssl-cert (1.0.39) ...
debconf: unable to initialize frontend: Dialog
debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)
debconf: falling back to frontend: Readline
Setting up xz-utils (5.2.4-1ubuntu1) ...
update-alternatives: using /usr/bin/xz to provide /usr/bin/lzma (lzma) in auto mode
update-alternatives: warning: skip creation of /usr/share/man/man1/lzma.1.gz because associated file /usr/share/man/man1/xz.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/unlzma.1.gz because associated file /usr/share/man/man1/unxz.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzcat.1.gz because associated file /usr/share/man/man1/xzcat.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzmore.1.gz because associated file /usr/share/man/man1/xzmore.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzless.1.gz because associated file /usr/share/man/man1/xzless.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzdiff.1.gz because associated file /usr/share/man/man1/xzdiff.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzcmp.1.gz because associated file /usr/share/man/man1/xzcmp.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzgrep.1.gz because associated file /usr/share/man/man1/xzgrep.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzegrep.1.gz because associated file /usr/share/man/man1/xzegrep.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzfgrep.1.gz because associated file /usr/share/man/man1/xzfgrep.1.gz (of link group lzma) doesn't exist
Setting up liblua5.2-0:amd64 (5.2.4-1.1build3) ...
Setting up netbase (6.1) ...
Setting up apache2-data (2.4.41-4ubuntu3.10) ...
Setting up libgdbm6:amd64 (1.18.1-5) ...
Setting up libaprutil1:amd64 (1.6.1-4ubuntu2) ...
Setting up libicu66:amd64 (66.1-2ubuntu2.1) ...
Setting up libaprutil1-ldap:amd64 (1.6.1-4ubuntu2) ...
Setting up libaprutil1-dbd-sqlite3:amd64 (1.6.1-4ubuntu2) ...
Setting up libgdbm-compat4:amd64 (1.18.1-5) ...
Setting up libperl5.30:amd64 (5.30.0-9ubuntu0.2) ...
Setting up libxml2:amd64 (2.9.10+dfsg-5ubuntu0.20.04.2) ...
Setting up apache2-utils (2.4.41-4ubuntu3.10) ...
Setting up perl (5.30.0-9ubuntu0.2) ...
Setting up apache2-bin (2.4.41-4ubuntu3.10) ...
Setting up apache2 (2.4.41-4ubuntu3.10) ...
Enabling module mpm_event.
Enabling module authz_core.
Enabling module authz_host.
Enabling module authn_core.
Enabling module auth_basic.
Enabling module access_compat.
Enabling module authn_file.
Enabling module authz_user.
Enabling module alias.
Enabling module dir.
Enabling module autoindex.
Enabling module env.
Enabling module mime.
Enabling module negotiation.
Enabling module setenvif.
Enabling module filter.
Enabling module deflate.
Enabling module status.
Enabling module reqtimeout.
Enabling conf charset.
Enabling conf localized-error-pages.
Enabling conf other-vhosts-access-log.
Enabling conf security.
Enabling conf serve-cgi-bin.
Enabling site 000-default.
invoke-rc.d: could not determine current runlevel
invoke-rc.d: policy-rc.d denied execution of start.
Processing triggers for libc-bin (2.31-0ubuntu9.7) ...
# 
# 
# 
# cd /etc/apache2
# 
# ls -ahl
total 88K
drwxr-xr-x  8 root root 4.0K Mar 18 20:46 .
drwxr-xr-x 38 root root 4.0K Mar 18 20:46 ..
-rw-r--r--  1 root root 7.1K Mar 16 08:52 apache2.conf
drwxr-xr-x  2 root root 4.0K Mar 18 20:46 conf-available
drwxr-xr-x  2 root root 4.0K Mar 18 20:46 conf-enabled
-rw-r--r--  1 root root 1.8K Sep 30  2020 envvars
-rw-r--r--  1 root root  31K Sep 30  2020 magic
drwxr-xr-x  2 root root  12K Mar 18 20:46 mods-available
drwxr-xr-x  2 root root 4.0K Mar 18 20:46 mods-enabled
-rw-r--r--  1 root root  320 Sep 30  2020 ports.conf
drwxr-xr-x  2 root root 4.0K Mar 18 20:46 sites-available
drwxr-xr-x  2 root root 4.0K Mar 18 20:46 sites-enabled
# vi ports.conf
/bin/sh: 10: vi: not found
# 
# which sed
/usr/bin/sed
# 
# 
# sed -i 's/Listen 80/Listen 5001/g' ports.conf
# 
# sed -i 's/:80/:5001/g' apache2.conf
# 
# 
#  sed -i 's/#ServerName www.example.com/ServerName localhost/g' apache2.conf
# 
# 
# cat ports.conf
# If you just change the port or add more ports here, you will likely also
# have to change the VirtualHost statement in
# /etc/apache2/sites-enabled/000-default.conf

Listen 5001

<IfModule ssl_module>
        Listen 443
</IfModule>

<IfModule mod_gnutls.c>
        Listen 443
</IfModule>

# vim: syntax=apache ts=4 sw=4 sts=4 sr noet
# 
# 
# 
# cat apache2.conf
# This is the main Apache server configuration file.  It contains the
# configuration directives that give the server its instructions.
# See http://httpd.apache.org/docs/2.4/ for detailed information about
# the directives and /usr/share/doc/apache2/README.Debian about Debian specific
# hints.
#
#
# Summary of how the Apache 2 configuration works in Debian:
# The Apache 2 web server configuration in Debian is quite different to
# upstream's suggested way to configure the web server. This is because Debian's
# default Apache2 installation attempts to make adding and removing modules,
# virtual hosts, and extra configuration directives as flexible as possible, in
# order to make automating the changes and administering the server as easy as
# possible.

# It is split into several files forming the configuration hierarchy outlined
# below, all located in the /etc/apache2/ directory:
#
#       /etc/apache2/
#       |-- apache2.conf
#       |       `--  ports.conf
#       |-- mods-enabled
#       |       |-- *.load
#       |       `-- *.conf
#       |-- conf-enabled
#       |       `-- *.conf
#       `-- sites-enabled
#               `-- *.conf
#
#
# * apache2.conf is the main configuration file (this file). It puts the pieces
#   together by including all remaining configuration files when starting up the
#   web server.
#
# * ports.conf is always included from the main configuration file. It is
#   supposed to determine listening ports for incoming connections which can be
#   customized anytime.
#
# * Configuration files in the mods-enabled/, conf-enabled/ and sites-enabled/
#   directories contain particular configuration snippets which manage modules,
#   global configuration fragments, or virtual host configurations,
#   respectively.
#
#   They are activated by symlinking available configuration files from their
#   respective *-available/ counterparts. These should be managed by using our
#   helpers a2enmod/a2dismod, a2ensite/a2dissite and a2enconf/a2disconf. See
#   their respective man pages for detailed information.
#
# * The binary is called apache2. Due to the use of environment variables, in
#   the default configuration, apache2 needs to be started/stopped with
#   /etc/init.d/apache2 or apache2ctl. Calling /usr/bin/apache2 directly will not
#   work with the default configuration.


# Global configuration
#

#
# ServerRoot: The top of the directory tree under which the server's
# configuration, error, and log files are kept.
#
# NOTE!  If you intend to place this on an NFS (or otherwise network)
# mounted filesystem then please read the Mutex documentation (available
# at <URL:http://httpd.apache.org/docs/2.4/mod/core.html#mutex>);
# you will save yourself a lot of trouble.
#
# Do NOT add a slash at the end of the directory path.
#
#ServerRoot "/etc/apache2"

#
# The accept serialization lock file MUST BE STORED ON A LOCAL DISK.
#
#Mutex file:${APACHE_LOCK_DIR} default

#
# The directory where shm and other runtime files will be stored.
#

DefaultRuntimeDir ${APACHE_RUN_DIR}

#
# PidFile: The file in which the server should record its process
# identification number when it starts.
# This needs to be set in /etc/apache2/envvars
#
PidFile ${APACHE_PID_FILE}

#
# Timeout: The number of seconds before receives and sends time out.
#
Timeout 300

#
# KeepAlive: Whether or not to allow persistent connections (more than
# one request per connection). Set to "Off" to deactivate.
#
KeepAlive On

#
# MaxKeepAliveRequests: The maximum number of requests to allow
# during a persistent connection. Set to 0 to allow an unlimited amount.
# We recommend you leave this number high, for maximum performance.
#
MaxKeepAliveRequests 100

#
# KeepAliveTimeout: Number of seconds to wait for the next request from the
# same client on the same connection.
#
KeepAliveTimeout 5


# These need to be set in /etc/apache2/envvars
User ${APACHE_RUN_USER}
Group ${APACHE_RUN_GROUP}

#
# HostnameLookups: Log the names of clients or just their IP addresses
# e.g., www.apache.org (on) or 204.62.129.132 (off).
# The default is off because it'd be overall better for the net if people
# had to knowingly turn this feature on, since enabling it means that
# each client request will result in AT LEAST one lookup request to the
# nameserver.
#
HostnameLookups Off

# ErrorLog: The location of the error log file.
# If you do not specify an ErrorLog directive within a <VirtualHost>
# container, error messages relating to that virtual host will be
# logged here.  If you *do* define an error logfile for a <VirtualHost>
# container, that host's errors will be logged there and not here.
#
ErrorLog ${APACHE_LOG_DIR}/error.log

#
# LogLevel: Control the severity of messages logged to the error_log.
# Available values: trace8, ..., trace1, debug, info, notice, warn,
# error, crit, alert, emerg.
# It is also possible to configure the log level for particular modules, e.g.
# "LogLevel info ssl:warn"
#
LogLevel warn

# Include module configuration:
IncludeOptional mods-enabled/*.load
IncludeOptional mods-enabled/*.conf

# Include list of ports to listen on
Include ports.conf


# Sets the default security model of the Apache2 HTTPD server. It does
# not allow access to the root filesystem outside of /usr/share and /var/www.
# The former is used by web applications packaged in Debian,
# the latter may be used for local directories served by the web server. If
# your system is serving content from a sub-directory in /srv you must allow
# access here, or in any related virtual host.
<Directory />
        Options FollowSymLinks
        AllowOverride None
        Require all denied
</Directory>

<Directory /usr/share>
        AllowOverride None
        Require all granted
</Directory>

<Directory /var/www/>
        Options Indexes FollowSymLinks
        AllowOverride None
        Require all granted
</Directory>

#<Directory /srv/>
#       Options Indexes FollowSymLinks
#       AllowOverride None
#       Require all granted
#</Directory>




# AccessFileName: The name of the file to look for in each directory
# for additional configuration directives.  See also the AllowOverride
# directive.
#
AccessFileName .htaccess

#
# The following lines prevent .htaccess and .htpasswd files from being
# viewed by Web clients.
#
<FilesMatch "^\.ht">
        Require all denied
</FilesMatch>


#
# The following directives define some format nicknames for use with
# a CustomLog directive.
#
# These deviate from the Common Log Format definitions in that they use %O
# (the actual bytes sent including headers) instead of %b (the size of the
# requested file), because the latter makes it impossible to detect partial
# requests.
#
# Note that the use of %{X-Forwarded-For}i instead of %h is not recommended.
# Use mod_remoteip instead.
#
LogFormat "%v:%p %h %l %u %t \"%r\" %>s %O \"%{Referer}i\" \"%{User-Agent}i\"" vhost_combined
LogFormat "%h %l %u %t \"%r\" %>s %O \"%{Referer}i\" \"%{User-Agent}i\"" combined
LogFormat "%h %l %u %t \"%r\" %>s %O" common
LogFormat "%{Referer}i -> %U" referer
LogFormat "%{User-agent}i" agent

# Include of directories ignores editors' and dpkg's backup files,
# see README.Debian for details.

# Include generic snippets of statements
IncludeOptional conf-enabled/*.conf

# Include the virtual host configurations:
IncludeOptional sites-enabled/*.conf

# vim: syntax=apache ts=4 sw=4 sts=4 sr noet
# 
# 
# 
# service apache2 start
 * Starting Apache httpd web server apache2                                                                                                  AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 192.168.3.2. Set the 'ServerName' directive globally to suppress this message
 * 
# 
# 
# 
# service apache2 status
 * apache2 is running
# 
# 
# 
# curl -Ik localhost:5001
HTTP/1.1 200 OK
Date: Sat, 19 Mar 2022 04:50:56 GMT
Server: Apache/2.4.41 (Ubuntu)
Last-Modified: Sat, 19 Mar 2022 04:46:14 GMT
ETag: "2aa6-5da8af170b5f3"
Accept-Ranges: bytes
Content-Length: 10918
Vary: Accept-Encoding
Content-Type: text/html

--------------------------------------------------------------------------------------------------------------------------------------------
Task 47 : 20/Mar/2022
Kubernetes Shared Volumes

We are working on an application that will be deployed on multiple containers within a pod on Kubernetes cluster. There is a requirement to share a volume among the containers to save some temporary data. The Nautilus DevOps team is developing a similar template to replicate the scenario. Below you can find more details about it.

    Create a pod named volume-share-devops.

    For the first container, use image centos with latest tag only and remember to mention the tag i.e centos:latest, container should be named as volume-container-devops-1, and run a sleep command for it so that it remains in running state. Volume volume-share should be mounted at path /tmp/media.

    For the second container, use image centos with the latest tag only and remember to mention the tag i.e centos:latest, container should be named as volume-container-devops-2, and again run a sleep command for it so that it remains in running state. Volume volume-share should be mounted at path /tmp/cluster.

    Volume name should be volume-share of type emptyDir.

    After creating the pod, exec into the first container i.e volume-container-devops-1, and just for testing create a file media.txt with any content under the mounted path of first container i.e /tmp/media.

    The file media.txt should be present under the mounted path /tmp/cluster on the second container volume-container-devops-2 as well, since they are using a shared volume.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

1.
kubectl get services
	NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
	kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   13m

2.
kubectl get pods
	No resources found in default namespace.

3.
vi /tmp/volume.yaml
		apiVersion: v1
		kind: Pod
		metadata:
		  name: volume-share-devops
		  labels:
		    name: myapp
		spec:
		  volumes:
		    - name: volume-share
		      emptyDir: {}
		  containers:
		    - name: volume-container-devops-1
		      image: centos:latest
		      command: ["/bin/bash", "-c", "sleep 10000"]
		      volumeMounts:
		        - name: volume-share
		          mountPath: /tmp/media
		    - name: volume-container-devops-2
		      image: centos:latest
		      command: ["/bin/bash", "-c", "sleep 10000"]
		      volumeMounts:
		        - name: volume-share
		          mountPath: /tmp/cluster

4.
kubectl create -f /tmp/volume.yaml 
	pod/volume-share-devops created

5.
kubectl get pods
	NAME                  READY   STATUS              RESTARTS   AGE
	volume-share-devops   0/2     ContainerCreating   0          7s
6.
kubectl get pods
	NAME                  READY   STATUS    RESTARTS   AGE
	volume-share-devops   2/2     Running   0          26s

7.
kubectl get pods -o wide
	NAME                  READY   STATUS    RESTARTS   AGE   IP           NODE                      NOMINATED NODE   READINESS GATES
	volume-share-devops   2/2     Running   0          40s   10.244.0.5   kodekloud-control-plane   <none>           <none>

8.
kubectl exec -it volume-share-devops -c volume-container-devops-1 -- /bin/bash
	[root@volume-share-devops /]# 
	[root@volume-share-devops /]# echo "Welcome to xFusionCorps!" > /tmp/media/media.txt
	[root@volume-share-devops /]# 
	[root@volume-share-devops /]# cat /tmp/media/media.txt 
		Welcome to xFusionCorps!
	[root@volume-share-devops /]# 
	[root@volume-share-devops /]# exit
exit

9.
kubectl exec -it volume-share-devops -c volume-container-devops-2 -- ls /tmp/cluster
	media.txt

--------------------------------------------------------------------------------------------------------------------------------------------
Task 48 : 22/Mar/2022
ReplicationController in Kubernetes

The Nautilus DevOps team wants to create a ReplicationController to deploy several pods. They are going to deploy applications on these pods, these applications need highly available infrastructure. Below you can find exact details, create the ReplicationController accordingly.

    Create a ReplicationController using nginx image, preferably with latest tag, and name it as nginx-replicationcontroller.

    Labels app should be nginx_app, and labels type should be front-end. The container should be named as nginx-container and also make sure replica counts are 3.

All pods should be running state after deployment.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

thor@jump_host ~$ kubectl get namespace
NAME                 STATUS   AGE
default              Active   62m
kube-node-lease      Active   62m
kube-public          Active   62m
kube-system          Active   62m
local-path-storage   Active   62m
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
No resources found in default namespace.
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ vi /tmp/replica.yml
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ cat /tmp/replica.yml 
apiVersion: v1

kind: ReplicationController

metadata:

  name: nginx-replicationcontroller

  labels:

    app: nginx_app

    type: front-end

spec:

  replicas: 3

  selector:

    app: nginx_app

  template:

    metadata:

      name: nginx_pod

      labels:

        app: nginx_app

        type: front-end

    spec:

      containers:

        - name: nginx-container

          image: nginx:latest

          ports:

            - containerPort: 80 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl create -f /tmp/replica.yml 
replicationcontroller/nginx-replicationcontroller created
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
NAME                                READY   STATUS              RESTARTS   AGE
nginx-replicationcontroller-6tk8s   0/1     ContainerCreating   0          17s
nginx-replicationcontroller-8sfw5   1/1     Running             0          17s
nginx-replicationcontroller-959vx   1/1     Running             0          17s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
nginx-replicationcontroller-6tk8s   1/1     Running   0          22s
nginx-replicationcontroller-8sfw5   1/1     Running   0          22s
nginx-replicationcontroller-959vx   1/1     Running   0          22s
thor@jump_host ~$ kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
nginx-replicationcontroller-6tk8s   1/1     Running   0          24s
nginx-replicationcontroller-8sfw5   1/1     Running   0          24s
nginx-replicationcontroller-959vx   1/1     Running   0          24s
thor@jump_host ~$ 
thor@jump_host ~$ kubectl exec nginx-replicationcontroller-6tk8s -- curl http://localhost
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
100   615  100   615    0     0   600k      0 --:--:-- --:--:-- --:--:--  600k

--------------------------------------------------------------------------------------------------------------------------------------------
Task 49 : 24/Mar/2022
Git Revert Some Changes

The Nautilus application development team was working on a git repository /usr/src/kodekloudrepos/beta present on Storage server in Stratos DC. However, they reported an issue with the recent commits being pushed to this repo. They have asked the DevOps team to revert repo HEAD to last commit. Below are more details about the task:

    In /usr/src/kodekloudrepos/beta git repository, revert the latest commit ( HEAD ) to the previous commit (JFYI the previous commit hash should be with initial commit message ).

    Use revert beta message (please use all small letters for commit message) for the new revert commit.


thor@jump_host ~$ ssh natasha@ststor01
The authenticity of host 'ststor01 (172.16.238.15)' can't be established.
ECDSA key fingerprint is SHA256:T7UXzHW+LeEeNkv73OTPOhek8Of2LkVCFEhN9lBPkfE.
ECDSA key fingerprint is MD5:b5:54:33:9f:35:0d:a8:92:25:d0:bb:db:e0:28:da:62.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'ststor01,172.16.238.15' (ECDSA) to the list of known hosts.
natasha@ststor01's password: 
[natasha@ststor01 ~]$ sudo su - 

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for natasha: 
[root@ststor01 ~]# 
[root@ststor01 ~]# 
[root@ststor01 ~]# 
[root@ststor01 ~]# cd /usr/src/kodekloudrepos/beta/
[root@ststor01 beta]# ls -ahl
total 16K
drwxr-xr-x 3 root root 4.0K Mar 24 07:33 .
drwxr-xr-x 3 root root 4.0K Mar 24 07:33 ..
-rw-r--r-- 1 root root   33 Mar 24 07:33 beta.txt
drwxr-xr-x 8 root root 4.0K Mar 24 07:33 .git
[root@ststor01 beta]# git status
# On branch master
# Untracked files:
#   (use "git add <file>..." to include in what will be committed)
#
#       beta.txt
nothing added to commit but untracked files present (use "git add" to track)
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# git log
commit cd69d88c6e045317a72558dbe8d0317cbd746a3f
Author: Admin <admin@kodekloud.com>
Date:   Thu Mar 24 07:33:32 2022 +0000

    add data.txt file

commit 5081b7a926a8f8eac484fe8a1ebe38dda72c0aec
Author: Admin <admin@kodekloud.com>
Date:   Thu Mar 24 07:33:32 2022 +0000

    initial commit
[root@ststor01 beta]# git revert HEAD
[master 5043e35] Revert "add data.txt file"
 1 file changed, 1 insertion(+)
 create mode 100644 info.txt
[root@ststor01 beta]# git add .
[root@ststor01 beta]# git commit -m "revert beta"
[master 5a846d4] revert beta
 1 file changed, 1 insertion(+)
 create mode 100644 beta.txt
[root@ststor01 beta]# 
[root@ststor01 beta]# git log
commit 5a846d477761f6238c832d9c7fc3f033c9802c7b
Author: Admin <admin@kodekloud.com>
Date:   Thu Mar 24 07:38:00 2022 +0000

    revert beta

commit 5043e350cffee736b097bd2bbbbf4423b1658221
Author: Admin <admin@kodekloud.com>
Date:   Thu Mar 24 07:35:52 2022 +0000

    Revert "add data.txt file"
    
    This reverts commit cd69d88c6e045317a72558dbe8d0317cbd746a3f.

commit cd69d88c6e045317a72558dbe8d0317cbd746a3f
Author: Admin <admin@kodekloud.com>
Date:   Thu Mar 24 07:33:32 2022 +0000

    add data.txt file

commit 5081b7a926a8f8eac484fe8a1ebe38dda72c0aec
Author: Admin <admin@kodekloud.com>
Date:   Thu Mar 24 07:33:32 2022 +0000

    initial commit
[root@ststor01 beta]# 

--------------------------------------------------------------------------------------------------------------------------------------------
Task 50 : 25/Mar/2022

Ansible Facts Gathering
The Nautilus DevOps team is trying to setup a simple Apache web server on all app servers in Stratos DC using Ansible. They also want to create a sample html page for now with some app specific data on it. Below you can find more details about the task.

You will find a valid inventory file /home/thor/playbooks/inventory on jump host (which we are using as an Ansible controller).

    Create a playbook index.yml under /home/thor/playbooks directory on jump host. Using blockinfile Ansible module create a file facts.txt under /root directory on all app servers and add the following given block in it. You will need to enable facts gathering for this task.

Ansible managed node IP is <default ipv4 address>

(You can obtain default ipv4 address from Ansible's gathered facts by using the correct Ansible variable while taking into account Jinja2 syntax)

    Install httpd server on all apps. After that make a copy of facts.txt file as index.html under /var/www/html directory. Make sure to start httpd service after that.

Note: Do not create a separate role for this task, just add all of the changes in index.yml playbook.

thor@jump_host ~$ cd /home/thor/playbooks/
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ ls -ahl
total 16K
drwxr-xr-x 2 thor thor 4.0K Mar 25 11:16 .
drwxr----- 1 thor thor 4.0K Mar 25 11:16 ..
-rw-r--r-- 1 thor thor   36 Mar 25 11:16 ansible.cfg
-rw-r--r-- 1 thor thor  237 Mar 25 11:16 inventory
thor@jump_host ~/playbooks$ cat inventory 
stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n ansible_user=tony
stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@ ansible_user=steve
stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n ansible_user=bannerthor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ vi index.yml
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ cat index.yml 
---
-
  hosts: stapp01, stapp02, stapp03
  gather_facts: true
  become: yes
  become_method: sudo
  tasks:
    - name: create a  file using blockinfile
      blockinfile:
       create: yes
       path: /root/facts.txt
       block: |
         Ansible managed node IP is {{ ansible_default_ipv4.address }}

    - name: Install apache packages
      package:
       name: httpd

    - name: file copy
      shell: cp /root/facts.txt /var/www/html/index.html

    - name: ensure httpd is running
      systemd:
       name: httpd
       state: restarted
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ ansible-playbook -i inventory index.yml 

PLAY [stapp01, stapp02, stapp03] ************************************************************************************************************

TASK [Gathering Facts] **********************************************************************************************************************
ok: [stapp02]
ok: [stapp01]
ok: [stapp03]

TASK [create a  file using blockinfile] *****************************************************************************************************
changed: [stapp02]
changed: [stapp01]
changed: [stapp03]

TASK [Install apache packages] **************************************************************************************************************
changed: [stapp01]
changed: [stapp03]
changed: [stapp02]

TASK [file copy] ****************************************************************************************************************************
changed: [stapp02]
changed: [stapp03]
changed: [stapp01]

TASK [ensure httpd is running] **************************************************************************************************************
changed: [stapp01]
changed: [stapp02]
changed: [stapp03]

PLAY RECAP **********************************************************************************************************************************
stapp01                    : ok=5    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
stapp02                    : ok=5    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
stapp03                    : ok=5    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ curl -i http://stapp01
HTTP/1.1 200 OK
Date: Fri, 25 Mar 2022 11:22:09 GMT
Server: Apache/2.4.6 (CentOS)
Last-Modified: Fri, 25 Mar 2022 11:21:53 GMT
ETag: "63-5db092b69f1bb"
Accept-Ranges: bytes
Content-Length: 99
Content-Type: text/html; charset=UTF-8

# BEGIN ANSIBLE MANAGED BLOCK
Ansible managed node IP is 172.16.238.10
# END ANSIBLE MANAGED BLOCK
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ curl http://stapp01
# BEGIN ANSIBLE MANAGED BLOCK
Ansible managed node IP is 172.16.238.10
# END ANSIBLE MANAGED BLOCK
thor@jump_host ~/playbooks$ curl http://stapp02
# BEGIN ANSIBLE MANAGED BLOCK
Ansible managed node IP is 172.16.238.11
# END ANSIBLE MANAGED BLOCK
thor@jump_host ~/playbooks$ curl http://stapp03
# BEGIN ANSIBLE MANAGED BLOCK
Ansible managed node IP is 172.16.238.12
# END ANSIBLE MANAGED BLOCK
thor@jump_host ~/playbooks$ 
thor@jump_host ~/playbooks$ 

--------------------------------------------------------------------------------------------------------------------------------------------
Task 51 : 9/Apr/2022
Deploy Guest Book App on Kubernetes

The Nautilus Application development team has finished development of one of the applications and it is ready for deployment. It is a guestbook application that will be used to manage entries for guests/visitors. As per discussion with the DevOps team, they have finalized the infrastructure that will be deployed on Kubernetes cluster. Below you can find more details about it.

BACK-END TIER

    Create a deployment named redis-master for Redis master.

    a.) Replicas count should be 1.

    b.) Container name should be master-redis-xfusion and it should use image redis.

    c.) Request resources as CPU should be 100m and Memory should be 100Mi.

    d.) Container port should be redis default port i.e 6379.

    Create a service named redis-master for Redis master. Port and targetPort should be Redis default port i.e 6379.

    Create another deployment named redis-slave for Redis slave.

    a.) Replicas count should be 2.

    b.) Container name should be slave-redis-xfusion and it should use gcr.io/google_samples/gb-redisslave:v3 image.

    c.) Requests resources as CPU should be 100m and Memory should be 100Mi.

    d.) Define an environment variable named GET_HOSTS_FROM and its value should be dns.

    e.) Container port should be Redis default port i.e 6379.

    Create another service named redis-slave. It should use Redis default port i.e 6379.

FRONT END TIER

    Create a deployment named frontend.

    a.) Replicas count should be 3.

    b.) Container name should be php-redis-xfusion and it should use gcr.io/google-samples/gb-frontend:v4 image.

    c.) Request resources as CPU should be 100m and Memory should be 100Mi.

    d.) Define an environment variable named as GET_HOSTS_FROM and its value should be dns.

    e.) Container port should be 80.

    Create a service named frontend. Its type should be NodePort, port should be 80 and its nodePort should be 30009.

Finally, you can check the guestbook app by clicking on + button in the top left corner and Select port to view on Host 1 then enter your nodePort.

You can use any labels as per your choice.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

1.
kubectl get deploy
	No resources found in default namespace.

2.
kubectl get pods
	No resources found in default namespace.

3.
kubectl get service
	NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
	kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4h13m

4.
cd /tmp

5.
vi Back-end-Deploy-Redis-Master-Guest-Book-App.yaml
		---
		apiVersion: apps/v1
		kind: Deployment
		metadata:
		  name: redis-master
		spec:
		  replicas: 1
		  selector:
		    matchLabels:
		      app: redis-master
		      tier: back-end
		  template:
		    metadata:
		      labels:
		        app: redis-master
		        tier: back-end
		    spec:
		      containers:
		        - name: master-redis-xfusion
		          image: redis
		          resources:
		            requests:
		              memory: "100Mi"
		              cpu: "100m"
		          ports:
		            - containerPort: 6379


6.
vi Back-end-Deploy-Redis-slave-Guest-Book-App.yaml
		---
		apiVersion: apps/v1
		kind: Deployment
		metadata:
		  name: redis-slave
		spec:
		  replicas: 2
		  selector:
		    matchLabels:
		      app: redis-slave
		      tier: back-end
		  template:
		    metadata:
		      labels:
		        app: redis-slave
		        tier: back-end
		    spec:
		      containers:
		        - name: slave-redis-xfusion
		          image: gcr.io/google_samples/gb-redisslave:v3
		          resources:
		            requests:
		              memory: "100Mi"
		              cpu: "100m"
		          env:
		            - name: GET_HOSTS_FROM
		              value: dns
		          ports:
		            - containerPort: 6379

7.
vi Back-end-service-Redis-Master-Guest-Book-App.yaml
		---
		apiVersion: v1
		kind: Service
		metadata:
		  name: redis-master
		spec:
		  type: ClusterIP
		  selector:
		    app: redis-master
		    tier: back-end
		  ports:
		    - port: 6379
		      targetPort: 6379

8.
vi Back-end-service-Redis-slave-Guest-Book-App.yaml
		---
		apiVersion: v1
		kind: Service
		metadata:
		  name: redis-slave
		spec:
		  type: ClusterIP
		  selector:
		    app: redis-slave
		    tier: back-end
		  ports:
		    - port: 6379
		      targetPort: 6379

9.
vi Front-end-Deploy-Redis-php-Guest-Book-App.yaml
		---
		apiVersion: apps/v1
		kind: Deployment
		metadata:
		  name: frontend
		spec:
		  replicas: 3
		  selector:
		    matchLabels:
		      app: guestbook
		      tier: front-end
		  template:
		    metadata:
		      labels:
		        app: guestbook
		        tier: front-end
		    spec:
		      containers:
		        - name: php-redis-xfusion
		          image: gcr.io/google-samples/gb-frontend:v4
		          resources:
		            requests:
		              memory: "100Mi"
		              cpu: "100m"
		          env:
		            - name: GET_HOSTS_FROM
		              value: dns
		          ports:
		            - containerPort: 80

10.
vi Front-end-service-Redis-php-Guest-Book-App.yaml
		---
		apiVersion: v1
		kind: Service
		metadata:
		  name: frontend
		spec:
		  type: NodePort
		  selector:
		    app: guestbook
		    tier: front-end
		  ports:
		    - port: 80
		      targetPort: 80
		      nodePort: 30009

11.Create Backend Master deployment

kubectl apply -f Back-end-Deploy-Redis-Master-Guest-Book-App.yaml
	deployment.apps/redis-master created

12.Create Backend  Master service 

kubectl apply -f Back-end-service-Redis-Master-Guest-Book-App.yaml
	service/redis-master created

13.Create Backend slave deployment

kubectl apply -f Back-end-Deploy-Redis-slave-Guest-Book-App.yaml
	deployment.apps/redis-slave created

14.Create Backend slave service 

kubectl apply -f Back-end-service-Redis-slave-Guest-Book-App.yaml
	service/redis-slave created

15.Create frontend deployment

kubectl apply -f Front-end-Deploy-Redis-php-Guest-Book-App.yaml
	deployment.apps/frontend created	 

16.Create front-end-service 

kubectl apply -f Front-end-service-Redis-php-Guest-Book-App.yaml
	service/frontend created


17.
 kubectl get deploy
	NAME           READY   UP-TO-DATE   AVAILABLE   AGE
	frontend       0/3     3            0           30s
	redis-master   1/1     1            1           88s
	redis-slave    2/2     2            2           59s

18.
kubectl get pods
	NAME                           READY   STATUS    RESTARTS   AGE
	frontend-586bdcd8bb-hkqm6      1/1     Running   0          118s
	frontend-586bdcd8bb-nts2c      1/1     Running   0          118s
	frontend-586bdcd8bb-skpw2      1/1     Running   0          118s
	redis-master-fd5fb5746-7qmw4   1/1     Running   0          2m55s
	redis-slave-8b57b5779-7pgc4    1/1     Running   0          2m27s
	redis-slave-8b57b5779-92szx    1/1     Running   0          2m27s

19.
 kubectl get service
	NAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
	frontend       NodePort    10.96.79.21     <none>        80:30009/TCP   30s
	kubernetes     ClusterIP   10.96.0.1       <none>        443/TCP        4h20m
	redis-master   ClusterIP   10.96.104.169   <none>        6379/TCP       92s
	redis-slave    ClusterIP   10.96.203.200   <none>        6379/TCP       65s

20.
 kubectl exec frontend-586bdcd8bb-hkqm6 -- curl -I http://localhost/
	  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
	                                 Dload  Upload   Total   Spent    Left  Speed
	  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0HTTP/1.1 200 OK
	Date: Sat, 09 Apr 2022 10:36:14 GMT
	Server: Apache/2.4.10 (Debian) PHP/5.6.20
	Last-Modified: Wed, 09 Sep 2015 18:35:04 GMT
	ETag: "399-51f54bdb4a600"
	Accept-Ranges: bytes
	Content-Length: 921
	Vary: Accept-Encoding
	Content-Type: text/html

	  0   921    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0

21. View Port <Advanced setting> : 30009
--------------------------------------------------------------------------------------------------------------------------------------------
Task 52 : 15/Apr/2022
Puppet Setup Database

The Nautilus DevOps team had a meeting with development team last week to discuss about some new requirements for an application deployment. Team is working on to setup a mariadb database server on Nautilus DB Server in Stratos Datacenter. They want to setup the same using Puppet. Below you can find more details about the requirements:

Create a puppet programming file news.pp under /etc/puppetlabs/code/environments/production/manifests directory on puppet master node i.e on Jump Server. Define a class mysql_database in puppet programming code and perform below mentioned tasks:

    Install package mariadb-server (whichever version is available by default in yum repo) on puppet agent node i.e on DB Server also start its service.

    Create a database kodekloud_db1 , a database userkodekloud_tim and set passwordTmPcZjtRQx for this new user also remember host should be localhost. Finally grant some usual permissions like select, update (or full) ect to this newly created user on newly created database.

Notes: :- Please make sure to run the puppet agent test using sudo on agent nodes, otherwise you can face certificate issues. In that case you will have to clean the certificates first and then you will be able to run the puppet agent test.

:- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.


1. Login to root user on jump host and go to mention manifest directory
	root@jump_host ~# cd /etc/puppetlabs/code/environments/production/manifests/
	root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
	root@jump_host /etc/puppetlabs/code/environments/production/manifests# ls -ahl
		total 8.0K
		drwxr-xr-x 1 puppet puppet 4.0K Jul 13  2021 .
		drwxr-xr-x 1 puppet puppet 4.0K Aug  9  2021 ..

2. Create the puppet manifest file and verify 
	root@jump_host /etc/puppetlabs/code/environments/production/manifests# vi news.pp
	 
	root@jump_host /etc/puppetlabs/code/environments/production/manifests# cat news.pp 
		class mysql_database {
		    package {'mariadb-server':
		      ensure => installed
		    }

		    service {'mariadb':
			ensure    => running,
			enable    => true,
		    }    

		    mysql::db { 'kodekloud_db1':
		      user     => 'kodekloud_tim',
		      password => 'TmPcZjtRQx',
		      host     => 'localhost',
		      grant    => ['ALL'],
		    }
		}

		node 'stdb01.stratos.xfusioncorp.com' {
		  include mysql_database
		}

3. Validate the news.pp file
	root@jump_host /etc/puppetlabs/code/environments/production/manifests# puppet parser validate news.pp
	root@jump_host /etc/puppetlabs/code/environments/production/manifests# 

4. Login to DB server , switch to root and puppet run test agent
	root@jump_host /etc/puppetlabs/code/environments/production/manifests# ssh peter@stdb01
	The authenticity of host 'stdb01 (172.16.239.10)' can't be established.
	ECDSA key fingerprint is SHA256:XEEWbBLqOUWSgyQ/M3FI1iWhqdu3LZCE0A3b5AdgHVc.
	ECDSA key fingerprint is MD5:42:dc:82:00:61:4f:8f:33:ff:b5:d1:cd:0a:da:1c:b7.
	Are you sure you want to continue connecting (yes/no)? yes
	Warning: Permanently added 'stdb01,172.16.239.10' (ECDSA) to the list of known hosts.
	peter@stdb01's password: 
	
	[peter@stdb01 ~]$ sudo su -

	We trust you have received the usual lecture from the local System
	Administrator. It usually boils down to these three things:

	    #1) Respect the privacy of others.
	    #2) Think before you type.
	    #3) With great power comes great responsibility.

	[sudo] password for peter: 
	[root@stdb01 ~]# 

	[root@stdb01 ~]# puppet agent -tv
		Info: Using configured environment 'production'
		Info: Retrieving pluginfacts
		Info: Retrieving plugin
		Info: Retrieving locales
		Info: Loading facts
		Info: Caching catalog for stdb01.stratos.xfusioncorp.com
		Info: Applying configuration version '1650080109'
		Notice: Applied catalog in 0.57 seconds

5. Check mariadb running status 
	[root@stdb01 ~]# systemctl status mariadb
	● mariadb.service - MariaDB database server
	   Loaded: loaded (/usr/lib/systemd/system/mariadb.service; enabled; vendor preset: disabled)
	   Active: active (running) since Sat 2022-04-16 03:34:02 UTC; 1min 29s ago
	 Main PID: 1393 (mysqld_safe)
	   CGroup: /docker/1671009373eeb7a708dd181a7f2be1c35d33621d36c3858b5f7f9ac27f62e2c3/system.slice/mariadb.service
		   ├─1393 /bin/sh /usr/bin/mysqld_safe --basedir=/usr
		   └─1557 /usr/libexec/mysqld --basedir=/usr --datadir=/var/lib/mysql --plugin-dir=/usr/lib64/mysql/plugin --log-error=/var/log/ma...

	Apr 16 03:34:00 stdb01.stratos.xfusioncorp.com systemd[1393]: Executing: /usr/bin/mysqld_safe --basedir=/usr
	Apr 16 03:34:00 stdb01.stratos.xfusioncorp.com systemd[1394]: Executing: /usr/libexec/mariadb-wait-ready 1393
	Apr 16 03:34:00 stdb01.stratos.xfusioncorp.com mysqld_safe[1393]: 220416 03:34:00 mysqld_safe Logging to '/var/log/mariadb/mariadb.log'.
	Apr 16 03:34:00 stdb01.stratos.xfusioncorp.com mysqld_safe[1393]: 220416 03:34:00 mysqld_safe Starting mysqld daemon with databases f...mysql
	Apr 16 03:34:02 stdb01.stratos.xfusioncorp.com systemd[1]: Child 1394 belongs to mariadb.service
	Apr 16 03:34:02 stdb01.stratos.xfusioncorp.com systemd[1]: mariadb.service: control process exited, code=exited status=0
	Apr 16 03:34:02 stdb01.stratos.xfusioncorp.com systemd[1]: mariadb.service got final SIGCHLD for state start-post
	Apr 16 03:34:02 stdb01.stratos.xfusioncorp.com systemd[1]: mariadb.service changed start-post -> running
	Apr 16 03:34:02 stdb01.stratos.xfusioncorp.com systemd[1]: Job mariadb.service/start finished, result=done
	Apr 16 03:34:02 stdb01.stratos.xfusioncorp.com systemd[1]: Started MariaDB database server.
	Hint: Some lines were ellipsized, use -l to show in full.

6. Login to DB server using give n user credentials
	[root@stdb01 ~]# mysql -u kodekloud_tim -p kodekloud_db1 -h localhost
		Enter password: 
		Welcome to the MariaDB monitor.  Commands end with ; or \g.
		Your MariaDB connection id is 50
		Server version: 5.5.68-MariaDB MariaDB Server

		Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

		Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

		MariaDB [kodekloud_db1]> 
		MariaDB [kodekloud_db1]> 
		MariaDB [kodekloud_db1]> :
		    -> ;
		ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near ':' at line 1
		MariaDB [kodekloud_db1]> ;
		ERROR: No query specified

		MariaDB [kodekloud_db1]> select * from kodekloud_db1;
		ERROR 1146 (42S02): Table 'kodekloud_db1.kodekloud_db1' doesn't exist
		MariaDB [kodekloud_db1]> exit
		Bye
	[root@stdb01 ~]# 

--------------------------------------------------------------------------------------------------------------------------------
Task 53 : 21/Apr/2022
Puppet Create Symlinks
Some directory structure in the Stratos Datacenter needs to be changed, there is a directory that needs to be linked to the default Apache document root. We need to accomplish this task using Puppet, as per the instructions given below:

Create a puppet programming file apps.pp under /etc/puppetlabs/code/environments/production/manifests directory on puppet master node i.e on Jump Server. Within that define a class symlink and perform below mentioned tasks:

    Create a symbolic link through puppet programming code. The source path should be /opt/security and destination path should be /var/www/html on Puppet agents 3 i.e on App Servers 3.

    Create a blank file blog.txt under /opt/security directory on puppet agent 3 nodes i.e on App Servers 3.

Notes: :- Please make sure to run the puppet agent test using sudo on agent nodes, otherwise you can face certificate issues. In that case you will have to clean the certificates first and then you will be able to run the puppet agent test.

:- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.

thor@jump_host ~$ sudo su -

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for thor: 
root@jump_host ~# 
root@jump_host ~# 
root@jump_host ~# 
root@jump_host ~# cd /etc/puppetlabs/code/environments/production/manifests
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# ls -ahl
total 8.0K
drwxr-xr-x 1 puppet puppet 4.0K Jul 13  2021 .
drwxr-xr-x 1 puppet puppet 4.0K Aug  9  2021 ..
root@jump_host /etc/puppetlabs/code/environments/production/manifests# vi apps.pp
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# cat apps.pp
class symlink {

  # First create a symlink to /var/www/html

  file { '/opt/security':

    ensure => 'link',

    target => '/var/www/html',

  }

   # Now create blog.txt under /opt/security

  file { '/opt/security/blog.txt':

    ensure => 'present',

  }

}

node 'stapp03.stratos.xfusioncorp.com' {

  include symlink

}
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# puppet parser validate apps.pp
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# ssh banner@stapp03
The authenticity of host 'stapp03 (172.16.238.12)' can't be established.
ECDSA key fingerprint is SHA256:0Nt2RVU4t8V7/rRWUyB+8tN7VvdIhGY2Gk4HUPWTQfM.
ECDSA key fingerprint is MD5:a9:29:ee:b1:f2:85:dd:f2:6e:1f:d2:bd:7d:81:da:8c.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'stapp03,172.16.238.12' (ECDSA) to the list of known hosts.
banner@stapp03's password: 
[banner@stapp03 ~]$ 
[banner@stapp03 ~]$ 
[banner@stapp03 ~]$ sudo su -

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for banner: 
[root@stapp03 ~]# 
[root@stapp03 ~]# 
[root@stapp03 ~]# 
[root@stapp03 ~]# puppet agent -tv
Info: Using configured environment 'production'
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Retrieving locales
Info: Caching catalog for stapp03.stratos.xfusioncorp.com
Info: Applying configuration version '1650526391'
Notice: /Stage[main]/Symlink/File[/opt/security]/ensure: created
Notice: /Stage[main]/Symlink/File[/opt/security/blog.txt]/ensure: created
Notice: Applied catalog in 0.04 seconds
[root@stapp03 ~]# ls -ahl /var/www/html
total 8.0K
drwxr-xr-x 2 root root 4.0K Apr 21 07:33 .
drwxr-xr-x 3 root root 4.0K Apr 21 07:25 ..
-rw-r--r-- 1 root root    0 Apr 21 07:33 blog.txt
[root@stapp03 ~]# 
[root@stapp03 ~]# ls -ahl /opt/security/
total 8.0K
drwxr-xr-x 2 root root 4.0K Apr 21 07:33 .
drwxr-xr-x 3 root root 4.0K Apr 21 07:25 ..
-rw-r--r-- 1 root root    0 Apr 21 07:33 blog.txt
[root@stapp03 ~]# ll /opt/security/
total 0
-rw-r--r-- 1 root root 0 Apr 21 07:33 blog.txt
[root@stapp03 ~]# 
[root@stapp03 ~]# 
--------------------------------------------------------------------------------------------------------------------------------
Task 54: 24/Apr/2022
Install Docker Package
Last week the Nautilus DevOps team met with the application development team and decided to containerize several of their applications. The DevOps team wants to do some testing per the following:

    Install docker-ce and docker-compose packages on App Server 3.

    Start docker service.

1. Login to App Server 3 and switch to root
 ssh banner@stapp03

 sudo su -

2. Fetch docker compose executable
# curl -L "https://github.com/docker/compose/releases/download/1.28.6/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
	  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
		                         Dload  Upload   Total   Spent    Left  Speed
	100   664  100   664    0     0   1646      0 --:--:-- --:--:-- --:--:--  1643
	100 11.6M  100 11.6M    0     0  12.2M      0 --:--:-- --:--:-- --:--:-- 12.2M

3. Provide executable permission to docker-compose 
# ls -ahl /usr/local/bin/docker-compose
	-rw-r--r-- 1 root root 12M Apr 24 05:37 /usr/local/bin/docker-compose

# chmod +x /usr/local/bin/docker-compose 

# ls -ahl /usr/local/bin/docker-compose
	-rwxr-xr-x 1 root root 12M Apr 24 05:37 /usr/local/bin/docker-compose

4. Validate instllation
# docker-compose --version
	docker-compose version 1.28.6, build 5db8d86f

5. Add docker repo for installation:
# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
	Loaded plugins: fastestmirror, ovl
	adding repo from: https://download.docker.com/linux/centos/docker-ce.repo
	grabbing file https://download.docker.com/linux/centos/docker-ce.repo to /etc/yum.repos.d/docker-ce.repo
	repo saved to /etc/yum.repos.d/docker-ce.repo

6. Install docker
# yum install docker-ce docker-ce-cli containerd-io
	Loaded plugins: fastestmirror, ovl
	Determining fastest mirrors
	 * base: mirrors.gigenet.com
	 * extras: mirror.vacares.com
	 * updates: bay.uchicago.edu
	base                                                                                                                  | 3.6 kB  00:00:00     
	docker-ce-stable                                                                                                      | 3.5 kB  00:00:00     
	extras                                                                                                                | 2.9 kB  00:00:00     
	updates                                                                                                               | 2.9 kB  00:00:00     
	(1/6): base/7/x86_64/group_gz                                                                                         | 153 kB  00:00:00     
	(2/6): extras/7/x86_64/primary_db                                                                                     | 246 kB  00:00:00     
	(3/6): docker-ce-stable/7/x86_64/primary_db                                                                           |  75 kB  00:00:00     
	(4/6): docker-ce-stable/7/x86_64/updateinfo                                                                           |   55 B  00:00:00     
	(5/6): base/7/x86_64/primary_db                                                                                       | 6.1 MB  00:00:02     
	(6/6): updates/7/x86_64/primary_db                                                                                    |  15 MB  00:00:04     
	No package containerd-io available.
	Resolving Dependencies
	--> Running transaction check
	---> Package docker-ce.x86_64 3:20.10.14-3.el7 will be installed
	--> Processing Dependency: container-selinux >= 2:2.74 for package: 3:docker-ce-20.10.14-3.el7.x86_64
	--> Processing Dependency: containerd.io >= 1.4.1 for package: 3:docker-ce-20.10.14-3.el7.x86_64
	--> Processing Dependency: libseccomp >= 2.3 for package: 3:docker-ce-20.10.14-3.el7.x86_64
	--> Processing Dependency: docker-ce-rootless-extras for package: 3:docker-ce-20.10.14-3.el7.x86_64
	--> Processing Dependency: libcgroup for package: 3:docker-ce-20.10.14-3.el7.x86_64
	---> Package docker-ce-cli.x86_64 1:20.10.14-3.el7 will be installed
	--> Processing Dependency: docker-scan-plugin(x86-64) for package: 1:docker-ce-cli-20.10.14-3.el7.x86_64
	--> Running transaction check
	---> Package container-selinux.noarch 2:2.119.2-1.911c772.el7_8 will be installed
	--> Processing Dependency: selinux-policy-targeted >= 3.13.1-216.el7 for package: 2:container-selinux-2.119.2-1.911c772.el7_8.noarch
	--> Processing Dependency: selinux-policy-base >= 3.13.1-216.el7 for package: 2:container-selinux-2.119.2-1.911c772.el7_8.noarch
	--> Processing Dependency: selinux-policy >= 3.13.1-216.el7 for package: 2:container-selinux-2.119.2-1.911c772.el7_8.noarch
	--> Processing Dependency: policycoreutils >= 2.5-11 for package: 2:container-selinux-2.119.2-1.911c772.el7_8.noarch
	--> Processing Dependency: policycoreutils-python for package: 2:container-selinux-2.119.2-1.911c772.el7_8.noarch
	--> Processing Dependency: libselinux-utils for package: 2:container-selinux-2.119.2-1.911c772.el7_8.noarch
	---> Package containerd.io.x86_64 0:1.5.11-3.1.el7 will be installed
	---> Package docker-ce-rootless-extras.x86_64 0:20.10.14-3.el7 will be installed
	--> Processing Dependency: fuse-overlayfs >= 0.7 for package: docker-ce-rootless-extras-20.10.14-3.el7.x86_64
	--> Processing Dependency: slirp4netns >= 0.4 for package: docker-ce-rootless-extras-20.10.14-3.el7.x86_64
	---> Package docker-scan-plugin.x86_64 0:0.17.0-3.el7 will be installed
	---> Package libcgroup.x86_64 0:0.41-21.el7 will be installed
	---> Package libseccomp.x86_64 0:2.3.1-4.el7 will be installed
	--> Running transaction check
	---> Package fuse-overlayfs.x86_64 0:0.7.2-6.el7_8 will be installed
	--> Processing Dependency: libfuse3.so.3(FUSE_3.2)(64bit) for package: fuse-overlayfs-0.7.2-6.el7_8.x86_64
	--> Processing Dependency: libfuse3.so.3(FUSE_3.0)(64bit) for package: fuse-overlayfs-0.7.2-6.el7_8.x86_64
	--> Processing Dependency: libfuse3.so.3()(64bit) for package: fuse-overlayfs-0.7.2-6.el7_8.x86_64
	---> Package libselinux-utils.x86_64 0:2.5-15.el7 will be installed
	--> Processing Dependency: libselinux(x86-64) = 2.5-15.el7 for package: libselinux-utils-2.5-15.el7.x86_64
	---> Package policycoreutils.x86_64 0:2.5-34.el7 will be installed
	---> Package policycoreutils-python.x86_64 0:2.5-34.el7 will be installed
	--> Processing Dependency: setools-libs >= 3.3.8-4 for package: policycoreutils-python-2.5-34.el7.x86_64
	--> Processing Dependency: libsemanage-python >= 2.5-14 for package: policycoreutils-python-2.5-34.el7.x86_64
	--> Processing Dependency: audit-libs-python >= 2.1.3-4 for package: policycoreutils-python-2.5-34.el7.x86_64
	--> Processing Dependency: python-IPy for package: policycoreutils-python-2.5-34.el7.x86_64
	--> Processing Dependency: libselinux-python for package: policycoreutils-python-2.5-34.el7.x86_64
	--> Processing Dependency: libqpol.so.1(VERS_1.4)(64bit) for package: policycoreutils-python-2.5-34.el7.x86_64
	--> Processing Dependency: libqpol.so.1(VERS_1.2)(64bit) for package: policycoreutils-python-2.5-34.el7.x86_64
	--> Processing Dependency: libapol.so.4(VERS_4.0)(64bit) for package: policycoreutils-python-2.5-34.el7.x86_64
	--> Processing Dependency: checkpolicy for package: policycoreutils-python-2.5-34.el7.x86_64
	--> Processing Dependency: libqpol.so.1()(64bit) for package: policycoreutils-python-2.5-34.el7.x86_64
	--> Processing Dependency: libapol.so.4()(64bit) for package: policycoreutils-python-2.5-34.el7.x86_64
	---> Package selinux-policy.noarch 0:3.13.1-268.el7_9.2 will be installed
	---> Package selinux-policy-targeted.noarch 0:3.13.1-268.el7_9.2 will be installed
	---> Package slirp4netns.x86_64 0:0.4.3-4.el7_8 will be installed
	--> Running transaction check
	---> Package audit-libs-python.x86_64 0:2.8.5-4.el7 will be installed
	--> Processing Dependency: audit-libs(x86-64) = 2.8.5-4.el7 for package: audit-libs-python-2.8.5-4.el7.x86_64
	---> Package checkpolicy.x86_64 0:2.5-8.el7 will be installed
	---> Package fuse3-libs.x86_64 0:3.6.1-4.el7 will be installed
	---> Package libselinux.x86_64 0:2.5-14.1.el7 will be updated
	---> Package libselinux.x86_64 0:2.5-15.el7 will be an update
	---> Package libselinux-python.x86_64 0:2.5-15.el7 will be installed
	---> Package libsemanage-python.x86_64 0:2.5-14.el7 will be installed
	---> Package python-IPy.noarch 0:0.75-6.el7 will be installed
	---> Package setools-libs.x86_64 0:3.3.8-4.el7 will be installed
	--> Running transaction check
	---> Package audit-libs.x86_64 0:2.8.4-4.el7 will be updated
	---> Package audit-libs.x86_64 0:2.8.5-4.el7 will be an update
	--> Finished Dependency Resolution

	Dependencies Resolved

	=============================================================================================================================================
	 Package                                 Arch                 Version                                   Repository                      Size
	=============================================================================================================================================
	Installing:
	 docker-ce                               x86_64               3:20.10.14-3.el7                          docker-ce-stable                22 M
	 docker-ce-cli                           x86_64               1:20.10.14-3.el7                          docker-ce-stable                30 M
	Installing for dependencies:
	 audit-libs-python                       x86_64               2.8.5-4.el7                               base                            76 k
	 checkpolicy                             x86_64               2.5-8.el7                                 base                           295 k
	 container-selinux                       noarch               2:2.119.2-1.911c772.el7_8                 extras                          40 k
	 containerd.io                           x86_64               1.5.11-3.1.el7                            docker-ce-stable                29 M
	 docker-ce-rootless-extras               x86_64               20.10.14-3.el7                            docker-ce-stable               8.1 M
	 docker-scan-plugin                      x86_64               0.17.0-3.el7                              docker-ce-stable               3.7 M
	 fuse-overlayfs                          x86_64               0.7.2-6.el7_8                             extras                          54 k
	 fuse3-libs                              x86_64               3.6.1-4.el7                               extras                          82 k
	 libcgroup                               x86_64               0.41-21.el7                               base                            66 k
	 libseccomp                              x86_64               2.3.1-4.el7                               base                            56 k
	 libselinux-python                       x86_64               2.5-15.el7                                base                           236 k
	 libselinux-utils                        x86_64               2.5-15.el7                                base                           151 k
	 libsemanage-python                      x86_64               2.5-14.el7                                base                           113 k
	 policycoreutils                         x86_64               2.5-34.el7                                base                           917 k
	 policycoreutils-python                  x86_64               2.5-34.el7                                base                           457 k
	 python-IPy                              noarch               0.75-6.el7                                base                            32 k
	 selinux-policy                          noarch               3.13.1-268.el7_9.2                        updates                        498 k
	 selinux-policy-targeted                 noarch               3.13.1-268.el7_9.2                        updates                        7.0 M
	 setools-libs                            x86_64               3.3.8-4.el7                               base                           620 k
	 slirp4netns                             x86_64               0.4.3-4.el7_8                             extras                          81 k
	Updating for dependencies:
	 audit-libs                              x86_64               2.8.5-4.el7                               base                           102 k
	 libselinux                              x86_64               2.5-15.el7                                base                           162 k

	Transaction Summary
	=============================================================================================================================================
	Install  2 Packages (+20 Dependent packages)
	Upgrade             (  2 Dependent packages)

	Total download size: 104 M
	Is this ok [y/d/N]: y
	Downloading packages:
	Delta RPMs disabled because /usr/bin/applydeltarpm not installed.
	(1/24): audit-libs-python-2.8.5-4.el7.x86_64.rpm                                                                      |  76 kB  00:00:00     
	(2/24): audit-libs-2.8.5-4.el7.x86_64.rpm                                                                             | 102 kB  00:00:00     
	(3/24): checkpolicy-2.5-8.el7.x86_64.rpm                                                                              | 295 kB  00:00:00     
	(4/24): container-selinux-2.119.2-1.911c772.el7_8.noarch.rpm                                                          |  40 kB  00:00:00     
	warning: /var/cache/yum/x86_64/7/docker-ce-stable/packages/docker-ce-20.10.14-3.el7.x86_64.rpm: Header V4 RSA/SHA512 Signature, key ID 621e9f35: NOKEY
	Public key for docker-ce-20.10.14-3.el7.x86_64.rpm is not installed
	(5/24): docker-ce-20.10.14-3.el7.x86_64.rpm                                                                           |  22 MB  00:00:00     
	(6/24): containerd.io-1.5.11-3.1.el7.x86_64.rpm                                                                       |  29 MB  00:00:00     
	(7/24): docker-ce-rootless-extras-20.10.14-3.el7.x86_64.rpm                                                           | 8.1 MB  00:00:00     
	(8/24): docker-ce-cli-20.10.14-3.el7.x86_64.rpm                                                                       |  30 MB  00:00:00     
	(9/24): docker-scan-plugin-0.17.0-3.el7.x86_64.rpm                                                                    | 3.7 MB  00:00:00     
	(10/24): fuse-overlayfs-0.7.2-6.el7_8.x86_64.rpm                                                                      |  54 kB  00:00:00     
	(11/24): libcgroup-0.41-21.el7.x86_64.rpm                                                                             |  66 kB  00:00:00     
	(12/24): libselinux-2.5-15.el7.x86_64.rpm                                                                             | 162 kB  00:00:00     
	(13/24): fuse3-libs-3.6.1-4.el7.x86_64.rpm                                                                            |  82 kB  00:00:00     
	(14/24): libselinux-python-2.5-15.el7.x86_64.rpm                                                                      | 236 kB  00:00:00     
	(15/24): libselinux-utils-2.5-15.el7.x86_64.rpm                                                                       | 151 kB  00:00:00     
	(16/24): libseccomp-2.3.1-4.el7.x86_64.rpm                                                                            |  56 kB  00:00:00     
	(17/24): libsemanage-python-2.5-14.el7.x86_64.rpm                                                                     | 113 kB  00:00:00     
	(18/24): policycoreutils-python-2.5-34.el7.x86_64.rpm                                                                 | 457 kB  00:00:00     
	(19/24): python-IPy-0.75-6.el7.noarch.rpm                                                                             |  32 kB  00:00:00     
	(20/24): setools-libs-3.3.8-4.el7.x86_64.rpm                                                                          | 620 kB  00:00:00     
	(21/24): policycoreutils-2.5-34.el7.x86_64.rpm                                                                        | 917 kB  00:00:00     
	(22/24): slirp4netns-0.4.3-4.el7_8.x86_64.rpm                                                                         |  81 kB  00:00:00     
	(23/24): selinux-policy-3.13.1-268.el7_9.2.noarch.rpm                                                                 | 498 kB  00:00:00     
	(24/24): selinux-policy-targeted-3.13.1-268.el7_9.2.noarch.rpm                                                        | 7.0 MB  00:00:02     
	---------------------------------------------------------------------------------------------------------------------------------------------
	Total                                                                                                         23 MB/s | 104 MB  00:00:04     
	Retrieving key from https://download.docker.com/linux/centos/gpg
	Importing GPG key 0x621E9F35:
	 Userid     : "Docker Release (CE rpm) <docker@docker.com>"
	 Fingerprint: 060a 61c5 1b55 8a7f 742b 77aa c52f eb6b 621e 9f35
	 From       : https://download.docker.com/linux/centos/gpg
	Is this ok [y/N]: y
	Running transaction check
	Running transaction test
	Transaction test succeeded
	Running transaction
	  Updating   : libselinux-2.5-15.el7.x86_64                                                                                             1/26 
	  Installing : libseccomp-2.3.1-4.el7.x86_64                                                                                            2/26 
	  Installing : libselinux-utils-2.5-15.el7.x86_64                                                                                       3/26 
	  Installing : libcgroup-0.41-21.el7.x86_64                                                                                             4/26 
	  Updating   : audit-libs-2.8.5-4.el7.x86_64                                                                                            5/26 
	  Installing : policycoreutils-2.5-34.el7.x86_64                                                                                        6/26 
	  Installing : selinux-policy-3.13.1-268.el7_9.2.noarch                                                                                 7/26 
	  Installing : selinux-policy-targeted-3.13.1-268.el7_9.2.noarch                                                                        8/26 
	  Installing : audit-libs-python-2.8.5-4.el7.x86_64                                                                                     9/26 
	  Installing : slirp4netns-0.4.3-4.el7_8.x86_64                                                                                        10/26 
	  Installing : setools-libs-3.3.8-4.el7.x86_64                                                                                         11/26 
	  Installing : libselinux-python-2.5-15.el7.x86_64                                                                                     12/26 
	  Installing : 1:docker-ce-cli-20.10.14-3.el7.x86_64                                                                                   13/26 
	  Installing : docker-scan-plugin-0.17.0-3.el7.x86_64                                                                                  14/26 
	  Installing : libsemanage-python-2.5-14.el7.x86_64                                                                                    15/26 
	  Installing : fuse3-libs-3.6.1-4.el7.x86_64                                                                                           16/26 
	  Installing : fuse-overlayfs-0.7.2-6.el7_8.x86_64                                                                                     17/26 
	  Installing : python-IPy-0.75-6.el7.noarch                                                                                            18/26 
	  Installing : checkpolicy-2.5-8.el7.x86_64                                                                                            19/26 
	  Installing : policycoreutils-python-2.5-34.el7.x86_64                                                                                20/26 
	  Installing : 2:container-selinux-2.119.2-1.911c772.el7_8.noarch                                                                      21/26 
	setsebool:  SELinux is disabled.
	  Installing : containerd.io-1.5.11-3.1.el7.x86_64                                                                                     22/26 
	  Installing : docker-ce-rootless-extras-20.10.14-3.el7.x86_64                                                                         23/26 
	  Installing : 3:docker-ce-20.10.14-3.el7.x86_64                                                                                       24/26 
	  Cleanup    : audit-libs-2.8.4-4.el7.x86_64                                                                                           25/26 
	  Cleanup    : libselinux-2.5-14.1.el7.x86_64                                                                                          26/26 
	  Verifying  : containerd.io-1.5.11-3.1.el7.x86_64                                                                                      1/26 
	  Verifying  : fuse-overlayfs-0.7.2-6.el7_8.x86_64                                                                                      2/26 
	  Verifying  : libselinux-2.5-15.el7.x86_64                                                                                             3/26 
	  Verifying  : docker-ce-rootless-extras-20.10.14-3.el7.x86_64                                                                          4/26 
	  Verifying  : 2:container-selinux-2.119.2-1.911c772.el7_8.noarch                                                                       5/26 
	  Verifying  : selinux-policy-targeted-3.13.1-268.el7_9.2.noarch                                                                        6/26 
	  Verifying  : audit-libs-2.8.5-4.el7.x86_64                                                                                            7/26 
	  Verifying  : checkpolicy-2.5-8.el7.x86_64                                                                                             8/26 
	  Verifying  : policycoreutils-2.5-34.el7.x86_64                                                                                        9/26 
	  Verifying  : python-IPy-0.75-6.el7.noarch                                                                                            10/26 
	  Verifying  : libseccomp-2.3.1-4.el7.x86_64                                                                                           11/26 
	  Verifying  : libselinux-utils-2.5-15.el7.x86_64                                                                                      12/26 
	  Verifying  : policycoreutils-python-2.5-34.el7.x86_64                                                                                13/26 
	  Verifying  : docker-scan-plugin-0.17.0-3.el7.x86_64                                                                                  14/26 
	  Verifying  : setools-libs-3.3.8-4.el7.x86_64                                                                                         15/26 
	  Verifying  : 3:docker-ce-20.10.14-3.el7.x86_64                                                                                       16/26 
	  Verifying  : fuse3-libs-3.6.1-4.el7.x86_64                                                                                           17/26 
	  Verifying  : libsemanage-python-2.5-14.el7.x86_64                                                                                    18/26 
	  Verifying  : slirp4netns-0.4.3-4.el7_8.x86_64                                                                                        19/26 
	  Verifying  : libselinux-python-2.5-15.el7.x86_64                                                                                     20/26 
	  Verifying  : selinux-policy-3.13.1-268.el7_9.2.noarch                                                                                21/26 
	  Verifying  : audit-libs-python-2.8.5-4.el7.x86_64                                                                                    22/26 
	  Verifying  : 1:docker-ce-cli-20.10.14-3.el7.x86_64                                                                                   23/26 
	  Verifying  : libcgroup-0.41-21.el7.x86_64                                                                                            24/26 
	  Verifying  : audit-libs-2.8.4-4.el7.x86_64                                                                                           25/26 
	  Verifying  : libselinux-2.5-14.1.el7.x86_64                                                                                          26/26 

	Installed:
	  docker-ce.x86_64 3:20.10.14-3.el7                                   docker-ce-cli.x86_64 1:20.10.14-3.el7                                  

	Dependency Installed:
	  audit-libs-python.x86_64 0:2.8.5-4.el7                               checkpolicy.x86_64 0:2.5-8.el7                                       
	  container-selinux.noarch 2:2.119.2-1.911c772.el7_8                   containerd.io.x86_64 0:1.5.11-3.1.el7                                
	  docker-ce-rootless-extras.x86_64 0:20.10.14-3.el7                    docker-scan-plugin.x86_64 0:0.17.0-3.el7                             
	  fuse-overlayfs.x86_64 0:0.7.2-6.el7_8                                fuse3-libs.x86_64 0:3.6.1-4.el7                                      
	  libcgroup.x86_64 0:0.41-21.el7                                       libseccomp.x86_64 0:2.3.1-4.el7                                      
	  libselinux-python.x86_64 0:2.5-15.el7                                libselinux-utils.x86_64 0:2.5-15.el7                                 
	  libsemanage-python.x86_64 0:2.5-14.el7                               policycoreutils.x86_64 0:2.5-34.el7                                  
	  policycoreutils-python.x86_64 0:2.5-34.el7                           python-IPy.noarch 0:0.75-6.el7                                       
	  selinux-policy.noarch 0:3.13.1-268.el7_9.2                           selinux-policy-targeted.noarch 0:3.13.1-268.el7_9.2                  
	  setools-libs.x86_64 0:3.3.8-4.el7                                    slirp4netns.x86_64 0:0.4.3-4.el7_8                                   

	Dependency Updated:
	  audit-libs.x86_64 0:2.8.5-4.el7                                       libselinux.x86_64 0:2.5-15.el7                                      

	Complete!

7. Validate installation
# rpm -qa |grep docker
	docker-scan-plugin-0.17.0-3.el7.x86_64
	docker-ce-20.10.14-3.el7.x86_64
	docker-ce-cli-20.10.14-3.el7.x86_64
	docker-ce-rootless-extras-20.10.14-3.el7.x86_64

8. Enable and run docker and check status
# systemctl enable docker
	Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.

# systemctl start docker

# systemctl status docker
	● docker.service - Docker Application Container Engine
	   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)
	   Active: active (running) since Sun 2022-04-24 05:42:51 UTC; 7s ago
	     Docs: https://docs.docker.com
	 Main PID: 1608 (dockerd)
	    Tasks: 28
	   Memory: 39.1M
	   CGroup: /docker/14a2eb37eda22233817fc68b10c29135f57b0a84c4d97c43bfbbef29352d2dc9/system.slice/docker.service
		   └─1608 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock

	Apr 24 05:42:50 stapp03.stratos.xfusioncorp.com dockerd[1608]: time="2022-04-24T05:42:50.860791328Z" level=info msg="Loading containe...one."
	Apr 24 05:42:50 stapp03.stratos.xfusioncorp.com dockerd[1608]: time="2022-04-24T05:42:50.936515661Z" level=info msg="Docker daemon" c...10.14
	Apr 24 05:42:50 stapp03.stratos.xfusioncorp.com dockerd[1608]: time="2022-04-24T05:42:50.936725295Z" level=info msg="Daemon has compl...tion"
	Apr 24 05:42:51 stapp03.stratos.xfusioncorp.com systemd[1]: Got notification message for unit docker.service
	Apr 24 05:42:51 stapp03.stratos.xfusioncorp.com systemd[1]: docker.service: Got notification message from PID 1608 (READY=1)
	Apr 24 05:42:51 stapp03.stratos.xfusioncorp.com systemd[1]: docker.service: got READY=1
	Apr 24 05:42:51 stapp03.stratos.xfusioncorp.com systemd[1]: docker.service changed start -> running
	Apr 24 05:42:51 stapp03.stratos.xfusioncorp.com systemd[1]: Job docker.service/start finished, result=done
	Apr 24 05:42:51 stapp03.stratos.xfusioncorp.com systemd[1]: Started Docker Application Container Engine.
	Apr 24 05:42:51 stapp03.stratos.xfusioncorp.com dockerd[1608]: time="2022-04-24T05:42:51.144159968Z" level=info msg="API listen on /v...sock"
	Hint: Some lines were ellipsized, use -l to show in full.

9. Validate the installations  
# docker --version
	Docker version 20.10.14, build a224086

# docker ps
	CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES

# docker-compose --version
	docker-compose version 1.28.6, build 5db8d86f

--------------------------------------------------------------------------------------------------------------------------------
Task 55 : 1/May/2022
Deploy Apache Web Server on Kubernetes CLuster

There is an application that needs to be deployed on Kubernetes cluster under Apache web server. The Nautilus application development team has asked the DevOps team to deploy it. We need to develop a template as per requirements mentioned below:

    Create a namespace named as httpd-namespace-devops.

    Create a deployment named as httpd-deployment-devops under newly created namespace. For the deployment use httpd image with latest tag only and remember to mention the tag i.e httpd:latest, and make sure replica counts are 2.

    Create a service named as httpd-service-devops under same namespace to expose the deployment, nodePort should be 30004.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.


thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get namespace
NAME                 STATUS   AGE
default              Active   61m
kube-node-lease      Active   61m
kube-public          Active   62m
kube-system          Active   62m
local-path-storage   Active   61m
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl create namespace httpd-namespace-devops
namespace/httpd-namespace-devops created
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get namespace
NAME                     STATUS   AGE
default                  Active   62m
httpd-namespace-devops   Active   4s
kube-node-lease          Active   62m
kube-public              Active   62m
kube-system              Active   62m
local-path-storage       Active   62m
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods -n  httpd-namespace-devops
No resources found in httpd-namespace-devops namespace.
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get service  -n  httpd-namespace-devops
No resources found in httpd-namespace-devops namespace.
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ vi /tmp/httpd.yaml
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ cat /tmp/httpd.yaml 
apiVersion: v1
kind: Service
metadata:
  name: httpd-service-devops
  namespace: httpd-namespace-devops
spec:
  type: NodePort
  selector:
    app: httpd_app_devops
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30004
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-deployment-devops
  namespace: httpd-namespace-devops
  labels:
    app: httpd_app_devops
spec:
  replicas: 2
  selector:
    matchLabels:
      app: httpd_app_devops
  template:
    metadata:
      labels:
        app: httpd_app_devops
    spec:
      containers:
        - name: httpd-container-devops
          image: httpd:latest
          ports:
            - containerPort: 80
thor@jump_host ~$ kubectl create -f /tmp/httpd.yaml
service/httpd-service-devops created
deployment.apps/httpd-deployment-devops created
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get service  -n  httpd-namespace-devops
NAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
httpd-service-devops   NodePort   10.96.72.224   <none>        80:30004/TCP   40s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods -n  httpd-namespace-devops
NAME                                      READY   STATUS    RESTARTS   AGE
httpd-deployment-devops-867b499f4-qdx2m   1/1     Running   0          47s
httpd-deployment-devops-867b499f4-sw7qc   1/1     Running   0          47s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
--------------------------------------------------------------------------------------------------------------------------------
Task 56: 3/May/2022
Git Merge Branches
The Nautilus application development team has been working on a project repository /opt/beta.git. This repo is cloned at /usr/src/kodekloudrepos on storage server in Stratos DC. They recently shared the following requirements with DevOps team:

a. Create a new branch nautilus in /usr/src/kodekloudrepos/beta repo from master and copy the /tmp/index.html file (on storage server itself). Add/commit this file in the new branch and merge back that branch to the master branch. Finally, push the changes to origin for both of the branches.

thor@jump_host ~$ ssh natasha@ststor01
The authenticity of host 'ststor01 (172.16.238.15)' can't be established.
ECDSA key fingerprint is SHA256:AMgD8K2XIX3sOAvahEWXDs51RbPFOGSsgp8+UWIfXNc.
ECDSA key fingerprint is MD5:9b:83:09:6b:3b:a8:af:dc:8b:6a:b8:99:35:a0:8d:c0.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'ststor01,172.16.238.15' (ECDSA) to the list of known hosts.
natasha@ststor01's password: 
[natasha@ststor01 ~]$ 
[natasha@ststor01 ~]$ 
[natasha@ststor01 ~]$ sudo su - 

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for natasha: 
[root@ststor01 ~]# 
[root@ststor01 ~]# 
[root@ststor01 ~]# 
[root@ststor01 ~]# cd /usr/src/kodekloudrepos/beta/
[root@ststor01 beta]# ls -ahl
total 20K
drwxr-xr-x 3 root root 4.0K May  3 03:44 .
drwxr-xr-x 3 root root 4.0K May  3 03:44 ..
drwxr-xr-x 8 root root 4.0K May  3 03:44 .git
-rw-r--r-- 1 root root   34 May  3 03:44 info.txt
-rw-r--r-- 1 root root   26 May  3 03:44 welcome.txt
[root@ststor01 beta]# git status
# On branch master
nothing to commit, working directory clean
[root@ststor01 beta]# 
[root@ststor01 beta]# git checkout -b nautilus
Switched to a new branch 'nautilus'
[root@ststor01 beta]# 
[root@ststor01 beta]# git status
# On branch nautilus
nothing to commit, working directory clean
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# git branch
  master
* nautilus
[root@ststor01 beta]# 
[root@ststor01 beta]# cp /tmp/index.html /usr/src/kodekloudrepos/beta/
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# ls -ahl
total 24K
drwxr-xr-x 3 root root 4.0K May  3 03:48 .
drwxr-xr-x 3 root root 4.0K May  3 03:44 ..
drwxr-xr-x 8 root root 4.0K May  3 03:47 .git
-rw-r--r-- 1 root root   27 May  3 03:48 index.html
-rw-r--r-- 1 root root   34 May  3 03:44 info.txt
-rw-r--r-- 1 root root   26 May  3 03:44 welcome.txt
[root@ststor01 beta]# git add index.html 
[root@ststor01 beta]# git commit -m "add beta"
[nautilus 71d74be] add beta
 1 file changed, 1 insertion(+)
 create mode 100644 index.html
[root@ststor01 beta]# 
[root@ststor01 beta]# git checkout master
Switched to branch 'master'
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# git merge nautilus
Updating 440bde1..71d74be
Fast-forward
 index.html | 1 +
 1 file changed, 1 insertion(+)
 create mode 100644 index.html
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# git push -u origin nautilus
Counting objects: 4, done.
Delta compression using up to 36 threads.
Compressing objects: 100% (2/2), done.
Writing objects: 100% (3/3), 325 bytes | 0 bytes/s, done.
Total 3 (delta 0), reused 0 (delta 0)
To /opt/beta.git
 * [new branch]      nautilus -> nautilus
Branch nautilus set up to track remote branch nautilus from origin.
[root@ststor01 beta]# 
[root@ststor01 beta]# git push -u origin master
Total 0 (delta 0), reused 0 (delta 0)
To /opt/beta.git
   440bde1..71d74be  master -> master
Branch master set up to track remote branch master from origin.
[root@ststor01 beta]# 
[root@ststor01 beta]# 
[root@ststor01 beta]# git status
# On branch master
nothing to commit, working directory clean

--------------------------------------------------------------------------------------------------------------------------------
Task 57: 17/May/2022

Countdown job in Kubernetes
The Nautilus DevOps team is working on to create few jobs in Kubernetes cluster. They might come up with some real scripts/commands to use, but for now they are preparing the templates and testing the jobs with dummy commands. Please create a job template as per details given below:

    Create a job countdown-xfusion.

    The spec template should be named as countdown-xfusion (under metadata), and the container should be named as container-countdown-xfusion

    Use image fedora with latest tag only and remember to mention tag i.e fedora:latest, and restart policy should be Never.

    Use command for i in 10 9 8 7 6 5 4 3 2 1 ; do echo $i ; done

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.



thor@jump_host ~$ kubectl get services
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   87m
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
No resources found in default namespace.

 
thor@jump_host ~$ vi /tmp/countdown.yaml
		apiVersion: batch/v1
		kind: Job
		metadata:
		  name: countdown-xfusion
		spec:
		  template:
		    metadata:
		      name: countdown-xfusion
		    spec:
		      containers:
		        - name: container-countdown-xfusion
		          image: fedora:latest
		          command: ["/bin/sh", "-c"]
		          args:
		            [
		              "for i in 10 9 8 7 6 5 4 3 2 1 ; do echo $i ; done",
		            ]
		      restartPolicy: Never 


thor@jump_host ~$ kubectl create -f /tmp/countdown.yaml 
job.batch/countdown-xfusion created
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
NAME                      READY   STATUS      RESTARTS   AGE
countdown-xfusion-xtgxv   0/1     Completed   0          14s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
NAME                      READY   STATUS      RESTARTS   AGE
countdown-xfusion-xtgxv   0/1     Completed   0          20s
thor@jump_host ~$ kubectl get logs countdown-xfusion-xtgxv
error: the server doesn't have a resource type "logs"
thor@jump_host ~$ kubectl get log countdown-xfusion-xtgxv
error: the server doesn't have a resource type "log"
thor@jump_host ~$ kubectl logs countdown-xfusion-xtgxv
10
9
8
7
6
5
4
3
2
1
thor@jump_host ~$

--------------------------------------------------------------------------------------------------------------------------------
Task 58:

Rolling Updates And Rolling Back Deployments in Kubernetes

--------------------------------------------------------------------------------------------------------------------------------
Task 59: 19/May/2022
Ansible Ping Module Usage

The Nautilus DevOps team is planning to test several Ansible playbooks on different app servers in Stratos DC. Before that, some pre-requisites must be met. Essentially, the team needs to set up a password-less SSH connection between Ansible controller and Ansible managed nodes. One of the tickets is assigned to you; please complete the task as per details mentioned below:

a. Jump host is our Ansible controller, and we are going to run Ansible playbooks through thor user on jump host.

b.Make appropriate changes on jump host so that user thor on jump host can SSH into App Server 1 through its respective sudo user. (for example tony for app server 1).

c. There is an inventory file /home/thor/ansible/inventory on jump host. Using that inventory file test Ansible ping from jump host to App Server 1, make sure ping works.

1. check inventory file on jump_host
	thor@jump_host ~$ cd /home/thor/ansible/
	thor@jump_host ~/ansible$ ls -ahl
		total 12K
		drwxr-xr-x 2 thor thor 4.0K May 19 07:27 .
		drwxr----- 1 thor thor 4.0K May 19 07:27 ..
		-rw-r--r-- 1 thor thor  237 May 19 07:27 inventory
	thor@jump_host ~/ansible$ cat inventory 
		stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n ansible_user=tony
		stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@ ansible_user=steve
		stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n ansible_user=banner


2. Generate ssh key for thor user		
	thor@jump_host ~/ansible$ ssh-keygen -t rsa -b 2048
			Generating public/private rsa key pair.
			Enter file in which to save the key (/home/thor/.ssh/id_rsa): 
			Enter passphrase (empty for no passphrase): 
			Enter same passphrase again: 
			Your identification has been saved in /home/thor/.ssh/id_rsa.
			Your public key has been saved in /home/thor/.ssh/id_rsa.pub.
			The key fingerprint is:
			SHA256:NzmC45AvaXU/9ysD+BFkpyScRmPmFCGR3249dJRIhcs thor@jump_host.stratos.xfusioncorp.com
			The key's randomart image is:
			+---[RSA 2048]----+
			|      o=O+ ..+.. |
			|      .*=.+ + o  |
			|       o.* + o   |
			|     . .. +.E .  |
			|    o + So=+ .   |
			|     * o.+=oo    |
			|    + o  ooo..   |
			|   . .    .oo.   |
			|             oo. |
			+----[SHA256]-----+

3. Copy ssh key to stapp01
	thor@jump_host ~/ansible$ ssh-copy-id tony@stapp01
			/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/home/thor/.ssh/id_rsa.pub"
			The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
			ECDSA key fingerprint is SHA256:/0nejVh3XjAevkSdnZs2/CE2zqcM0ewv/P7fZsa1PW0.
			ECDSA key fingerprint is MD5:bc:71:bc:a7:af:cb:94:31:61:9c:f4:c2:a6:8c:6d:41.
			Are you sure you want to continue connecting (yes/no)? yes
			/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
			/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
			tony@stapp01's password: 

			Number of key(s) added: 1

			Now try logging into the machine, with:   "ssh 'tony@stapp01'"
			and check to make sure that only the key(s) you wanted were added.

4. Verify ping command using ansible on stapp01
	thor@jump_host ~/ansible$ ansible stapp01 -m ping -i inventory -v
		Using /etc/ansible/ansible.cfg as config file
		stapp01 | SUCCESS => {
		    "ansible_facts": {
		        "discovered_interpreter_python": "/usr/bin/python"
		    }, 
		    "changed": false, 
		    "ping": "pong"
		}
--------------------------------------------------------------------------------------------------------------------------------
Task 60: 20/May/2022
Docker Ports Mapping

The Nautilus DevOps team is planning to host an application on a nginx-based container. There are number of tickets already been created for similar tasks. One of the tickets has been assigned to set up a nginx container on Application Server 1 in Stratos Datacenter. Please perform the task as per details mentioned below:

a. Pull nginx:alpine-perl docker image on Application Server 1.

b. Create a container named blog using the image you pulled.

c. Map host port 5002 to container port 80. Please keep the container in running state.


thor@jump_host ~$ ssh tony@stapp01
		The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
		ECDSA key fingerprint is SHA256:2U4MljMQTrcY/I3RlI2CxvvzdmhD0nAnVlhmv+swf2Q.
		ECDSA key fingerprint is MD5:5a:15:c9:96:07:43:d7:16:1a:18:b2:5d:55:f2:c4:5e.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp01,172.16.238.10' (ECDSA) to the list of known hosts.
		tony@stapp01's password: 
 
[tony@stapp01 ~]$ sudo su - 

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for tony: 
[root@stapp01 ~]# 

[root@stapp01 ~]# docker ps
		CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES

[root@stapp01 ~]# docker images
		REPOSITORY   TAG       IMAGE ID   CREATED   SIZE

[root@stapp01 ~]# docker pull nginx:alpine-perl
		alpine-perl: Pulling from library/nginx
		df9b9388f04a: Pull complete 
		fb94416861d8: Pull complete 
		cc9b6bc3348b: Pull complete 
		a2a262a4bae0: Pull complete 
		9a73c45724b0: Pull complete 
		69c0568aaf5b: Pull complete 
		Digest: sha256:b446bc9ed53d02c8a769b52bbdc35414e8555188559e8be143732b10d3b21b8e
		Status: Downloaded newer image for nginx:alpine-perl
		docker.io/library/nginx:alpine-perl


[root@stapp01 ~]# docker images
		REPOSITORY   TAG           IMAGE ID       CREATED      SIZE
		nginx        alpine-perl   c0800a068427   2 days ago   58.4MB



[root@stapp01 ~]# docker container run -d --name blog -p 5002:80 nginx:alpine-perl
		b894ab1eef89fe6711927203135074b90e6b08c02f294bf89f615c285fc380b8



[root@stapp01 ~]# docker ps
		CONTAINER ID   IMAGE               COMMAND                  CREATED          STATUS          PORTS                  NAMES
		b894ab1eef89   nginx:alpine-perl   "/docker-entrypoint.…"   16 seconds ago   Up 12 seconds   0.0.0.0:5002->80/tcp   blog


[root@stapp01 ~]# curl http://localhost:5002/
		<!DOCTYPE html>
		<html>
		<head>
		<title>Welcome to nginx!</title>
		<style>
		html { color-scheme: light dark; }
		body { width: 35em; margin: 0 auto;
		font-family: Tahoma, Verdana, Arial, sans-serif; }
		</style>
		</head>
		<body>
		<h1>Welcome to nginx!</h1>
		<p>If you see this page, the nginx web server is successfully installed and
		working. Further configuration is required.</p>

		<p>For online documentation and support please refer to
		<a href="http://nginx.org/">nginx.org</a>.<br/>
		Commercial support is available at
		<a href="http://nginx.com/">nginx.com</a>.</p>

		<p><em>Thank you for using nginx.</em></p>
		</body>
		</html>
[root@stapp01 ~]#

--------------------------------------------------------------------------------------------------------------------------------
Task 61: 25/May/2022

Deploy Nginx Web Server on Kubernetes Cluster

Some of the Nautilus team developers are developing a static website and they want to deploy it on Kubernetes cluster. They want it to be highly available and scalable. Therefore, based on the requirements, the DevOps team has decided to create a deployment for it with multiple replicas. Below you can find more details about it:

    Create a deployment using nginx image with latest tag only and remember to mention the tag i.e nginx:latest. Name it as nginx-deployment. The container should be named as nginx-container, also make sure replica counts are 3.

    Create a NodePort type service named nginx-service. The nodePort should be 30011.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

thor@jump_host ~$ kubectl get namespace
NAME                 STATUS   AGE
default              Active   94m
kube-node-lease      Active   94m
kube-public          Active   94m
kube-system          Active   94m
local-path-storage   Active   93m
thor@jump_host ~$ kubectl get pods
No resources found in default namespace.
thor@jump_host ~$ 
thor@jump_host ~$ vi /tmp/nginx.yml
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ cat /tmp/nginx.yml 
apiVersion: v1

kind: Service

metadata:

  name: nginx-service

spec:

  type: NodePort

  selector:

    app: nginx-app

    type: front-end

  ports:

    - port: 80

      targetPort: 80

      nodePort: 30011

---

apiVersion: apps/v1

kind: Deployment

metadata:

  name: nginx-deployment

  labels:

    app: nginx-app

    type: front-end

spec:

  replicas: 3

  selector:

    matchLabels:

      app: nginx-app

      type: front-end

  template:

    metadata:

      labels:

        app: nginx-app

        type: front-end

    spec:

      containers:

        - name: nginx-container

          image: nginx:latest 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl create -f /tmp/nginx.yml
service/nginx-service created
deployment.apps/nginx-deployment created
thor@jump_host ~$ kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-56cbd5d774-6v5jb   1/1     Running   0          20s
nginx-deployment-56cbd5d774-vm8kr   1/1     Running   0          20s
nginx-deployment-56cbd5d774-ww7jn   1/1     Running   0          20s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl exec nginx-deployment-56cbd5d774-6v5jb --curl http://localhost
Error: unknown flag: --curl
See 'kubectl exec --help' for usage.
thor@jump_host ~$ kubectl exec nginx-deployment-56cbd5d774-6v5jb -- curl http://localhost
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
100   615  100   615    0     0   600k      0 --:--:-- --:--:-- --:--:--  600k
thor@jump_host ~$ 

--------------------------------------------------------------------------------------------------------------------------------
Task 62: 26/May/2022
Ansible Basic Playbook

One of the Nautilus DevOps team members was working on to test an Ansible playbook on jump host. However, he was only able to create the inventory, and due to other priorities that came in he has to work on other tasks. Please pick up this task from where he left off and complete it. Below are more details about the task:

    The inventory file /home/thor/ansible/inventory seems to be having some issues, please fix them. The playbook needs to be run on App Server 2 in Stratos DC, so inventory file needs to be updated accordingly.

    Create a playbook /home/thor/ansible/playbook.yml and add a task to create an empty file /tmp/file.txt on App Server 2.

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml so please make sure the playbook works this way without passing any extra arguments.

thor@jump_host ~$ vi /home/thor/ansible/inventory 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ cat  /home/thor/ansible/inventory 
stapp02 ansible_host=172.16.238.11 ansible_user=steve ansible_ssh_pass=Am3ric@
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ vi /home/thor/ansible/plabook.yml
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ cat /home/thor/ansible/plabook.yml
- name: Create file in appserver

  hosts: stapp02

  become: yes

  tasks:

    - name: Create the file

      file:

        path: /tmp/file.txt

        state: touch
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ ansible all -a "ls -ltr /tmp/" -i inventory
[WARNING]: Unable to parse /home/thor/inventory as an inventory source
[WARNING]: No inventory was parsed, only implicit localhost is available
[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'
thor@jump_host ~$ 
thor@jump_host ~$ ansible all -a "ls -ltr /tmp/" -i ansible/inventory
stapp02 | CHANGED | rc=0 >>
total 8
-rw------- 1 root  root     0 Aug  1  2019 yum.log
-rwx------ 1 root  root   836 Aug  1  2019 ks-script-rnBCJB
drwx------ 2 steve steve 4096 May 26 13:20 ansible_command_payload_Jn4wIF
thor@jump_host ~$ 
thor@jump_host ~$ ansible-playbook - i inventory play^C
thor@jump_host ~$ cd ansible/
thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook^C
thor@jump_host ~/ansible$ ls -ahl
total 16K
drwxr-xr-x 2 thor thor 4.0K May 26 13:19 .
drwxr----- 1 thor thor 4.0K May 26 13:19 ..
-rw-r--r-- 1 thor thor   79 May 26 13:18 inventory
-rw-rw-r-- 1 thor thor  169 May 26 13:19 plabook.yml
thor@jump_host ~/ansible$ mv plabook.yml playbook.yml
thor@jump_host ~/ansible$ ls -ahl
total 16K
drwxr-xr-x 2 thor thor 4.0K May 26 13:21 .
drwxr----- 1 thor thor 4.0K May 26 13:19 ..
-rw-r--r-- 1 thor thor   79 May 26 13:18 inventory
-rw-rw-r-- 1 thor thor  169 May 26 13:19 playbook.yml
thor@jump_host ~/ansible$ cat /home/thor/ansible/playbook.yml
- name: Create file in appserver

  hosts: stapp02

  become: yes

  tasks:

    - name: Create the file

      file:

        path: /tmp/file.txt

        state: touch
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook.yml 

PLAY [Create file in appserver] *************************************************************************************************************

TASK [Gathering Facts] **********************************************************************************************************************
ok: [stapp02]

TASK [Create the file] **********************************************************************************************************************
changed: [stapp02]

PLAY RECAP **********************************************************************************************************************************
stapp02                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   


thor@jump_host ~/ansible$ ansible all -a "ls -ltr /tmp/" -i ansible/inventory
[WARNING]: Unable to parse /home/thor/ansible/ansible/inventory as an inventory source
[WARNING]: No inventory was parsed, only implicit localhost is available
[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'


thor@jump_host ~/ansible$ ansible all -a "ls -ltr /tmp/" -i inventory
stapp02 | CHANGED | rc=0 >>
total 8
-rw------- 1 root  root     0 Aug  1  2019 yum.log
-rwx------ 1 root  root   836 Aug  1  2019 ks-script-rnBCJB
-rw-r--r-- 1 root  root     0 May 26 13:22 file.txt
drwx------ 2 steve steve 4096 May 26 13:22 ansible_command_payload_ld0RbL


--------------------------------------------------------------------------------------------------------------------------------
Task 63: 13/Jun/2022
Fix issue with PhpFpm Application Deployed on Kubernetes

We deployed a Nginx and PHPFPM based application on Kubernetes cluster last week and it had been working fine. This morning one of the team members was troubleshooting an issue with this stack and he was supposed to run Nginx welcome page for now on this stack till issue with phpfpm is fixed but he made a change somewhere which caused some issue and the application stopped working. Please look into the issue and fix the same:

The deployment name is nginx-phpfpm-dp and service name is nginx-service. Figure out the issues and fix them. FYI Nginx is configured to use default http port, node port is 30008 and copy index.php under /tmp/index.php to deployment under /var/www/html. Please do not try to delete/modify any other existing components like deployment name, service name etc.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

kubectl get deploy

kubectl get pods

kubectl get svc

kubectl get configmap

kubectl describe pods

kubectl get pods

kubectl describe service

kubectl edit service

kubectl edit configmap

kubectl logs





--------------------------------------------------------------------------------------------------------------------------------
Task 64: 19/Jun/2022

Deploy Tomcat App on Kubernetes

A new java-based application is ready to be deployed on a Kubernetes cluster. The development team had a meeting with the DevOps team to share the requirements and application scope. The team is ready to setup an application stack for it under their existing cluster. Below you can find the details for this:

    Create a namespace named tomcat-namespace-xfusion.

    Create a deployment for tomcat app which should be named as tomcat-deployment-xfusion under the same namespace you created. Replica count should be 1, the container should be named as tomcat-container-xfusion, its image should be gcr.io/kodekloud/centos-ssh-enabled:tomcat and its container port should be 8080.

    Create a service for tomcat app which should be named as tomcat-service-xfusion under the same namespace you created. Service type should be NodePort and nodePort should be 32227.

Before clicking on Check button please make sure the application is up and running.

You can use any labels as per your choice.

Note: The kubectl on jump_host has been configured to work with the kubernetes cluster.


1. kubectl get namespace
		NAME                 STATUS   AGE
		default              Active   118m
		kube-node-lease      Active   118m
		kube-public          Active   118m
		kube-system          Active   118m
		local-path-storage   Active   117m

2. kubectl get pods
		No resources found in default namespace.


3. kubectl create namespace tomcat-namespace-xfusion
		namespace/tomcat-namespace-xfusion created

	 kubectl get namespace
		NAME                       STATUS   AGE
		default                    Active   118m
		kube-node-lease            Active   118m
		kube-public                Active   118m
		kube-system                Active   118m
		local-path-storage         Active   118m
		tomcat-namespace-xfusion   Active   4s
4. vi /tmp/tomcat.yaml
  
   cat /tmp/tomcat.yaml 
			apiVersion: v1

			kind: Service

			metadata:

			  name: tomcat-service-xfusion

			  namespace: tomcat-namespace-xfusion

			spec:

			  type: NodePort

			  selector:

			    app: tomcat

			  ports:

			    - port: 80

			      protocol: TCP

			      targetPort: 8080

			      nodePort: 32227

			---

			apiVersion: apps/v1                          

			kind: Deployment

			metadata:

			  name: tomcat-deployment-xfusion

			  namespace: tomcat-namespace-xfusion

			spec:

			  replicas: 1

			  selector:

			    matchLabels:

			      app: tomcat

			  template:

			    metadata:

			      labels:

			        app: tomcat

			    spec:

			      containers:

			        - name: tomcat-container-xfusion

			          image: gcr.io/kodekloud/centos-ssh-enabled:tomcat

			          ports:

			            - containerPort: 8080

5. kubectl create -f /tmp/tomcat.yaml 
		service/tomcat-service-xfusion created
		deployment.apps/tomcat-deployment-xfusion created

6. kubectl get deploy -n tomcat-namespace-xfusion
		NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
		tomcat-deployment-xfusion   0/1     1            0           31s
   
   kubectl get pods -n tomcat-namespace-xfusion
		NAME                                         READY   STATUS    RESTARTS   AGE
		tomcat-deployment-xfusion-654c5b77ff-qvnqh   1/1     Running   0          52s

7. kubectl exec tomcat-deployment-xfusion-654c5b77ff-qvnqh -n tomcat-namespace-xfusion -- curl http://localhost:8080
		  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
		                                 Dload  Upload   Total   Spent    Left  Speed
		  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0<!DOCTYPE html>
		<!--
		To change this license header, choose License Headers in Project Properties.
		To change this template file, choose Tools | Templates
		and open the template in the editor.
		-->
		<html>
		    <head>
		        <title>SampleWebApp</title>
		        <meta charset="UTF-8">
		        <meta name="viewport" content="width=device-width, initial-scale=1.0">
		    </head>
		    <body>
		        <h2>Welcome to xFusionCorp Industries!</h2>
		        <br>
		    
		    </body>
		</html>
		100   471  100   471    0     0   1304      0 --:--:-- --:--:-- --:--:--  1304

--------------------------------------------------------------------------------------------------------------------------------
Task 65: 23/Jun/2022

Rolling Updates in Kubernetes

We have an application running on Kubernetes cluster using nginx web server. The Nautilus application development team has pushed some of the latest changes and those changes need be deployed. The Nautilus DevOps team has created an image nginx:1.19 with the latest changes.

Perform a rolling update for this application and incorporate nginx:1.19 image. The deployment name is nginx-deployment

Make sure all pods are up and running after the update.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.


1. kubectl get deploy
		NAME               READY   UP-TO-DATE   AVAILABLE   AGE
		nginx-deployment   3/3     3            3           23s

2.kubectl get pods
		NAME                                READY   STATUS    RESTARTS   AGE
		nginx-deployment-74fb588559-56499   1/1     Running   0          29s
		nginx-deployment-74fb588559-l5d6q   1/1     Running   0          28s
		nginx-deployment-74fb588559-qrt22   1/1     Running   0          28s

3. kubectl describe deployment nginx-deployment
		Name:                   nginx-deployment
		Namespace:              default
		CreationTimestamp:      Thu, 23 Jun 2022 03:07:35 +0000
		Labels:                 app=nginx-app
		                        type=front-end
		Annotations:            deployment.kubernetes.io/revision: 1
		Selector:               app=nginx-app
		Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
		StrategyType:           RollingUpdate
		MinReadySeconds:        0
		RollingUpdateStrategy:  25% max unavailable, 25% max surge
		Pod Template:
		  Labels:  app=nginx-app
		  Containers:
		   nginx-container:
		    Image:        nginx:1.16                                  <---------------------------------
		    Port:         <none>
		    Host Port:    <none>
		    Environment:  <none>
		    Mounts:       <none>
		  Volumes:        <none>
		Conditions:
		  Type           Status  Reason
		  ----           ------  ------
		  Available      True    MinimumReplicasAvailable
		  Progressing    True    NewReplicaSetAvailable
		OldReplicaSets:  <none>
		NewReplicaSet:   nginx-deployment-74fb588559 (3/3 replicas created)
		Events:
		  Type    Reason             Age   From                   Message
		  ----    ------             ----  ----                   -------
		  Normal  ScalingReplicaSet  55s   deployment-controller  Scaled up replica set nginx-deployment-74fb588559 to 3

4.kubectl set image deployment nginx-deployment nginx-container=nginx:1.19
		deployment.apps/nginx-deployment image updated

5. kubectl get pods
		NAME                                READY   STATUS              RESTARTS   AGE
		nginx-deployment-57bf6d6978-7b4ml   0/1     ContainerCreating   0          9s                          <--------------------------------
		nginx-deployment-74fb588559-56499   1/1     Running             0          3m4s
		nginx-deployment-74fb588559-l5d6q   1/1     Running             0          3m3s
		nginx-deployment-74fb588559-qrt22   1/1     Running             0          3m3s

6. kubectl get deploy
		NAME               READY   UP-TO-DATE   AVAILABLE   AGE
		nginx-deployment   3/3     3            3           3m23s

7. kubectl get pods
		NAME                                READY   STATUS    RESTARTS   AGE
		nginx-deployment-57bf6d6978-7b4ml   1/1     Running   0          35s                  <--------------
		nginx-deployment-57bf6d6978-7v8jj   1/1     Running   0          18s                  <--------------     
		nginx-deployment-57bf6d6978-tbnp4   1/1     Running   0          21s                  <-------------

8. kubectl describe deployment nginx-deployment
		Name:                   nginx-deployment
		Namespace:              default
		CreationTimestamp:      Thu, 23 Jun 2022 03:07:35 +0000
		Labels:                 app=nginx-app
		                        type=front-end
		Annotations:            deployment.kubernetes.io/revision: 2
		Selector:               app=nginx-app
		Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
		StrategyType:           RollingUpdate
		MinReadySeconds:        0
		RollingUpdateStrategy:  25% max unavailable, 25% max surge
		Pod Template:
		  Labels:  app=nginx-app
		  Containers:
		   nginx-container:
		    Image:        nginx:1.19                                                 <----------------------------------
		    Port:         <none>
		    Host Port:    <none>
		    Environment:  <none>
		    Mounts:       <none>
		  Volumes:        <none>
		Conditions:
		  Type           Status  Reason
		  ----           ------  ------
		  Available      True    MinimumReplicasAvailable
		  Progressing    True    NewReplicaSetAvailable
		OldReplicaSets:  <none>
		NewReplicaSet:   nginx-deployment-57bf6d6978 (3/3 replicas created)
		Events:
		  Type    Reason             Age    From                   Message
		  ----    ------             ----   ----                   -------
		  Normal  ScalingReplicaSet  3m50s  deployment-controller  Scaled up replica set nginx-deployment-74fb588559 to 3
		  Normal  ScalingReplicaSet  55s    deployment-controller  Scaled up replica set nginx-deployment-57bf6d6978 to 1
		  Normal  ScalingReplicaSet  41s    deployment-controller  Scaled down replica set nginx-deployment-74fb588559 to 2
		  Normal  ScalingReplicaSet  41s    deployment-controller  Scaled up replica set nginx-deployment-57bf6d6978 to 2
		  Normal  ScalingReplicaSet  38s    deployment-controller  Scaled down replica set nginx-deployment-74fb588559 to 1
		  Normal  ScalingReplicaSet  38s    deployment-controller  Scaled up replica set nginx-deployment-57bf6d6978 to 3
		  Normal  ScalingReplicaSet  35s    deployment-controller  Scaled down replica set nginx-deployment-74fb588559 to 0
		thor@jump_host ~$ 

9. kubectl get deploy
		NAME               READY   UP-TO-DATE   AVAILABLE   AGE
		nginx-deployment   3/3     3            3           4m32s

10. kubectl get pods
		NAME                                READY   STATUS    RESTARTS   AGE
		nginx-deployment-57bf6d6978-7b4ml   1/1     Running   0          103s
		nginx-deployment-57bf6d6978-7v8jj   1/1     Running   0          86s
		nginx-deployment-57bf6d6978-tbnp4   1/1     Running   0          89s

11. kubectl rollout status deployment nginx-deployment
		deployment "nginx-deployment" successfully rolled out

--------------------------------------------------------------------------------------------------------------------------------
Task 66: 27/Jun/2022
Create Namespaces in Kubernetes Cluster

The Nautilus DevOps team is planning to deploy some micro services on Kubernetes platform. The team has already set up a Kubernetes cluster and now they want set up some namespaces, deployments etc. Based on the current requirements, the team has shared some details as below:

Create a namespace named dev and create a POD under it; name the pod dev-nginx-pod and use nginx image with latest tag only and remember to mention tag i.e nginx:latest.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

1. kubectl get namespaces
		NAME                 STATUS   AGE
		default              Active   128m
		kube-node-lease      Active   128m
		kube-public          Active   128m
		kube-system          Active   128m
		local-path-storage   Active   127m

2. kubectl get pods
		No resources found in default namespace.

3. kubectl get namespace
		NAME                 STATUS   AGE
		default              Active   128m
		kube-node-lease      Active   128m
		kube-public          Active   128m
		kube-system          Active   128m
		local-path-storage   Active   128m

4. kubectl create namespace dev
		namespace/dev created

5. kubectl get namespace
		NAME                 STATUS   AGE
		default              Active   128m
		dev                  Active   5s
		kube-node-lease      Active   128m
		kube-public          Active   128m
		kube-system          Active   128m
		local-path-storage   Active   128m

6. kubectl run dev-nginx-pod --image=nginx:latest -n dev
		pod/dev-nginx-pod created

7. kubectl get pods -n dev
		NAME            READY   STATUS              RESTARTS   AGE
		dev-nginx-pod   0/1     ContainerCreating   0          9s

--------------------------------------------------------------------------------------------------------------------------------
Task 67: 4/Jul/2022
Ansible Create Users and Groups

Several new developers and DevOps engineers just joined the xFusionCorp industries. They have been assigned the Nautilus project, and as per the onboarding process we need to create user accounts for new joinees on at least one of the app servers in Stratos DC. We also need to create groups and make new users members of those groups. We need to accomplish this task using Ansible. Below you can find more information about the task.

There is already an inventory file ~/playbooks/inventory on jump host.

On jump host itself there is a list of users in ~/playbooks/data/users.yml file and there are two groups — admins and developers —that have list of different users. Create a playbook ~/playbooks/add_users.yml on jump host to perform the following tasks on app server 2 in Stratos DC.

a. Add all users given in the users.yml file on app server 2.

b. Also add developers and admins groups on the same server.

c. As per the list given in the users.yml file, make each user member of the respective group they are listed under.

d. Make sure home directory for all of the users under developers group is /var/www (not the default i.e /var/www/{USER}). Users under admins group should use the default home directory (i.e /home/devid for user devid).

e. Set password BruCStnMT5 for all of the users under developers group and LQfKeWWxWD for of the users under admins group. Make sure to use the password given in the ~/playbooks/secrets/vault.txt file as Ansible vault password to encrypt the original password strings. You can use ~/playbooks/secrets/vault.txt file as a vault secret file while running the playbook (make necessary changes in ~/playbooks/ansible.cfg file).

f. All users under admins group must be added as sudo users. To do so, simply make them member of the wheel group as well.

Note: Validation will try to run the playbook using command ansible-playbook -i inventory add_users.yml so please make sure playbook works this way, without passing any extra arguments.


Last login: Mon Jul  4 11:36:22 UTC 2022 on pts/0
1.cd playbooks/
  ls -ahl
		total 24K
		drwxr-xr-x 4 thor thor 4.0K Jul  4 11:36 .
		drwxr----- 1 thor thor 4.0K Jul  4 11:36 ..
		-rw-r--r-- 1 thor thor   36 Jul  4 11:36 ansible.cfg
		drwxr-xr-x 2 thor thor 4.0K Jul  4 10:30 data
		-rw-r--r-- 1 thor thor  237 Jul  4 11:36 inventory
		drwxr-xr-x 2 thor thor 4.0K Jul  4 11:36 secrets

2. cat data/users.yml 
		admins:
		  - rob
		  - david
		  - joy

		developers:
		  - tim
		  - ray
		  - jim
		  - mark

3. cat secrets/vault.txt 
		P@ss3or432

4.Add password vault file location in ansible.cfg 
	 vi ansible.cfg 
	 cat ansible.cfg 
		[defaults]
		host_key_checking = False
		vault_password_file = /home/thor/playbooks/secrets/vault.txt     <------------------- Add password vault file location in ansible.cfg

5. Verify ansible connection to mentioned server
  ansible stapp02 -a "cat /etc/passwd" -i inventory
		stapp02 | CHANGED | rc=0 >>
		root:x:0:0:root:/root:/bin/bash
		bin:x:1:1:bin:/bin:/sbin/nologin
		daemon:x:2:2:daemon:/sbin:/sbin/nologin
		adm:x:3:4:adm:/var/adm:/sbin/nologin
		lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin
		sync:x:5:0:sync:/sbin:/bin/sync
		shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown
		halt:x:7:0:halt:/sbin:/sbin/halt
		mail:x:8:12:mail:/var/spool/mail:/sbin/nologin
		operator:x:11:0:operator:/root:/sbin/nologin
		games:x:12:100:games:/usr/games:/sbin/nologin
		ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin
		nobody:x:99:99:Nobody:/:/sbin/nologin
		systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin
		dbus:x:81:81:System message bus:/:/sbin/nologin
		sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin
		ansible:x:1000:1000::/home/ansible:/bin/bash
		steve:x:1001:1001::/home/steve:/bin/bash

6. vi ~/playbooks/add_users.yml
   cat add_users.yml 
			---                                                                                                              
			- name: Ansbile Add User & Group                                                                       
			  hosts: stapp02                                                                                                
			  become: yes                                                                                                    
			  tasks:                                                                                                         
			  - name: Creating Admin Groups                                                                                  
			    group:                                                                                                       
			     name:                                                                                                       
			      admins                                                                                                     
			     state: present                                                                                              
			  - name: Creating Dev Groups                                                                                    
			    group:                                                                                                       
			     name:                                                                                                       
			      developers                                                                                                 
			     state: present                                                                                              
			  - name: Creating Admins Group Users                                                                            
			    user:                                                                                                        
			     name: "{{ item }}"                                                                                          
			     password: "{{ 'LQfKeWWxWD' | password_hash ('sha512') }}"                                                   
			     groups: admins,wheel
			     state: present                                                                                              
			    loop:                                                                                                        
			    - rob                                                                                                        
			    - joy                                                                                                        
			    - david                                                                                                      
			  - name: Creating Developers Group Users                                                                        
			    user:                                                                                                        
			     name: "{{ item }}"                                                                                          
			     password: "{{ 'BruCStnMT5' | password_hash ('sha512') }}"                                                   
			     home: "/var/www/{{ item }}"                                                                                             
			     group: developers                                                                                           
			     state: present                                                                                              
			    loop:                                                                                                        
			    - tim                                                                                                        
			    - jim                                                                                                        
			    - mark                                                                                                       
			    - ray  

7. ansible-playbook -i inventory add_users.yml 

			PLAY [Ansbile Add User & Group] *************************************************************************************************************

			TASK [Gathering Facts] **********************************************************************************************************************
			ok: [stapp02]

			TASK [Creating Admin Groups] ****************************************************************************************************************
			changed: [stapp02]

			TASK [Creating Dev Groups] ******************************************************************************************************************
			changed: [stapp02]

			TASK [Creating Admins Group Users] **********************************************************************************************************
			changed: [stapp02] => (item=rob)
			changed: [stapp02] => (item=joy)
			changed: [stapp02] => (item=david)

			TASK [Creating Developers Group Users] ******************************************************************************************************
			changed: [stapp02] => (item=tim)
			changed: [stapp02] => (item=jim)
			changed: [stapp02] => (item=mark)
			changed: [stapp02] => (item=ray)

			PLAY RECAP **********************************************************************************************************************************
			stapp02                    : ok=5    changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

8. ansible stapp02 -a "cat /etc/passwd" -i inventory
			stapp02 | CHANGED | rc=0 >>
			root:x:0:0:root:/root:/bin/bash
			bin:x:1:1:bin:/bin:/sbin/nologin
			daemon:x:2:2:daemon:/sbin:/sbin/nologin
			adm:x:3:4:adm:/var/adm:/sbin/nologin
			lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin
			sync:x:5:0:sync:/sbin:/bin/sync
			shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown
			halt:x:7:0:halt:/sbin:/sbin/halt
			mail:x:8:12:mail:/var/spool/mail:/sbin/nologin
			operator:x:11:0:operator:/root:/sbin/nologin
			games:x:12:100:games:/usr/games:/sbin/nologin
			ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin
			nobody:x:99:99:Nobody:/:/sbin/nologin
			systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin
			dbus:x:81:81:System message bus:/:/sbin/nologin
			sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin
			ansible:x:1000:1000::/home/ansible:/bin/bash
			steve:x:1001:1001::/home/steve:/bin/bash
			rob:x:1002:1004::/home/rob:/bin/bash
			joy:x:1003:1005::/home/joy:/bin/bash
			david:x:1004:1006::/home/david:/bin/bash
			tim:x:1005:1003::/var/www/tim:/bin/bash
			jim:x:1006:1003::/var/www/jim:/bin/bash
			mark:x:1007:1003::/var/www/mark:/bin/bash
			ray:x:1008:1003::/var/www/ray:/bin/bash

9. ssh rob@stapp02
		The authenticity of host 'stapp02 (172.16.238.11)' can't be established.
		ECDSA key fingerprint is SHA256:7RCf6YZUPBcZw8m3zXVF7SaKj4h3sMVtsQb1QBHyIwE.
		ECDSA key fingerprint is MD5:74:a3:7f:20:34:82:27:27:3a:27:4f:32:25:a3:67:c4.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp02' (ECDSA) to the list of known hosts.
		rob@stapp02's password: 
		[rob@stapp02 ~]$ 

10. [rob@stapp02 ~]$ sudo su -
		[root@stapp02 ~]# 
		[root@stapp02 ~]# pwd
		/root
		[root@stapp02 ~]# exit
		logout
		[rob@stapp02 ~]$ pwd
		/home/rob
		[rob@stapp02 ~]$ 
		[rob@stapp02 ~]$ exit
		logout
		Connection to stapp02 closed.
11. ssh ray@stapp02
		ray@stapp02's password: 
		[ray@stapp02 ~]$ 
		[ray@stapp02 ~]$ 
		[ray@stapp02 ~]$ pwd
		/var/www/ray
		[ray@stapp02 ~]$ 
		[ray@stapp02 ~]$ 
		[ray@stapp02 ~]$ sudo su -

			We trust you have received the usual lecture from the local System
			Administrator. It usually boils down to these three things:

			    #1) Respect the privacy of others.
			    #2) Think before you type.
			    #3) With great power comes great responsibility.

			[sudo] password for ray: 
			ray is not in the sudoers file.  This incident will be reported.
		[ray@stapp02 ~]$ 
		[ray@stapp02 ~]$ 
		[ray@stapp02 ~]$ 
		[ray@stapp02 ~]$ exit
		logout
		Connection to stapp02 closed.
thor@jump_host ~/playbooks$

--------------------------------------------------------------------------------------------------------------------------------
Task 68: 9/Jul/2022

Set Limits for Resources in Kubernetes


Recently some of the performance issues were observed with some applications hosted on Kubernetes cluster. The Nautilus DevOps team has observed some resources constraints, where some of the applications are running out of resources like memory, cpu etc., and some of the applications are consuming more resources than needed. Therefore, the team has decided to add some limits for resources utilization. Below you can find more details.

Create a pod named httpd-pod and a container under it named as httpd-container, use httpd image with latest tag only and remember to mention tag i.e httpd:latest and set the following limits:

Requests: Memory: 15Mi, CPU: 100m

Limits: Memory: 20Mi, CPU: 100m

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

thor@jump_host ~$ kubectl get namespace
NAME                 STATUS   AGE
default              Active   4h4m
kube-node-lease      Active   4h4m
kube-public          Active   4h4m
kube-system          Active   4h4m
local-path-storage   Active   4h3m
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
No resources found in default namespace.
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ vi /tmp/reslimit.yaml
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ cat /tmp/reslimit.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: httpd-pod
spec:
  containers:
  - name: httpd-container
    image: httpd:latest
    resources:
      requests:
        memory: "15Mi"
        cpu: "100m"
      limits:
        memory: "20Mi"
        cpu: "100m"
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl create -f /tmp/reslimit.yaml 
pod/httpd-pod created
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
httpd-pod   1/1     Running   0          14s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl describe pods http-pod
Error from server (NotFound): pods "http-pod" not found
thor@jump_host ~$ kubectl describe pods httpd-pod
Name:         httpd-pod
Namespace:    default
Priority:     0
Node:         kodekloud-control-plane/172.17.0.2
Start Time:   Sat, 09 Jul 2022 03:52:03 +0000
Labels:       <none>
Annotations:  <none>
Status:       Running
IP:           10.244.0.5
IPs:
  IP:  10.244.0.5
Containers:
  httpd-container:
    Container ID:   containerd://445b86e0a90eb7e789f611a291bff2f5bc7280620d4a7c8a617b8d2b02731ef5
    Image:          httpd:latest
    Image ID:       docker.io/library/httpd@sha256:886f273536ebef2239ef7dc42e6486544fbace3e36e5a42735cfdc410e36d33c
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Sat, 09 Jul 2022 03:52:14 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  20Mi
    Requests:
      cpu:        100m
      memory:     15Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-5sbwr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-5sbwr:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-5sbwr
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  53s   default-scheduler  Successfully assigned default/httpd-pod to kodekloud-control-plane
  Normal  Pulling    52s   kubelet            Pulling image "httpd:latest"
  Normal  Pulled     43s   kubelet            Successfully pulled image "httpd:latest" in 9.316827757s
  Normal  Created    43s   kubelet            Created container httpd-container
  Normal  Started    42s   kubelet            Started container httpd-container


--------------------------------------------------------------------------------------------------------------------------------
Task 69: 14/Jul/2022

Ansible Copy Module


There is data on jump host that needs to be copied on all application servers in Stratos DC. Nautilus DevOps team want to perform this task using Ansible. Perform the task as per details mentioned below:

a. On jump host create an inventory file /home/thor/ansible/inventory and add all application servers as managed nodes.

b. On jump host create a playbook /home/thor/ansible/playbook.yml to copy /usr/src/sysops/index.html file to all application servers at location /opt/sysops.

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml so please make sure the playbook works this way without passing any extra arguments.

thor@jump_host ~$ cd /home/thor/ansible/
thor@jump_host ~/ansible$ ls -ahl
total 8.0K
drwxr-xr-x 2 thor thor 4.0K Jul 14 02:48 .
drwxr----- 1 thor thor 4.0K Jul 14 02:48 ..
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ vi inventory
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ cat inventory 
stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n  ansible_user=tony

stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@  ansible_user=steve

stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n  ansible_user=banner
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ ansible all -a "ls -ahl /opt/sysops" -i inventory 
stapp03 | CHANGED | rc=0 >>
total 8.0K
drwxr-xr-x 2 root root 4.0K Jul 14 02:48 .
drwxr-xr-x 1 root root 4.0K Jul 14 02:48 ..
stapp02 | CHANGED | rc=0 >>
total 8.0K
drwxr-xr-x 2 root root 4.0K Jul 14 02:48 .
drwxr-xr-x 1 root root 4.0K Jul 14 02:48 ..
stapp01 | CHANGED | rc=0 >>
total 8.0K
drwxr-xr-x 2 root root 4.0K Jul 14 02:48 .
drwxr-xr-x 1 root root 4.0K Jul 14 02:48 ..
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ vi playbook.yml
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ cat playbook.yml 
- name: Ansible copy

  hosts: all

  become: yes

  tasks:

    - name: copy index.html to sysops folder

      copy: src=/usr/src/sysops/index.html dest=/opt/sysops
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook.yml

PLAY [Ansible copy] *************************************************************************************************************************

TASK [Gathering Facts] **********************************************************************************************************************
ok: [stapp03]
ok: [stapp02]
ok: [stapp01]

TASK [copy index.html to sysops folder] *****************************************************************************************************
changed: [stapp03]
changed: [stapp02]
changed: [stapp01]

PLAY RECAP **********************************************************************************************************************************
stapp01                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
stapp02                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
stapp03                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

thor@jump_host ~/ansible$ ansible all -a "ls -ahl /opt/sysops" -i inventory 
stapp01 | CHANGED | rc=0 >>
total 12K
drwxr-xr-x 2 root root 4.0K Jul 14 02:52 .
drwxr-xr-x 1 root root 4.0K Jul 14 02:48 ..
-rw-r--r-- 1 root root   35 Jul 14 02:52 index.html
stapp03 | CHANGED | rc=0 >>
total 12K
drwxr-xr-x 2 root root 4.0K Jul 14 02:52 .
drwxr-xr-x 1 root root 4.0K Jul 14 02:48 ..
-rw-r--r-- 1 root root   35 Jul 14 02:52 index.html
stapp02 | CHANGED | rc=0 >>
total 12K
drwxr-xr-x 2 root root 4.0K Jul 14 02:52 .
drwxr-xr-x 1 root root 4.0K Jul 14 02:48 ..
-rw-r--r-- 1 root root   35 Jul 14 02:52 index.html
thor@jump_host ~/ansible$

--------------------------------------------------------------------------------------------------------------------------------
Task 70: 17/Jul/2022

Ansible Archive Module

The Nautilus DevOps team has some data on jump host in Stratos DC that they want to copy on all app servers in the same data center. However, they want to create an archive of data and copy it to the app servers. Additionally, there are some specific requirements for each server. Perform the task using Ansible playbook as per requirements mentioned below:

Create a playbook.yml under /home/thor/ansible on jump host, an inventory file is already placed under /home/thor/ansible/ on Jump Server itself.

    Create an archive official.tar.gz (make sure archive format is tar.gz) of /usr/src/itadmin/ directory ( present on each app server ) and copy it to /opt/itadmin/ directory on all app servers. The user and group owner of archive official.tar.gz should be tony for App Server 1, steve for App Server 2 and banner for App Server 3.

Note: Validation will try to run playbook using command ansible-playbook -i inventory playbook.yml so please make sure playbook works this way, without passing any extra arguments.
thor@jump_host ~$ cd /home/thor/ansible/
thor@jump_host ~/ansible$ ls -agl
total 16
drwxr-xr-x 2 thor 4096 Jul 17 16:14 .
drwxr----- 1 thor 4096 Jul 17 16:14 ..
-rw-r--r-- 1 thor   36 Jul 17 16:14 ansible.cfg
-rw-r--r-- 1 thor  237 Jul 17 16:14 inventory
thor@jump_host ~/ansible$ ls -ahl
total 16K
drwxr-xr-x 2 thor thor 4.0K Jul 17 16:14 .
drwxr----- 1 thor thor 4.0K Jul 17 16:14 ..
-rw-r--r-- 1 thor thor   36 Jul 17 16:14 ansible.cfg
-rw-r--r-- 1 thor thor  237 Jul 17 16:14 inventory
thor@jump_host ~/ansible$ cat inventory 
stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n ansible_user=tony
stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@ ansible_user=steve
stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n ansible_user=bannerthor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ ansible all -a "ls -ahl /opt/sysops" -i inventory 
stapp03 | FAILED | rc=2 >>
ls: cannot access /opt/sysops: No such file or directorynon-zero return code
stapp02 | FAILED | rc=2 >>
ls: cannot access /opt/sysops: No such file or directorynon-zero return code
stapp01 | FAILED | rc=2 >>
ls: cannot access /opt/sysops: No such file or directorynon-zero return code
thor@jump_host ~/ansible$ ansible all -a "ls -ahl /opt/itadmin" -i inventory 
stapp03 | CHANGED | rc=0 >>
total 8.0K
drwxr-xr-x 2 root root 4.0K Jul 17 16:14 .
drwxr-xr-x 1 root root 4.0K Jul 17 16:14 ..
stapp01 | CHANGED | rc=0 >>
total 8.0K
drwxr-xr-x 2 root root 4.0K Jul 17 16:14 .
drwxr-xr-x 1 root root 4.0K Jul 17 16:14 ..
stapp02 | CHANGED | rc=0 >>
total 8.0K
drwxr-xr-x 2 root root 4.0K Jul 17 16:14 .
drwxr-xr-x 1 root root 4.0K Jul 17 16:14 ..
thor@jump_host ~/ansible$ vi playbook.yml
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ cat playbook.yml 
- name: Task create archive and copy to host

  hosts: stapp01, stapp02, stapp03

  become: yes

  tasks:

    - name: As per the task create the archive file and set the owner

      archive:

        path: /usr/src/itadmin/

        dest: /opt/itadmin/official.tar.gz

        format: gz

        force_archive: true

        owner: "{{ ansible_user }}"

        group: "{{ ansible_user }}"
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ ls -ahl /usr/src/itadmin
ls: cannot access /usr/src/itadmin: No such file or directory
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook.yml

PLAY [Task create archive and copy to host] *************************************************************************************************

TASK [Gathering Facts] **********************************************************************************************************************
ok: [stapp03]
ok: [stapp01]
ok: [stapp02]

TASK [As per the task create the archive file and set the owner] ****************************************************************************
changed: [stapp03]
changed: [stapp02]
changed: [stapp01]

PLAY RECAP **********************************************************************************************************************************
stapp01                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
stapp02                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
stapp03                    : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ 
thor@jump_host ~/ansible$ ansible all -a "ls -ahl /opt/itadmin" -i inventory 
stapp01 | CHANGED | rc=0 >>
total 12K
drwxr-xr-x 2 root root 4.0K Jul 17 16:19 .
drwxr-xr-x 1 root root 4.0K Jul 17 16:14 ..
-rw-r--r-- 1 tony tony  169 Jul 17 16:19 official.tar.gz
stapp03 | CHANGED | rc=0 >>
total 12K
drwxr-xr-x 2 root   root   4.0K Jul 17 16:19 .
drwxr-xr-x 1 root   root   4.0K Jul 17 16:14 ..
-rw-r--r-- 1 banner banner  166 Jul 17 16:19 official.tar.gz
stapp02 | CHANGED | rc=0 >>
total 12K
drwxr-xr-x 2 root  root  4.0K Jul 17 16:19 .
drwxr-xr-x 1 root  root  4.0K Jul 17 16:14 ..
-rw-r--r-- 1 steve steve  178 Jul 17 16:19 official.tar.gz
thor@jump_host ~/ansible$


--------------------------------------------------------------------------------------------------------------------------------
Task 71: 21/Jul/2022

 Puppet Manage Services

 New packages need to be installed on some of the app servers in Stratos Datacenter. The Nautilus DevOps team has decided to install the same using Puppet. Since jump host is already configured to run as Puppet master server and all app servers are already configured to work as puppet agent nodes, we need to create the required manifests on the Puppet master server so that it can be applied on the required Puppet agent node. Please find more details about the task below.

Create a Puppet programming file demo.pp under /etc/puppetlabs/code/environments/production/manifests directory on master node i.e Jump Host to perform the below given tasks.

    Install package httpd using puppet package resource and start its service using puppet service resource on Puppet agent node 2 i.e App Server 2.

Notes: :- Please make sure to run the puppet agent test using sudo on agent nodes, otherwise you can face certificate issues. In that case you will have to clean the certificates first and then you will be able to run the puppet agent test.

:- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.




thor@jump_host ~$ ssh -t steve@stapp02 "systemctl status httpd"
The authenticity of host 'stapp02 (172.16.238.11)' can't be established.
ECDSA key fingerprint is SHA256:cPKTsscSCR1Q3bxjworQ0ZblELpSiTS9fk6221Fg3qA.
ECDSA key fingerprint is MD5:90:4b:f5:fc:fa:44:1c:a0:01:d3:4d:21:b9:1a:4a:27.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'stapp02,172.16.238.11' (ECDSA) to the list of known hosts.
steve@stapp02's password: 
Unit httpd.service could not be found.
Connection to stapp02 closed.
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ sudo su -

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for thor: 
root@jump_host ~# 
root@jump_host ~# 
root@jump_host ~# 
root@jump_host ~# cd /etc/puppetlabs/code/environments/production/manifests/
root@jump_host /etc/puppetlabs/code/environments/production/manifests# ls -ahl
total 8.0K
drwxr-xr-x 1 puppet puppet 4.0K Jul 13  2021 .
drwxr-xr-x 1 puppet puppet 4.0K Aug  9  2021 ..
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# vi demo.pp
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# cat demo.pp 
class httpd_installer {

    package {'httpd':

        ensure => installed

    }

    service {'httpd':

        ensure    => running,

        enable    => true,

    }

}

node 'stapp02.stratos.xfusioncorp.com' {

  include httpd_installer

}
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# puppet parser validate demo.pp
root@jump_host /etc/puppetlabs/code/environments/production/manifests# ssh  steve@stapp02
The authenticity of host 'stapp02 (172.16.238.11)' can't be established.
ECDSA key fingerprint is SHA256:cPKTsscSCR1Q3bxjworQ0ZblELpSiTS9fk6221Fg3qA.
ECDSA key fingerprint is MD5:90:4b:f5:fc:fa:44:1c:a0:01:d3:4d:21:b9:1a:4a:27.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'stapp02,172.16.238.11' (ECDSA) to the list of known hosts.
steve@stapp02's password: 
Last login: Thu Jul 21 14:37:42 2022 from jump_host.stratos.xfusioncorp.com
[steve@stapp02 ~]$ 
[steve@stapp02 ~]$ 
[steve@stapp02 ~]$ sudo su - 

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for steve: 
[root@stapp02 ~]# 
[root@stapp02 ~]# 
[root@stapp02 ~]# 
[root@stapp02 ~]# puppet agent -tv
Info: Using configured environment 'production'
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Retrieving locales
Info: Caching catalog for stapp02.stratos.xfusioncorp.com
Info: Applying configuration version '1658414537'
Notice: /Stage[main]/Httpd_installer/Package[httpd]/ensure: created
Notice: /Stage[main]/Httpd_installer/Service[httpd]/ensure: ensure changed 'stopped' to 'running'
Info: /Stage[main]/Httpd_installer/Service[httpd]: Unscheduling refresh on Service[httpd]
Notice: Applied catalog in 23.21 seconds
[root@stapp02 ~]# systemctl status httpd
● httpd.service - The Apache HTTP Server
   Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled; vendor preset: disabled)
   Active: active (running) since Thu 2022-07-21 14:42:46 UTC; 32s ago
     Docs: man:httpd(8)
           man:apachectl(8)
 Main PID: 1462 (httpd)
   Status: "Total requests: 0; Current requests/sec: 0; Current traffic:   0 B/sec"
   CGroup: /docker/50268e59ce67a7e23d4897f43cdfb3200edd62d3198d32104728e7ec150d21c6/system.slice/httpd.service
           ├─1462 /usr/sbin/httpd -DFOREGROUND
           ├─1463 /usr/sbin/httpd -DFOREGROUND
           ├─1464 /usr/sbin/httpd -DFOREGROUND
           ├─1465 /usr/sbin/httpd -DFOREGROUND
           ├─1466 /usr/sbin/httpd -DFOREGROUND
           └─1467 /usr/sbin/httpd -DFOREGROUND

Jul 21 14:42:46 stapp02.stratos.xfusioncorp.com systemd[1]: httpd.service: got MAINPID=1462
Jul 21 14:42:46 stapp02.stratos.xfusioncorp.com systemd[1]: httpd.service: got READY=1
Jul 21 14:42:46 stapp02.stratos.xfusioncorp.com systemd[1]: httpd.service changed start -> running
Jul 21 14:42:46 stapp02.stratos.xfusioncorp.com systemd[1]: Job httpd.service/start finished, result=done
Jul 21 14:42:46 stapp02.stratos.xfusioncorp.com systemd[1]: Started The Apache HTTP Server.
Jul 21 14:42:46 stapp02.stratos.xfusioncorp.com systemd[1]: httpd.service: got STATUS=Processing requests...
Jul 21 14:43:16 stapp02.stratos.xfusioncorp.com systemd[1]: Got notification message for unit httpd.service
Jul 21 14:43:16 stapp02.stratos.xfusioncorp.com systemd[1]: httpd.service: Got notification message from PID 1462 (READY=1, STATUS=T...B/sec)
Jul 21 14:43:16 stapp02.stratos.xfusioncorp.com systemd[1]: httpd.service: got READY=1
Jul 21 14:43:16 stapp02.stratos.xfusioncorp.com systemd[1]: httpd.service: got STATUS=Total requests: 0; Current requests/sec: 0; Cu... B/sec
Hint: Some lines were ellipsized, use -l to show in full.
[root@stapp02 ~]# 
[root@stapp02 ~]# 
[root@stapp02 ~]#

--------------------------------------------------------------------------------------------------------------------------------
Task 72: 2/Aug/2022

Environment Variables in Kubernetes

There are a number of parameters that are used by the applications. We need to define these as environment variables, so that we can use them as needed within different configs. Below is a scenario which needs to be configured on Kubernetes cluster. Please find below more details about the same.

    Create a pod named envars.

    Container name should be fieldref-container, use image busybox preferable latest tag, use command 'sh', '-c' and args should be

'while true; do echo -en '/n'; printenv NODE_NAME POD_NAME; printenv POD_IP POD_SERVICE_ACCOUNT; sleep 10; done;'

(Note: please take care of indentations)

    Define Four environment variables as mentioned below:

a.) The first env should be named as NODE_NAME, set valueFrom fieldref and fieldPath should be spec.nodeName.

b.) The second env should be named as POD_NAME, set valueFrom fieldref and fieldPath should be metadata.name.

c.) The third env should be named as POD_IP, set valueFrom fieldref and fieldPath should be status.podIP.

d.) The fourth env should be named as POD_SERVICE_ACCOUNT, set valueFrom fieldref and fieldPath shoulbe be spec.serviceAccountName.

    Set restart policy to Never.

    To check the output, exec into the pod and use printenv command.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.


thor@jump_host ~$ kubectl get namespace
NAME                 STATUS   AGE
default              Active   96m
kube-node-lease      Active   96m
kube-public          Active   96m
kube-system          Active   97m
local-path-storage   Active   96m
thor@jump_host ~$ kubectl get pods
No resources found in default namespace.

thor@jump_host ~$ vi /tmp/envars.yml
thor@jump_host ~$ cat /tmp/envars.yml 
apiVersion: v1
kind: Pod
metadata:
  name: envars
  namespace: default
spec:
  restartPolicy: Never
  containers:
    - name: fieldref-container
      image: busybox:latest
      command: ["sh", "-c"]
      args:
        - while true; do
          echo -en '\n';
          printenv NODE_NAME POD_NAME;
          printenv POD_IP POD_SERVICE_ACCOUNT;
          sleep 10;
          done;
      env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: POD_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName

thor@jump_host ~$ kubectl create -f /tmp/envars.yml 
pod/envars created
thor@jump_host ~$ 

thor@jump_host ~$ kubectl get pods
NAME     READY   STATUS    RESTARTS   AGE
envars   1/1     Running   0          6s

thor@jump_host ~$ 
thor@jump_host ~$ 
 
thor@jump_host ~$ kubectl exec -it envars  -n default  -- /bin/sh

/ # printenv
		POD_IP=10.244.0.5
		KUBERNETES_SERVICE_PORT=443
		KUBERNETES_PORT=tcp://10.96.0.1:443
		HOSTNAME=envars
		SHLVL=1
		HOME=/root
		NODE_NAME=kodekloud-control-plane
		TERM=xterm
		POD_NAME=envars
		KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
		PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
		POD_SERVICE_ACCOUNT=default
		KUBERNETES_PORT_443_TCP_PORT=443
		KUBERNETES_PORT_443_TCP_PROTO=tcp
		KUBERNETES_SERVICE_PORT_HTTPS=443
		KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
		KUBERNETES_SERVICE_HOST=10.96.0.1
		PWD=/
/ # 

--------------------------------------------------------------------------------------------------------------------------------
Task 73: 4/Aug/2022

 Puppet String Manipulation

 There is some data on App Server 1 in Stratos DC. The Nautilus development team shared some requirement with the DevOps team to alter some of the data as per recent changes. The DevOps team is working to prepare a Puppet programming file to accomplish this. Below you can find more details about the task.

Create a Puppet programming file beta.pp under /etc/puppetlabs/code/environments/production/manifests directory on Puppet master node i.e Jump Server and by using puppet file_line resource perform the following tasks.

    We have a file /opt/data/beta.txt on App Server 1. Use the Puppet programming file mentioned above to replace line Welcome to Nautilus Industries! to Welcome to xFusionCorp Industries!, no other data should be altered in this file.

Notes: :- Please make sure to run the puppet agent test using sudo on agent nodes, otherwise you can face certificate issues. In that case you will have to clean the certificates first and then you will be able to run the puppet agent test.

:- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.




thor@jump_host$ sudo su -

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for thor: 

root@jump_host ~# 
root@jump_host ~# cd /etc/puppetlabs/code/environments/production/manifests/
root@jump_host /etc/puppetlabs/code/environments/production/manifests# ls -ahl
total 8.0K
drwxr-xr-x 1 puppet puppet 4.0K Jul 13  2021 .
drwxr-xr-x 1 puppet puppet 4.0K Aug  9  2021 ..
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# vi beta.pp
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# cat beta.pp 
class data_replacer {

  file_line { 'line_replace':

    path => '/opt/data/beta.txt',

    match => 'Welcome to Nautilus Industries!',

    line  => 'Welcome to xFusionCorp Industries!',

  }

}

node 'stapp01.stratos.xfusioncorp.com' {

  include data_replacer

}
root@jump_host /etc/puppetlabs/code/environments/production/manifests# puppet  parser validate beta.pp
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# ssh tony@stapp01
The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
ECDSA key fingerprint is SHA256:WbglaHl1g4fRfeVoPlwyr5SjSs6/Zzzo+KrvtnxmOqc.
ECDSA key fingerprint is MD5:bb:62:cb:94:db:24:9d:e7:b1:fa:56:bb:1b:02:09:8d.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'stapp01,172.16.238.10' (ECDSA) to the list of known hosts.
tony@stapp01's password: 
[tony@stapp01 ~]$ 
[tony@stapp01 ~]$ sudo su -

We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:

    #1) Respect the privacy of others.
    #2) Think before you type.
    #3) With great power comes great responsibility.

[sudo] password for tony: 
[root@stapp01 ~]# puppet  agent -tv
Info: Using configured environment 'production'
Info: Retrieving pluginfacts
Info: Retrieving plugin
Info: Retrieving locales
Info: Loading facts
Info: Caching catalog for stapp01.stratos.xfusioncorp.com
Info: Applying configuration version '1659637132'
Notice: Applied catalog in 0.18 seconds
[root@stapp01 ~]# 
[root@stapp01 ~]# 
[root@stapp01 ~]# cat /opt/data/beta.txt 
This is  Nautilus sample file, created using Puppet!
Welcome to xFusionCorp Industries!
Please do not modify this file manually!
[root@stapp01 ~]#

--------------------------------------------------------------------------------------------------------------------------------
Task 74: 11/Aug/2022

Create a Docker Network


The Nautilus DevOps team needs to set up several docker environments for different applications. One of the team members has been assigned a ticket where he has been asked to create some docker networks to be used later. Complete the task based on the following ticket description:

a. Create a docker network named as ecommerce on App Server 1 in Stratos DC.

b. Configure it to use bridge drivers.

c. Set it to use subnet 172.168.0.0/24 and iprange 172.168.0.1/24.

1.
thor@jump_host ~$ ssh tony@stapp01
	tony@stapp01's password: 
	Last login: Thu Aug 11 06:00:49 2022 from jump_host.devops-docker-network-v2_app_net
 
[tony@stapp01 ~]$ sudo su -
	[sudo] password for tony: 
	Last login: Thu Aug 11 06:01:06 UTC 2022 on pts/0

2.
[root@stapp01 ~]# docker network ls
	NETWORK ID     NAME      DRIVER    SCOPE
	820c238da3c4   bridge    bridge    local
	283b6c449fa5   host      host      local
	373edcfd5a8e   none      null      local

3. 
[root@stapp01 ~]# docker network create -d bridge --subnet=172.168.0.0/24 --ip-range=172.168.0.1/24 ecommerce
	f167ca63f027883998a63bf2d8f633522a7e7b400f09785afa9da0ccef2948d6

4.
[root@stapp01 ~]# docker network ls
	NETWORK ID     NAME        DRIVER    SCOPE
	820c238da3c4   bridge      bridge    local
	f167ca63f027   ecommerce   bridge    local
	283b6c449fa5   host        host      local
	373edcfd5a8e   none        null      local

5.
[root@stapp01 ~]# docker network inspect ecommerce
		[
		    {
		        "Name": "ecommerce",
		        "Id": "f167ca63f027883998a63bf2d8f633522a7e7b400f09785afa9da0ccef2948d6",
		        "Created": "2022-08-11T06:03:22.153851571Z",
		        "Scope": "local",
		        "Driver": "bridge",
		        "EnableIPv6": false,
		        "IPAM": {
		            "Driver": "default",
		            "Options": {},
		            "Config": [
		                {
		                    "Subnet": "172.168.0.0/24",
		                    "IPRange": "172.168.0.1/24"
		                }
		            ]
		        },
		        "Internal": false,
		        "Attachable": false,
		        "Ingress": false,
		        "ConfigFrom": {
		            "Network": ""
		        },
		        "ConfigOnly": false,
		        "Containers": {},
		        "Options": {},
		        "Labels": {}
		    }
		]

--------------------------------------------------------------------------------------------------------------------------------
Task 75: 20/Aug/2022

 Deploy Nagios on Kubernetes


The Nautilus DevOps team is planning to set up a Nagios monitoring tool to monitor some applications, services etc. They are planning to deploy it on Kubernetes cluster. Below you can find more details.

1) Create a deployment nagios-deployment for Nagios core. The container name must be nagios-container and it must use jasonrivers/nagios image.

2) Create a user and password for the Nagios core web interface, user must be xFusionCorp and password must be LQfKeWWxWD. (you can manually perform this step after deployment)

3) Create a service nagios-service for Nagios, which must be of targetPort type. nodePort must be 30008.

You can use any labels as per your choice.

Note: The kubectl on jump_host has been configured to work with the kubernetes cluster.


thor@jump_host ~$ kubectl get deploy
	No resources found in default namespace.
 
thor@jump_host ~$ kubectl get service
	NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
	kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4h48m

thor@jump_host ~$ vi /tmp/nagios.yaml
thor@jump_host ~$ cat /tmp/nagios.yaml 
		---
		apiVersion: apps/v1
		kind: Deployment
		metadata:
		  name: nagios-deployment
		spec:
		  replicas: 1
		  selector:
		    matchLabels:
		      app: nagios-core
		  template:
		    metadata:
		      labels:
		        app: nagios-core
		    spec:
		      containers:
		        - name: nagios-container
		          image: jasonrivers/nagios
		---
		apiVersion: v1
		kind: Service
		metadata:
		  name: nagios-service
		spec:
		  type: NodePort
		  selector:
		    app: nagios-core
		  ports:
		    - port: 80
		      targetPort: 80
		      nodePort: 30008
 
thor@jump_host ~$ kubectl create -f /tmp/nagios.yaml 
	deployment.apps/nagios-deployment created
	service/nagios-service created


thor@jump_host ~$ kubectl get deploy
	NAME                READY   UP-TO-DATE   AVAILABLE   AGE
	nagios-deployment   0/1     1            0           9s

thor@jump_host ~$ kubectl get service
	NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
	kubernetes       ClusterIP   10.96.0.1      <none>        443/TCP        4h51m
	nagios-service   NodePort    10.96.133.33   <none>        80:30008/TCP   17s
 
thor@jump_host ~$ kubectl get pods -o wide
	NAME                                 READY   STATUS              RESTARTS   AGE   IP       NODE                      NOMINATED NODE   READINESS GATES
	nagios-deployment-6674945696-hhlzz   0/1     ContainerCreating   0          30s   <none>   kodekloud-control-plane   <none>           <none>


thor@jump_host ~$ kubectl get pods -o wide
	NAME                                 READY   STATUS    RESTARTS   AGE   IP           NODE                      NOMINATED NODE   READINESS GATES
	nagios-deployment-6674945696-hhlzz   1/1     Running   0          61s   10.244.0.5   kodekloud-control-plane   <none>           <none>

thor@jump_host ~$ kubectl get deploy
	NAME                READY   UP-TO-DATE   AVAILABLE   AGE
	nagios-deployment   1/1     1            1           66s

thor@jump_host ~$ kubectl exec -it nagios-deployment-6674945696-hhlzz -- /bin/bash
		root@nagios-deployment-6674945696-hhlzz:/# 
		root@nagios-deployment-6674945696-hhlzz:/# 
		root@nagios-deployment-6674945696-hhlzz:/# hostname
		nagios-deployment-6674945696-hhlzz
		root@nagios-deployment-6674945696-hhlzz:/# 
		root@nagios-deployment-6674945696-hhlzz:/# 
		root@nagios-deployment-6674945696-hhlzz:/# 
		root@nagios-deployment-6674945696-hhlzz:/# 
		root@nagios-deployment-6674945696-hhlzz:/# htpasswd /opt/nagios/etc/htpasswd.users xFusionCorp
		New password: 
		Re-type new password: 
		Adding password for user xFusionCorp
		root@nagios-deployment-6674945696-hhlzz:/# 
		root@nagios-deployment-6674945696-hhlzz:/# 
		root@nagios-deployment-6674945696-hhlzz:/# 
		root@nagios-deployment-6674945696-hhlzz:/# curl -u xFusionCorp http://localhost/
		Enter host password for user 'xFusionCorp':
					<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Frameset//EN" "http://www.w3.org/TR/html4/frameset.dtd">

					<html>
					<head>
					        <meta name="ROBOTS" content="NOINDEX, NOFOLLOW">
					        <title>Nagios: localhost</title>
					        <link rel="shortcut icon" href="images/favicon.ico" type="image/ico">

					        <script LANGUAGE="javascript">
					                var n = Math.round(Math.random() * 10000000000);
					                document.cookie = "NagFormId=" + n.toString(16);
					        </script>
					</head>

					<frameset cols="180,*" style="border: 0px; framespacing: 0px">
					        <frame src="side.php" name="side" frameborder="0" style="">
					        <frame src="main.php" name="main" frameborder="0" style="">

					        <noframes>
					                <!-- This page requires a web browser which supports frames. -->
					                <h2>Nagios Core</h2>
					                <p align="center">
					                        <a href="https://www.nagios.org/">www.nagios.org</a><br>
					                        Copyright &copy; 2010-2022 Nagios Core Development Team and Community Contributors.
					                        Copyright &copy; 1999-2010 Ethan Galstad<br>
					                </p>
					                <p>
					                        <i>Note: These pages require a browser which supports frames</i>
					                </p>
					        </noframes>
					</frameset>

					</html>
		root@nagios-deployment-6674945696-hhlzz:/#


View Port : 30008 on browser username /password 

--------------------------------------------------------------------------------------------------------------------------------
Task 76: 23/Aug/2022

Docker Copy Operations

The Nautilus DevOps team has some conditional data present on App Server 1 in Stratos Datacenter. There is a container ubuntu_latest running on the same server. We received a request to copy some of the data from the docker host to the container. Below are more details about the task:

On App Server 1 in Stratos Datacenter copy an encrypted file /tmp/nautilus.txt.gpg from docker host to ubuntu_latest container (running on same server) in /tmp/ location. Please do not try to modify this file in any way.


thor@jump_host ~$ ssh tony@stapp01
		The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
		ECDSA key fingerprint is SHA256:mLdWSn3TjPUitnlhOHi1JPUIS6a/P9zTxc5hRMxVVnA.
		ECDSA key fingerprint is MD5:ef:03:06:06:c2:d5:d5:8f:f8:79:3f:3d:86:bb:4c:81.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp01,172.16.238.10' (ECDSA) to the list of known hosts.
		tony@stapp01's password: 
 
[tony@stapp01 ~]$ sudo su - 

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for tony: 

[root@stapp01 ~]# docker ps
		CONTAINER ID   IMAGE     COMMAND   CREATED              STATUS              PORTS     NAMES
		f30b949d9b53   ubuntu    "bash"    About a minute ago   Up About a minute             ubuntu_latest

[root@stapp01 ~]# docker cp /tmp/nautilus.txt.gpg ubuntu_latest:/tmp/

[root@stapp01 ~]# docker exec ubuntu_latest ls -ahl /tmp/
		total 12K
		drwxrwxrwt  2 root root 4.0K Aug 23 13:54 .
		drwxr-xr-x 17 root root 4.0K Aug 23 13:52 ..
		-rw-r--r--  1 root root   74 Aug 23 13:51 nautilus.txt.gpg

--------------------------------------------------------------------------------------------------------------------------------
Task 77: 1/Sep/2022

 Kubernetes Sidecar Containers


We have a web server container running the nginx image. The access and error logs generated by the web server are not critical enough to be placed on a persistent volume. However, Nautilus developers need access to the last 24 hours of logs so that they can trace issues and bugs. Therefore, we need to ship the access and error logs for the web server to a log-aggregation service. Following the separation of concerns principle, we implement the Sidecar pattern by deploying a second container that ships the error and access logs from nginx. Nginx does one thing, and it does it well—serving web pages. The second container also specializes in its task—shipping logs. Since containers are running on the same Pod, we can use a shared emptyDir volume to read and write logs.

    Create a pod named webserver.

    Create an emptyDir volume shared-logs.

    Create two containers from nginx and ubuntu images with latest tag only and remember to mention tag i.e nginx:latest, nginx container name should be nginx-container and ubuntu container name should be sidecar-container on webserver pod.

    Add command on sidecar-container "sh","-c","while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log; sleep 30; done"

    Mount the volume shared-logs on both containers at location /var/log/nginx, all containers should be up and running.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.


thor@jump_host ~$ kubectl get services
		NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   117m

thor@jump_host ~$ kubectl get pods
		No resources found in default namespace.

thor@jump_host ~$ vi /tmp/webserver.yaml
thor@jump_host ~$ cat /tmp/webserver.yaml
		apiVersion: v1

		kind: Pod

		metadata:

		  name: webserver

		  labels:

		    name: webserver

		spec:

		  volumes:

		    - name: shared-logs

		      emptyDir: {}

		  containers:

		    - name: nginx-container

		      image: nginx:latest

		      volumeMounts:

		        - name: shared-logs

		          mountPath: /var/log/nginx

		    - name: sidecar-container

		      image: ubuntu:latest

		      command:

		        [

		          "/bin/bash",

		          "-c",

		          "while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log; sleep 30; done",

		        ]

		      volumeMounts:

		        - name: shared-logs

		          mountPath: /var/log/nginx
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl create -f /tmp/webserver.yaml 
		pod/webserver created
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl get pods
		NAME        READY   STATUS              RESTARTS   AGE
		webserver   0/2     ContainerCreating   0          8s
thor@jump_host ~$ kubectl get pods
		NAME        READY   STATUS    RESTARTS   AGE
		webserver   2/2     Running   0          32s
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ 
thor@jump_host ~$ kubectl describe pods webserver
		Name:         webserver
		Namespace:    default
		Priority:     0
		Node:         kodekloud-control-plane/172.17.0.2
		Start Time:   Thu, 01 Sep 2022 11:47:55 +0000
		Labels:       name=webserver
		Annotations:  <none>
		Status:       Running
		IP:           10.244.0.5
		IPs:
		  IP:  10.244.0.5
		Containers:
		  nginx-container:
		    Container ID:   containerd://5085d8516606c1b9dfe2346d7b126a097c2422a5472b53bbb044d275720e39f1
		    Image:          nginx:latest
		    Image ID:       docker.io/library/nginx@sha256:b95a99feebf7797479e0c5eb5ec0bdfa5d9f504bc94da550c2f58e839ea6914f
		    Port:           <none>
		    Host Port:      <none>
		    State:          Running
		      Started:      Thu, 01 Sep 2022 11:48:12 +0000
		    Ready:          True
		    Restart Count:  0
		    Environment:    <none>
		    Mounts:
		      /var/log/nginx from shared-logs (rw)
		      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jc4dn (ro)
		  sidecar-container:
		    Container ID:  containerd://85c6684de2540bc374787b3eb3226b831a44450449a64d37e2896dbe08058f36
		    Image:         ubuntu:latest
		    Image ID:      docker.io/library/ubuntu@sha256:34fea4f31bf187bc915536831fd0afc9d214755bf700b5cdb1336c82516d154e
		    Port:          <none>
		    Host Port:     <none>
		    Command:
		      /bin/bash
		      -c
		      while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log; sleep 30; done
		    State:          Running
		      Started:      Thu, 01 Sep 2022 11:48:21 +0000
		    Ready:          True
		    Restart Count:  0
		    Environment:    <none>
		    Mounts:
		      /var/log/nginx from shared-logs (rw)
		      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jc4dn (ro)
		Conditions:
		  Type              Status
		  Initialized       True 
		  Ready             True 
		  ContainersReady   True 
		  PodScheduled      True 
		Volumes:
		  shared-logs:
		    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
		    Medium:     
		    SizeLimit:  <unset>
		  default-token-jc4dn:
		    Type:        Secret (a volume populated by a Secret)
		    SecretName:  default-token-jc4dn
		    Optional:    false
		QoS Class:       BestEffort
		Node-Selectors:  <none>
		Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
		                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
		Events:
		  Type    Reason     Age   From               Message
		  ----    ------     ----  ----               -------
		  Normal  Scheduled  46s   default-scheduler  Successfully assigned default/webserver to kodekloud-control-plane
		  Normal  Pulling    43s   kubelet            Pulling image "nginx:latest"
		  Normal  Pulled     30s   kubelet            Successfully pulled image "nginx:latest" in 12.903081783s
		  Normal  Created    30s   kubelet            Created container nginx-container
		  Normal  Started    29s   kubelet            Started container nginx-container
		  Normal  Pulling    29s   kubelet            Pulling image "ubuntu:latest"
		  Normal  Pulled     23s   kubelet            Successfully pulled image "ubuntu:latest" in 6.621630852s
		  Normal  Created    22s   kubelet            Created container sidecar-container
		  Normal  Started    20s   kubelet            Started container sidecar-container 

--------------------------------------------------------------------------------------------------------------------------------
Task 78: 9/Sep/2022

 Print Environment Variables

 The Nautilus DevOps team is working on to setup some pre-requisites for an application that will send the greetings to different users. There is a sample deployment, that needs to be tested. Below is a scenario which needs to be configured on Kubernetes cluster. Please find below more details about it.

    Create a pod named print-envars-greeting.

    Configure spec as, the container name should be print-env-container and use bash image.

    Create three environment variables:

a. GREETING and its value should be Welcome to

b. COMPANY and its value should be Nautilus

c. GROUP and its value should be Datacenter

    Use command to echo ["$(GREETING) $(COMPANY) $(GROUP)"] message.

    You can check the output using <kubctl logs -f [ pod-name ]> command.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

1.
thor@jump_host ~$ kubectl get services
		NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   88m

thor@jump_host ~$ kubectl get pods
		No resources found in default namespace.

2.
thor@jump_host ~$ vi /tmp/env.yml

thor@jump_host ~$ cat /tmp/env.yml
		apiVersion: v1

		kind: Pod

		metadata:

		  name: print-envars-greeting

		  labels:

		    name: print-envars-greeting

		spec:

		  containers:

		    - name: print-env-container

		      image: bash

		      env:

		        - name: GREETING

		          value: "Welcome to"

		        - name: COMPANY

		          value: "Nautilus"

		        - name: GROUP

		          value: "Datacenter"

		      command: ["echo"]

		      args: ["$(GREETING) $(COMPANY) $(GROUP)"]


3.
thor@jump_host ~$ kubectl create -f /tmp/env.yml
		pod/print-envars-greeting created

4.
thor@jump_host ~$ kubectl get pods
		NAME                    READY   STATUS      RESTARTS   AGE
		print-envars-greeting   0/1     Completed   1          14s

thor@jump_host ~$ kubectl get pods
		NAME                    READY   STATUS      RESTARTS   AGE
		print-envars-greeting   0/1     Completed   2          30s

5.
thor@jump_host ~$ kubectl logs -f print-envars-greeting
		Welcome to Nautilus Datacenter
thor@jump_host ~$


--------------------------------------------------------------------------------------------------------------------------------
Task 79: 18/Sep/2022

Manage Secrets in Kubernetes


The Nautilus DevOps team is working to deploy some tools in Kubernetes cluster. Some of the tools are licence based so that licence information needs to be stored securely within Kubernetes cluster. Therefore, the team wants to utilize Kubernetes secrets to store those secrets. Below you can find more details about the requirements:

    We already have a secret key file news.txt under /opt location on jump host. Create a generic secret named news, it should contain the password/license-number present in news.txt file.

    Also create a pod named secret-xfusion.

    Configure pod's spec as container name should be secret-container-xfusion, image should be fedora preferably with latest tag (remember to mention the tag with image). Use sleep command for container so that it remains in running state. Consume the created secret and mount it under /opt/cluster within the container.

    To verify you can exec into the container secret-container-xfusion, to check the secret key under the mounted path /opt/cluster. Before hitting the Check button please make sure pod/pods are in running state, also validation can take some time to complete so keep patience.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

1.
thor@jump_host ~$ cat /opt/news.txt 
	5ecur3

2. 
thor@jump_host ~$ kubectl create secret generic news --from-file=/opt/news.txt 
	secret/news created
 
thor@jump_host ~$ ls -ahl /opt/
	total 12K
	drwxr-xr-x 1 thor thor 4.0K Sep 18 17:35 .
	drwxr-xr-x 1 root root 4.0K Sep 18 17:35 ..
	-rw-r--r-- 1 root root    7 Sep 18 17:35 news.txt

3.
thor@jump_host ~$ vi /tmp/secret.yml
 
thor@jump_host ~$ cat /tmp/secret.yml 
		apiVersion: v1

		kind: Pod

		metadata:

		  name: secret-xfusion

		  labels:

		    name: myapp

		spec:

		  volumes:

		    - name: secret-volume-xfusion

		      secret:

		        secretName: news

		  containers:

		    - name: secret-container-xfusion

		      image: fedora:latest

		      command: ["/bin/bash", "-c", "sleep 10000"]

		      volumeMounts:

		        - name: secret-volume-xfusion

		          mountPath: /opt/cluster

		          readOnly: true

4.
thor@jump_host ~$ kubectl create -f /tmp/secret.yml 
	pod/secret-xfusion created

5.
thor@jump_host ~$ kubectl get pods 
	NAME             READY   STATUS              RESTARTS   AGE
	secret-xfusion   0/1     ContainerCreating   0          7s

thor@jump_host ~$ kubectl get pods 
	NAME             READY   STATUS              RESTARTS   AGE
	secret-xfusion   0/1     ContainerCreating   0          16s

thor@jump_host ~$ kubectl get pods 
	NAME             READY   STATUS    RESTARTS   AGE
	secret-xfusion   1/1     Running   0          78s

6.	
thor@jump_host ~$ kubectl exec secret-xfusion -- cat /opt/cluster/news.txt
	5ecur3
thor@jump_host ~$ 


--------------------------------------------------------------------------------------------------------------------------------
Task 80: 20/Sep/2022

Creating Soft Links Using Ansible


The Nautilus DevOps team is practicing some of the Ansible modules and creating and testing different Ansible playbooks to accomplish tasks. Recently they started testing an Ansible file module to create soft links on all app servers. Below you can find more details about it.

Write a playbook.yml under /home/thor/ansible directory on jump host, an inventory file is already present under /home/thor/ansible directory on jump host itself. Using this playbook accomplish below given tasks:

    Create an empty file /opt/security/blog.txt on app server 1; its user owner and group owner should be tony. Create a symbolic link of source path /opt/security to destination /var/www/html.

    Create an empty file /opt/security/story.txt on app server 2; its user owner and group owner should be steve. Create a symbolic link of source path /opt/security to destination /var/www/html.

    Create an empty file /opt/security/media.txt on app server 3; its user owner and group owner should be banner. Create a symbolic link of source path /opt/security to destination /var/www/html.

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml so please make sure playbook works this way without passing any extra arguments.


1. Verify the ansible inventory file

thor@jump_host ~$ cd /home/thor/ansible/

thor@jump_host ~/ansible$ ls -ahl
	total 16K
	drwxr-xr-x 2 thor thor 4.0K Sep 20 14:52 .
	drwxr----- 1 thor thor 4.0K Sep 20 14:52 ..
	-rw-r--r-- 1 thor thor   36 Sep 20 14:52 ansible.cfg
	-rw-r--r-- 1 thor thor  237 Sep 20 14:52 inventory
 
thor@jump_host ~/ansible$ ansible all -a "ls -ahl /opt/security" -i inventory
	stapp01 | CHANGED | rc=0 >>
	total 8.0K
	drwxr-xr-x 2 root root 4.0K Sep 20 14:52 .
	drwxr-xr-x 1 root root 4.0K Sep 20 14:52 ..
	stapp02 | CHANGED | rc=0 >>
	total 8.0K
	drwxr-xr-x 2 root root 4.0K Sep 20 14:52 .
	drwxr-xr-x 1 root root 4.0K Sep 20 14:52 ..
	stapp03 | CHANGED | rc=0 >>
	total 8.0K
	drwxr-xr-x 2 root root 4.0K Sep 20 14:52 .
	drwxr-xr-x 1 root root 4.0K Sep 20 14:52 ..

2. Create the playbook.yml file

thor@jump_host ~/ansible$ pwd
	/home/thor/ansible

thor@jump_host ~/ansible$ vi playbook.yml
 
thor@jump_host ~/ansible$ cat playbook.yml 
	- name: Create text files and create soft link

	  hosts: stapp01, stapp02, stapp03

	  become: yes

	  tasks:

	    - name: Create the blog.txt on stapp01

	      file:

	        path: /opt/security/blog.txt

	        owner: tony

	        group: tony

	        state: touch

	      when: inventory_hostname == "stapp01"

	    - name: Create the story.txt on stapp02

	      file:

	        path: /opt/security/story.txt

	        owner: steve

	        group: steve

	        state: touch

	      when: inventory_hostname == "stapp02"

	    - name: Create the media.txt on stapp03

	      file:

	        path: /opt/security/media.txt

	        owner: banner

	        group: banner

	        state: touch

	      when: inventory_hostname == "stapp03"

	    - name: Link /opt/security directory

	      file:

	        src: /opt/security/

	        dest: /var/www/html

	        state: link

3. Run the playbook	        
thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook.yml 

		PLAY [Create text files and create soft link] ***********************************************************************************************

		TASK [Gathering Facts] **********************************************************************************************************************
		ok: [stapp03]
		ok: [stapp01]
		ok: [stapp02]

		TASK [Create the blog.txt on stapp01] *******************************************************************************************************
		skipping: [stapp02]
		skipping: [stapp03]
		changed: [stapp01]

		TASK [Create the story.txt on stapp02] ******************************************************************************************************
		skipping: [stapp01]
		skipping: [stapp03]
		changed: [stapp02]

		TASK [Create the media.txt on stapp03] ******************************************************************************************************
		skipping: [stapp02]
		skipping: [stapp01]
		changed: [stapp03]

		TASK [Link /opt/security directory] *********************************************************************************************************
		changed: [stapp01]
		changed: [stapp03]
		changed: [stapp02]

		PLAY RECAP **********************************************************************************************************************************
		stapp01                    : ok=3    changed=2    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0   
		stapp02                    : ok=3    changed=2    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0   
		stapp03                    : ok=3    changed=2    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0   

4. Verify the file created and soft link made.

thor@jump_host ~/ansible$ ansible all -a "ls -ahl /opt/security" -i inventory
	stapp01 | CHANGED | rc=0 >>
	total 8.0K
	drwxr-xr-x 2 root root 4.0K Sep 20 14:57 .
	drwxr-xr-x 1 root root 4.0K Sep 20 14:52 ..
	-rw-r--r-- 1 tony tony    0 Sep 20 14:57 blog.txt
	stapp03 | CHANGED | rc=0 >>
	total 8.0K
	drwxr-xr-x 2 root   root   4.0K Sep 20 14:57 .
	drwxr-xr-x 1 root   root   4.0K Sep 20 14:52 ..
	-rw-r--r-- 1 banner banner    0 Sep 20 14:57 media.txt
	stapp02 | CHANGED | rc=0 >>
	total 8.0K
	drwxr-xr-x 2 root  root  4.0K Sep 20 14:57 .
	drwxr-xr-x 1 root  root  4.0K Sep 20 14:52 ..
	-rw-r--r-- 1 steve steve    0 Sep 20 14:57 story.txt

thor@jump_host ~/ansible$ ansible all -a "ls -ahl /var/www/html" -i inventory
	stapp02 | CHANGED | rc=0 >>
	lrwxrwxrwx 1 root root 14 Sep 20 14:57 /var/www/html -> /opt/security/
	stapp01 | CHANGED | rc=0 >>
	lrwxrwxrwx 1 root root 14 Sep 20 14:57 /var/www/html -> /opt/security/
	stapp03 | CHANGED | rc=0 >>
	lrwxrwxrwx 1 root root 14 Sep 20 14:57 /var/www/html -> /opt/security/

thor@jump_host ~/ansible$

--------------------------------------------------------------------------------------------------------------------------------
Task 81: 22/Sep/2022

Setup Puppet Certs Autosign


During last weekly meeting, the Nautilus DevOps team has decided to use Puppet autosign config to auto sign the certificates for all Puppet agent nodes that they will keep adding under the Puppet master in Stratos DC. The Puppet master and CA servers are currently running on jump host and all three app servers are configured as Puppet agents. To set up autosign configuration on the Puppet master server, some configuration settings must be done. Please find below more details:

The Puppet server package is already installed on puppet master i.e jump server and the Puppet agent package is already installed on all App Servers. However, you may need to start the required services on all of these servers.

    Configure autosign configuration on the Puppet master i.e jump server (by creating an autosign.conf in the puppet configuration directory) and assign the certificates for master node as well as for the all agent nodes. Use the respective host's FDQN to assign the certificates.

    Use alias puppet (dns_alt_names) for master node and add its entry in /etc/hosts config file on master i.e Jump Server as well as on the all agent nodes i.e App Servers.

Notes: :- Please make sure to run the puppet agent test using sudo on agent nodes, otherwise you can face certificate issues. In that case you will have to clean the certificates first and then you will be able to run the puppet agent test.

:- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.


1. Switch to root user to avoid reenter password

thor@jump_host ~$ sudo su -
	We trust you have received the usual lecture from the local System
	Administrator. It usually boils down to these three things:
	    #1) Respect the privacy of others.
	    #2) Think before you type.
	    #3) With great power comes great responsibility.
	 [sudo] password for thor:
root@jump_host ~#

2. Check /etc/hosts file and add alias for puppet server to jump_hosts

root@jump_host ~# cat /etc/hosts
		127.0.0.1       localhost
		::1     localhost ip6-localhost ip6-loopback
		fe00::0 ip6-localnet
		ff00::0 ip6-mcastprefix
		ff02::1 ip6-allnodes
		ff02::2 ip6-allrouters
		172.16.238.10   stapp01.stratos.xfusioncorp.com
		172.16.238.11   stapp02.stratos.xfusioncorp.com
		172.16.238.12   stapp03.stratos.xfusioncorp.com
		172.16.238.3    jump_host.stratos.xfusioncorp.com jump_host
		172.16.239.5    jump_host.stratos.xfusioncorp.com jump_host
		172.17.0.4      jump_host.stratos.xfusioncorp.com jump_host


root@jump_host ~# ping puppet
		PING puppet.stratos.xfusioncorp.com (216.245.213.73) 56(84) bytes of data.
		64 bytes from 73-213-245-216.static.reverse.lstn.net (216.245.213.73): icmp_seq=1 ttl=57 time=15.9 ms
		64 bytes from 73-213-245-216.static.reverse.lstn.net (216.245.213.73): icmp_seq=2 ttl=57 time=15.4 ms
		64 bytes from 73-213-245-216.static.reverse.lstn.net (216.245.213.73): icmp_seq=3 ttl=57 time=15.4 ms
		^C
		--- puppet.stratos.xfusioncorp.com ping statistics ---
		10 packets transmitted, 10 received, 0% packet loss, time 9191ms
		rtt min/avg/max/mdev = 15.438/15.530/15.933/0.148 ms

root@jump_host ~# vi /etc/hosts

root@jump_host ~# cat /etc/hosts
		127.0.0.1       localhost
		::1     localhost ip6-localhost ip6-loopback
		fe00::0 ip6-localnet
		ff00::0 ip6-mcastprefix
		ff02::1 ip6-allnodes
		ff02::2 ip6-allrouters
		172.16.238.10   stapp01.stratos.xfusioncorp.com
		172.16.238.11   stapp02.stratos.xfusioncorp.com
		172.16.238.12   stapp03.stratos.xfusioncorp.com
		172.16.238.3    jump_host.stratos.xfusioncorp.com jump_host puppet                 <--------------------------
		172.16.239.5    jump_host.stratos.xfusioncorp.com jump_host
		172.17.0.4      jump_host.stratos.xfusioncorp.com jump_host

root@jump_host ~# ping puppet
		PING jump_host.stratos.xfusioncorp.com (172.16.238.3) 56(84) bytes of data.
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=1 ttl=64 time=0.021 ms
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=2 ttl=64 time=0.043 ms
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=3 ttl=64 time=0.040 ms
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=4 ttl=64 time=0.044 ms
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=5 ttl=64 time=0.048 ms
		^C
		--- jump_host.stratos.xfusioncorp.com ping statistics ---
		5 packets transmitted, 5 received, 0% packet loss, time 4082ms
		rtt min/avg/max/mdev = 0.021/0.039/0.048/0.010 ms


3. Create autosign.conf configuration file and define all App Server FQDN 

root@jump_host ~# vi /etc/puppetlabs/puppet/autosign.conf

root@jump_host ~# cat /etc/puppetlabs/puppet/autosign.conf
		jump_host.stratos.xfusioncorp.com

		stapp01.stratos.xfusioncorp.com

		stapp02.stratos.xfusioncorp.com

		stapp03.stratos.xfusioncorp.com

4. Restart Puppet Server for changes to take into effect

root@jump_host ~# systemctl restart puppetserver

root@jump_host ~# systemctl status  puppetserver
		● puppetserver.service - puppetserver Service
		   Loaded: loaded (/usr/lib/systemd/system/puppetserver.service; disabled; vendor preset: disabled)
		   Active: active (running) since Fri 2022-09-23 03:00:02 UTC; 10s ago
		  Process: 13695 ExecStop=/opt/puppetlabs/server/apps/puppetserver/bin/puppetserver stop (code=exited, status=0/SUCCESS)
		  Process: 13885 ExecStart=/opt/puppetlabs/server/apps/puppetserver/bin/puppetserver start (code=exited, status=0/SUCCESS)
		 Main PID: 13946 (java)
		    Tasks: 88 (limit: 4915)
		   CGroup: /docker/f7fa4e392bae13c82fd14067c17a62828c77c574648a1fefb2cf6cf5a06e2503/system.slice/puppetserver.service
		           └─13946 /usr/bin/java -Xms512m -Xmx512m -Djruby.logger.class=com.puppetlabs.jruby_utils.jruby.Slf4jLogger -XX:OnOutOfMemoryErro...

5. Check for any certificates existing on puppet server

root@jump_host ~# puppetserver ca list --all
		Signed Certificates:
		    964369dd618d.c.argo-prod-us-east1.internal       (SHA256)  8C:19:47:FD:59:97:CE:0C:1D:DF:52:92:A3:BA:41:22:F2:3D:95:D4:38:D9:EF:85:65:9A:7B:B5:59:D5:CA:E6       alt names: ["DNS:puppet", "DNS:964369dd618d.c.argo-prod-us-east1.internal"]     authorization extensions: [pp_cli_auth: true]
		    jump_host.stratos.xfusioncorp.com                (SHA256)  54:42:58:8A:55:21:89:C0:B0:96:E0:6E:DE:DA:55:45:61:0A:D7:70:C1:4F:26:50:5C:61:DD:BB:05:0A:09:EE       alt names: ["DNS:puppet", "DNS:jump_host.stratos.xfusioncorp.com"]      authorization extensions: [pp_cli_auth: true]


6. Login to app server and switch to root  

root@jump_host ~# ssh tony@stapp01
		The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
		ECDSA key fingerprint is SHA256:l8lVvUaAxJzrg3ebcyecrCOol9GJqR/es1wvGjViQpg.
		ECDSA key fingerprint is MD5:c4:ad:6a:3d:5f:49:15:14:c8:81:dc:fc:ba:3e:48:d4.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp01,172.16.238.10' (ECDSA) to the list of known hosts.
		tony@stapp01's password: 

[tony@stapp01 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for tony: 

7. Make changes in /etc/hosts file to include alias for puppet server

[root@stapp01 ~]# vi /etc/hosts
 
[root@stapp01 ~]# cat /etc/hosts
		127.0.0.1       localhost
		::1     localhost ip6-localhost ip6-loopback
		fe00::0 ip6-localnet
		ff00::0 ip6-mcastprefix
		ff02::1 ip6-allnodes
		ff02::2 ip6-allrouters
		172.16.238.3    jump_host.stratos.xfusioncorp.com puppet                                <--------------------------
		172.16.238.10   stapp01.stratos.xfusioncorp.com stapp01
		172.16.239.3    stapp01.stratos.xfusioncorp.com stapp01
		172.17.0.6      stapp01.stratos.xfusioncorp.com stapp01

8. Verify /etc/hosts changes 

[root@stapp01 ~]# ping puppet
		PING jump_host.stratos.xfusioncorp.com (172.16.238.3) 56(84) bytes of data.
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=1 ttl=64 time=0.059 ms
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=2 ttl=64 time=0.071 ms
		64 bytes from jump_host.stratos.xfusioncorp.com (172.16.238.3): icmp_seq=3 ttl=64 time=0.065 ms
		^C
		--- jump_host.stratos.xfusioncorp.com ping statistics ---
		3 packets transmitted, 3 received, 0% packet loss, time 2050ms
		rtt min/avg/max/mdev = 0.059/0.065/0.071/0.005 ms

9. Restart puppet agent on the app server  

[root@stapp01 ~]# systemctl restart puppet

[root@stapp01 ~]# systemctl status puppet
		● puppet.service - Puppet agent
		   Loaded: loaded (/usr/lib/systemd/system/puppet.service; disabled; vendor preset: disabled)
		   Active: active (running) since Fri 2022-09-23 03:03:01 UTC; 10s ago
		 Main PID: 484 (puppet)
		   CGroup: /docker/a6e6ce41b55f652076ebaa21096dd19ff744035edef2d11926e7765ca43bbdf1/system.slice/puppet.service
		           └─484 /opt/puppetlabs/puppet/bin/ruby /opt/puppetlabs/puppet/bin/puppet agent --no-daemonize

		Sep 23 03:03:01 stapp01.stratos.xfusioncorp.com systemd[1]: Converting job puppet.service/restart -> puppet.service/start
		Sep 23 03:03:01 stapp01.stratos.xfusioncorp.com systemd[1]: puppet.service: cgroup is empty
		Sep 23 03:03:01 stapp01.stratos.xfusioncorp.com systemd[1]: About to execute: /opt/puppetlabs/puppet/bin/puppet agent $PUPPET_EXTRA_...monize
		Sep 23 03:03:01 stapp01.stratos.xfusioncorp.com systemd[1]: Forked /opt/puppetlabs/puppet/bin/puppet as 484
		Sep 23 03:03:01 stapp01.stratos.xfusioncorp.com systemd[1]: puppet.service changed dead -> running
		Sep 23 03:03:01 stapp01.stratos.xfusioncorp.com systemd[1]: Job puppet.service/start finished, result=done
		Sep 23 03:03:01 stapp01.stratos.xfusioncorp.com systemd[1]: Started Puppet agent.
		Sep 23 03:03:01 stapp01.stratos.xfusioncorp.com systemd[484]: Executing: /opt/puppetlabs/puppet/bin/puppet agent --no-daemonize
		Sep 23 03:03:04 stapp01.stratos.xfusioncorp.com puppet-agent[484]: Starting Puppet client version 6.15.0
		Sep 23 03:03:08 stapp01.stratos.xfusioncorp.com puppet-agent[506]: Applied catalog in 0.07 seconds
		Hint: Some lines were ellipsized, use -l to show in full.

10. Validate by running the puppet agent

[root@stapp01 ~]# puppet agent -tv 
		Info: Using configured environment 'production'
		Info: Retrieving pluginfacts
		Info: Retrieving plugin
		Info: Retrieving locales
		Info: Caching catalog for stapp01.stratos.xfusioncorp.com
		Info: Applying configuration version '1663902221'
		Notice: Applied catalog in 0.01 seconds

11. List the certificate auto signed on puppet server jump_hosts

root@jump_host ~# puppetserver ca list --all
		Signed Certificates:
		    964369dd618d.c.argo-prod-us-east1.internal       (SHA256)  8C:19:47:FD:59:97:CE:0C:1D:DF:52:92:A3:BA:41:22:F2:3D:95:D4:38:D9:EF:85:65:9A:7B:B5:59:D5:CA:E6       alt names: ["DNS:puppet", "DNS:964369dd618d.c.argo-prod-us-east1.internal"]     authorization extensions: [pp_cli_auth: true]
		    jump_host.stratos.xfusioncorp.com                (SHA256)  54:42:58:8A:55:21:89:C0:B0:96:E0:6E:DE:DA:55:45:61:0A:D7:70:C1:4F:26:50:5C:61:DD:BB:05:0A:09:EE       alt names: ["DNS:puppet", "DNS:jump_host.stratos.xfusioncorp.com"]      authorization extensions: [pp_cli_auth: true]
		    stapp01.stratos.xfusioncorp.com                  (SHA256)  8D:B1:DC:89:72:FC:12:00:C8:9D:D3:1A:9A:A4:76:C9:EA:0E:31:E2:F5:2D:DF:F3:27:D6:32:57:EA:04:CF:34       alt names: ["DNS:stapp01.stratos.xfusioncorp.com"]

12. Repeat steps 6 to 10 for app server 2 and app server 3 

13. List the certificate auto signed on puppet server jump_hosts

root@jump_host ~# puppetserver ca list --all
		Signed Certificates:
		    964369dd618d.c.argo-prod-us-east1.internal       (SHA256)  8C:19:47:FD:59:97:CE:0C:1D:DF:52:92:A3:BA:41:22:F2:3D:95:D4:38:D9:EF:85:65:9A:7B:B5:59:D5:CA:E6       alt names: ["DNS:puppet", "DNS:964369dd618d.c.argo-prod-us-east1.internal"]     authorization extensions: [pp_cli_auth: true]
		    jump_host.stratos.xfusioncorp.com                (SHA256)  54:42:58:8A:55:21:89:C0:B0:96:E0:6E:DE:DA:55:45:61:0A:D7:70:C1:4F:26:50:5C:61:DD:BB:05:0A:09:EE       alt names: ["DNS:puppet", "DNS:jump_host.stratos.xfusioncorp.com"]      authorization extensions: [pp_cli_auth: true]
		    stapp02.stratos.xfusioncorp.com                  (SHA256)  92:66:53:C5:67:D9:AE:18:E9:36:C2:7D:FF:38:FA:BE:60:12:CF:63:42:1C:C6:E8:E1:8C:2E:DC:3C:8F:B8:FC       alt names: ["DNS:stapp02.stratos.xfusioncorp.com"]
		    stapp03.stratos.xfusioncorp.com                  (SHA256)  9B:37:00:61:23:4A:55:7A:68:D1:9E:F6:6D:40:E8:73:18:8E:19:2E:B8:33:6E:3A:D0:BD:75:B0:9B:19:1F:88       alt names: ["DNS:stapp03.stratos.xfusioncorp.com"]
		    stapp01.stratos.xfusioncorp.com                  (SHA256)  8D:B1:DC:89:72:FC:12:00:C8:9D:D3:1A:9A:A4:76:C9:EA:0E:31:E2:F5:2D:DF:F3:27:D6:32:57:EA:04:CF:34       alt names: ["DNS:stapp01.stratos.xfusioncorp.com"]

--------------------------------------------------------------------------------------------------------------------------------
Task 82: 27/Sep/2022

Deploy Jenkins on Kubernetes

The Nautilus DevOps team is planning to set up a Jenkins CI server to create/manage some deployment pipelines for some of the projects. They want to set up the Jenkins server on Kubernetes cluster. Below you can find more details about the task:

1) Create a namespace jenkins

2) Create a Service for jenkins deployment. Service name should be jenkins-service under jenkins namespace, type should be NodePort, nodePort should be 30008

3) Create a Jenkins Deployment under jenkins namespace, It should be name as jenkins-deployment , labels app should be jenkins , container name should be jenkins-container , use jenkins/jenkins image , containerPort should be 8080 and replicas count should be 1.

Make sure to wait for the pods to be in running state and make sure you are able to access the Jenkins login screen in the browser before hitting the Check button.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.


1. Check kubectl utility configuration and working on jump_host
thor@jump_host ~$ kubectl get namespace
		NAME                 STATUS   AGE
		default              Active   4h50m
		kube-node-lease      Active   4h50m
		kube-public          Active   4h50m
		kube-system          Active   4h50m
		local-path-storage   Active   4h49m
 
thor@jump_host ~$ kubectl get service
		NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   4h50m

2. Create namespace  and verify
thor@jump_host ~$ kubectl create namespace jenkins
		namespace/jenkins created

thor@jump_host ~$ kubectl get namespace
		NAME                 STATUS   AGE
		default              Active   4h50m
		jenkins              Active   4s 															<-----------------------
		kube-node-lease      Active   4h50m
		kube-public          Active   4h50m
		kube-system          Active   4h50m
		local-path-storage   Active   4h50m

3. Create jenkins.yaml file as per requirement 
thor@jump_host ~$ vi /tmp/jenkins.yaml

thor@jump_host ~$ cat /tmp/jenkins.yaml
		apiVersion: v1

		kind: Service

		metadata:

		  name: jenkins-service

		  namespace: jenkins

		spec:

		  type: NodePort

		  selector:

		    app: jenkins

		  ports:

		    - port: 8080

		      targetPort: 8080

		      nodePort: 30008

		---

		apiVersion: apps/v1

		kind: Deployment

		metadata:

		  name: jenkins-deployment

		  namespace: jenkins

		  labels:

		    app: jenkins

		spec:

		  replicas: 1

		  selector:

		    matchLabels:

		      app: jenkins

		  template:

		    metadata:

		      labels:

		        app: jenkins

		    spec:

		      containers:

		        - name: jenkins-container

		          image: jenkins/jenkins

		          ports:

		            - containerPort: 8080

4. Create pod
thor@jump_host ~$ kubectl create -f /tmp/jenkins.yaml 
		service/jenkins-service created
		deployment.apps/jenkins-deployment created

5. Wait for deployment and pods  to running status
thor@jump_host ~$ kubectl get deploy -n jenkins 
		NAME                 READY   UP-TO-DATE   AVAILABLE   AGE
		jenkins-deployment   0/1     1            0           23s
 
thor@jump_host ~$ kubectl get deploy -n jenkins 
		NAME                 READY   UP-TO-DATE   AVAILABLE   AGE
		jenkins-deployment   1/1     1            1           39s

thor@jump_host ~$ kubectl get pods -n jenkins 
		NAME                                  READY   STATUS    RESTARTS   AGE
		jenkins-deployment-6b6c78f968-vw882   1/1     Running   0          48s
 
thor@jump_host ~$ kubectl get service -n jenkins 
		NAME              TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
		jenkins-service   NodePort   10.96.50.65   <none>        8080:30008/TCP   58s

6. Validate by executing curl against the pod or Open Port on Host1
thor@jump_host ~$ kubectl exec jenkins-deployment-6b6c78f968-vw882 -n jenkins -- curl http://localhost:8080
		  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
		                                 Dload  Upload   Total   Spent    Left  Speed
		100   541  100   541    0     0   1176      0 --:--:-- --:--:-- --:--:--  1176
		<html><head><meta http-equiv='refresh' content='1;url=/login?from=%2F'/><script>window.location.replace('/login?from=%2F');</script></head><body style='background-color:white; color:white;'>


		Authentication required
		<!--
		-->

		</body></html>                                                                                                                                                                      
thor@jump_host ~$ 

--------------------------------------------------------------------------------------------------------------------------------
Task 83: 29/Sep/2022

 Puppet Setup SSH Keys

 The Puppet master and Puppet agent nodes have been set up by the Nautilus DevOps team to perform some testing. In Stratos DC all app servers have been configured as Puppet agent nodes. They want to setup a password less SSH connection between Puppet master and Puppet agent nodes and this task needs to be done using Puppet itself. Below are details about the task:

Create a Puppet programming file games.pp under /etc/puppetlabs/code/environments/production/manifests directory on the Puppet master node i.e on Jump Server. Define a class ssh_node1 for agent node 1 i.e App Server 1, ssh_node2 for agent node 2 i.e App Server 2, ssh_node3 for agent node3 i.e App Server 3. You will need to generate a new ssh key for thor user on Jump Server, that needs to be added on all App Servers.

Configure a password less SSH connection from puppet master i.e jump host to all App Servers. However, please make sure the key is added to the authorized_keys file of each app's sudo user (i.e tony for App Server 1).

Notes: :- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.


1. Generate ssh key-pair for user thor on jump_host and check public key

thor@jump_host ~$ ssh-keygen
		Generating public/private rsa key pair.
		Enter file in which to save the key (/home/thor/.ssh/id_rsa): 
		Enter passphrase (empty for no passphrase): 
		Enter same passphrase again: 
		Your identification has been saved in /home/thor/.ssh/id_rsa.
		Your public key has been saved in /home/thor/.ssh/id_rsa.pub.
		The key fingerprint is:
		SHA256:zo/J8SxD46ejJSHPlSaDUX5GE69oTZyDGRjyyhbchtI thor@jump_host.stratos.xfusioncorp.com
		The key's randomart image is:
		+---[RSA 2048]----+
		|  . .oo +.       |
		| o =.o * +       |
		|. E = + B .      |
		| o + o * +       |
		|  + o * S        |
		| .   = Oo        |
		|      +o=.       |
		|       +=B.      |
		|      ..=*+      |
		+----[SHA256]-----+

thor@jump_host ~$ cat /home/thor/.ssh/id_rsa.pub 
		ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDRFnX7CXiMsSTQ77+9hZMRGtiiCGygqeS1I/qmmj0N6AoyvJypyJRBiTqGhtxMiTgEAp2Ju0KS/LfW2EKTObLFQ7AH1GGvckp4v0NXoY0TgxINKTPm8sD7/Z9Neiz1i2uOz+GvCbTvssrSyuo9jQxPAxJHowaVP+AIQW9XwrnlB0b4YzVGXp/eCugwGJfqzU4HN/lSR/UMwOiKjHvJf8VvhXTkehqI97wfqptkt7WvQ6crHmgor6SDmfAk7+9fNxHqxgQK9QiTHtnI8kjb4JIbNJvUIegjCx/l5Q2FgU5g/ETylmpPkG1ViTR844ICaVyo2Q3JPYsjGVt/XQzCdjor thor@jump_host.stratos.xfusioncorp.com


2. Create puppet programming file in the mentioned folder
thor@jump_host ~$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for thor: 
 
root@jump_host ~# cd /etc/puppetlabs/code/environments/production/manifests/

root@jump_host /etc/puppetlabs/code/environments/production/manifests# ls -ahl
		total 8.0K
		drwxr-xr-x 1 puppet puppet 4.0K Jul 13  2021 .
		drwxr-xr-x 1 puppet puppet 4.0K Aug  9  2021 ..

root@jump_host /etc/puppetlabs/code/environments/production/manifests# vi games.pp

root@jump_host /etc/puppetlabs/code/environments/production/manifests# cat games.pp

		$public_key =  'AAAAB3NzaC1yc2EAAAADAQABAAABAQDRFnX7CXiMsSTQ77+9hZMRGtiiCGygqeS1I/qmmj0N6AoyvJypyJRBiTqGhtxMiTgEAp2Ju0KS/LfW2EKTObLFQ7AH1GGvckp4v0NXoY0TgxINKTPm8sD7/Z9Neiz1i2uOz+GvCbTvssrSyuo9jQxPAxJHowaVP+AIQW9XwrnlB0b4YzVGXp/eCugwGJfqzU4HN/lSR/UMwOiKjHvJf8VvhXTkehqI97wfqptkt7WvQ6crHmgor6SDmfAk7+9fNxHqxgQK9QiTHtnI8kjb4JIbNJvUIegjCx/l5Q2FgU5g/ETylmpPkG1ViTR844ICaVyo2Q3JPYsjGVt/XQzCdjor'

		class ssh_node1 {

		   ssh_authorized_key { 'tony@stapp01':

		     ensure => present,

		    user   => 'tony',

		     type   => 'ssh-rsa',

		     key    => $public_key,

		   }

		 }

		 class ssh_node2 {

		   ssh_authorized_key { 'steve@stapp02':

		     ensure => present,

		     user   => 'steve',

		     type   => 'ssh-rsa',

		     key    => $public_key,

		   }

		 }

		 class ssh_node3 {

		   ssh_authorized_key { 'banner@stapp03':

		     ensure => present,

		     user   => 'banner',

		     type   => 'ssh-rsa',

		     key    => $public_key,

		   }

		 }

		 node stapp01.stratos.xfusioncorp.com {

		   include ssh_node1

		 }

		 node stapp02.stratos.xfusioncorp.com {

		   include ssh_node2

		 }

		 node stapp03.stratos.xfusioncorp.com {

		   include ssh_node3

		 }

3. Validate the puppet file
root@jump_host /etc/puppetlabs/code/environments/production/manifests# puppet parser validate games.pp
 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# exit
		logout

4. Login to all app server(stapp01,stapp02,stapp03) and switch to root 
thor@jump_host ~$ ssh tony@stapp01
		The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
		ECDSA key fingerprint is SHA256:6lM6dokqSC7w9D0MiUtgwL6o80mug/nLEa/HkCD2yg8.
		ECDSA key fingerprint is MD5:51:e4:3f:d8:50:89:9b:c0:8b:a1:50:33:fc:5d:8d:aa.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp01,172.16.238.10' (ECDSA) to the list of known hosts.
		tony@stapp01's password: 
 
[tony@stapp01 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for tony: 

5. Run the puppet agent to pull the configuration from puppet master server
[root@stapp01 ~]# puppet agent -tv
		Info: Using configured environment 'production'
		Info: Retrieving pluginfacts
		Info: Retrieving plugin
		Info: Retrieving locales
		Info: Caching catalog for stapp01.stratos.xfusioncorp.com
		Info: Applying configuration version '1664451362'
		Notice: /Stage[main]/Ssh_node1/Ssh_authorized_key[tony@stapp01]/ensure: created
		Notice: Applied catalog in 0.21 seconds

[root@stapp01 ~]# exit
		logout
[tony@stapp01 ~]$ exit
		logout
		Connection to stapp01 closed.
 
6. Validate by login on app server without password

thor@jump_host ~$ ssh tony@stapp01
		Last login: Thu Sep 29 11:35:32 2022 from jump_host.stratos.xfusioncorp.com
[tony@stapp01 ~]$ exit
		logout
		Connection to stapp01 closed.

thor@jump_host ~$ ssh steve@stapp02
		Last login: Thu Sep 29 11:36:32 2022 from jump_host.stratos.xfusioncorp.com
[steve@stapp02 ~]$ exit
		logout
		Connection to stapp02 closed.

thor@jump_host ~$ ssh banner@stapp03
		Last login: Thu Sep 29 11:37:17 2022 from jump_host.stratos.xfusioncorp.com
[banner@stapp03 ~]$ exit
		logout
		Connection to stapp03 closed.

thor@jump_host ~$
--------------------------------------------------------------------------------------------------------------------------------
Task 84: 03/Oct/2022

Kubernetes Redis Deployment

The Nautilus application development team observed some performance issues with one of the application that is deployed in Kubernetes cluster. After looking into number of factors, the team has suggested to use some in-memory caching utility for DB service. After number of discussions, they have decided to use Redis. Initially they would like to deploy Redis on kubernetes cluster for testing and later they will move it to production. Please find below more details about the task:

Create a redis deployment with following parameters:

    Create a config map called my-redis-config having maxmemory 2mb in redis-config.

    Name of the deployment should be redis-deployment, it should use redis:alpine image and container name should be redis-container. Also make sure it has only 1 replica.

    The container should request for 1 CPU.

    Mount 2 volumes:

a. An Empty directory volume called data at path /redis-master-data.

b. A configmap volume called redis-config at path /redis-master.

c. The container should expose the port 6379.

    Finally, redis-deployment should be in an up and running state.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.

1. Check existing running pods and services
thor@jump_host ~$ kubectl get all
		NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   68m

2. Create a YAML file as per required configuration 
thor@jump_host ~$ vi /tmp/redis.yml

thor@jump_host ~$ cat /tmp/redis.yml 
		---
		kind: ConfigMap
		apiVersion: v1
		metadata:
		  name: my-redis-config
		data:
		  maxmemory: 2mb
		---
		apiVersion: apps/v1
		kind: Deployment
		metadata:
		  name: redis-deployment
		spec:
		  replicas: 1
		  selector:
		    matchLabels:
		      app: redis
		  template:
		    metadata:
		      labels:
		        app: redis
		    spec:
		      containers:
		        - name: redis-container
		          image: redis:alpine
		          ports:
		            - containerPort: 6379
		          resources:
		            requests:
		              cpu: "1000m"
		          volumeMounts:
		            - mountPath: /redis-master-data
		              name: data
		            - mountPath: /redis-master
		              name: redis-config
		      volumes:
		      - name: data
		        emptyDir: {}
		      - name: redis-config
		        configMap:
		          name: my-redis-config

3. Create redis deployment using the yaml file 
thor@jump_host ~$ kubectl create -f /tmp/redis.yml 
		configmap/my-redis-config created
		deployment.apps/redis-deployment created

4. Wait for pods and deployment to running status
thor@jump_host ~$ kubectl get all
		NAME                                    READY   STATUS    RESTARTS   AGE
		pod/redis-deployment-544d696886-nmqbl   1/1     Running   0          6s

		NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   69m

		NAME                               READY   UP-TO-DATE   AVAILABLE   AGE
		deployment.apps/redis-deployment   1/1     1            1           6s

		NAME                                          DESIRED   CURRENT   READY   AGE
		replicaset.apps/redis-deployment-544d696886   1         1         1       6s

5. Verify configmap
thor@jump_host ~$ kubectl get configmap
		NAME               DATA   AGE
		kube-root-ca.crt   1      69m
		my-redis-config    1      26s

thor@jump_host ~$ kubectl describe configmap my-redis-config
		Name:         my-redis-config
		Namespace:    default
		Labels:       <none>
		Annotations:  <none>

		Data
		====
		maxmemory:
		----
		2mb
		Events:  <none>

--------------------------------------------------------------------------------------------------------------------------------
Task 85: 05/Oct/2022

Puppet Setup NTP Server

While troubleshooting one of the issues on app servers in Stratos Datacenter DevOps team identified the root cause that the time isn't synchronized properly among the all app servers which causes issues sometimes. So team has decided to use a specific time server for all app servers, so that they all remain in sync. This task needs to be done using Puppet so as per details mentioned below please compete the task:

    Create a puppet programming file apps.pp under /etc/puppetlabs/code/environments/production/manifests directory on puppet master node i.e on Jump Server. Within the programming file define a custom class ntpconfig to install and configure ntp server on app server 1.

    Add NTP Server server 1.africa.pool.ntp.org in default configuration file on app server 1, also remember to use iburst option for faster synchronization at startup.

    Please note that do not try to start/restart/stop ntp service, as we already have a scheduled restart for this service tonight and we don't want these changes to be applied right now.

Notes: :- Please make sure to run the puppet agent test using sudo on agent nodes, otherwise you can face certificate issues. In that case you will have to clean the certificates first and then you will be able to run the puppet agent test.

:- Before clicking on the Check button please make sure to verify puppet server and puppet agent services are up and running on the respective servers, also please make sure to run puppet agent test to apply/test the changes manually first.

:- Please note that once lab is loaded, the puppet server service should start automatically on puppet master server, however it can take upto 2-3 minutes to start.



1. Switch to root , list all puppet module and if not found install ntpd module
thor@jump_host ~$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for thor: 
 
root@jump_host ~# puppet module list
		/etc/puppetlabs/code/environments/production/modules (no modules installed)
		/etc/puppetlabs/code/modules (no modules installed)
		/opt/puppetlabs/puppet/modules (no modules installed)

root@jump_host ~# puppet module install puppetlabs-ntp
		Notice: Preparing to install into /etc/puppetlabs/code/environments/production/modules ...
		Notice: Downloading from https://forgeapi.puppet.com ...
		Notice: Installing -- do not interrupt ...
		/etc/puppetlabs/code/environments/production/modules
		└─┬ puppetlabs-ntp (v9.2.0)
		  └── puppetlabs-stdlib (v8.4.0)

root@jump_host ~# puppet module list
		/etc/puppetlabs/code/environments/production/modules
		├── puppetlabs-ntp (v9.2.0)
		└── puppetlabs-stdlib (v8.4.0)
		/etc/puppetlabs/code/modules (no modules installed)
		/opt/puppetlabs/puppet/modules (no modules installed)

2. Create the puppet programming file as per requirement
root@jump_host ~# cd /etc/puppetlabs/code/environments/production/manifests

root@jump_host /etc/puppetlabs/code/environments/production/manifests# ls -ahl
		total 8.0K
		drwxr-xr-x 1 puppet puppet 4.0K Jul 13  2021 .
		drwxr-xr-x 1 puppet puppet 4.0K Aug  9  2021 ..

root@jump_host /etc/puppetlabs/code/environments/production/manifests# vi apps.pp

root@jump_host /etc/puppetlabs/code/environments/production/manifests# cat apps.pp
		class { 'ntp':

		  servers => [ 'server 1.africa.pool.ntp.org iburst' ],                                               

		}    

		class ntpconfig {

		  include ntp

		}  

		node 'stapp01.stratos.xfusioncorp.com' {

		  include ntpconfig

		}

		node 'stapp02.stratos.xfusioncorp.com' {

		  include ntpconfig

		}

		node 'stapp03.stratos.xfusioncorp.com' {

		  include ntpconfig

		}

3. Validate the puppet file		
root@jump_host /etc/puppetlabs/code/environments/production/manifests# puppet parser validate apps.pp


4. Login to app servers 1,2 and 3 and switch to root users 
root@jump_host /etc/puppetlabs/code/environments/production/manifests# ssh tony@stapp01
		The authenticity of host 'stapp01 (172.16.238.10)' can't be established.
		ECDSA key fingerprint is SHA256:6/+jfJXDk6LzuxE5Dp95v1rwgND7rHqTZfBXDoY61WE.
		ECDSA key fingerprint is MD5:d8:fb:8c:3b:92:f0:5a:82:bf:6f:d8:ab:6a:c6:a4:e4.
		Are you sure you want to continue connecting (yes/no)? yes
		Warning: Permanently added 'stapp01,172.16.238.10' (ECDSA) to the list of known hosts.
		tony@stapp01's password: 

[tony@stapp01 ~]$ sudo su -

		We trust you have received the usual lecture from the local System
		Administrator. It usually boils down to these three things:

		    #1) Respect the privacy of others.
		    #2) Think before you type.
		    #3) With great power comes great responsibility.

		[sudo] password for tony:

5. Check ntpd service running 		 
[root@stapp01 ~]# puppet resource service ntpd
		service { 'ntpd':
		  ensure   => 'stopped',
		  enable   => 'false',
		  provider => 'systemd',
		}
 
6. Run Puppet agent to pull the configuration from puppet server  
[root@stapp01 ~]# puppet agent -tv
		Info: Using configured environment 'production'
		Info: Retrieving pluginfacts
		Info: Retrieving plugin
		Info: Retrieving locales
		Info: Loading facts
		Info: Caching catalog for stapp01.stratos.xfusioncorp.com
		Info: Applying configuration version '1664992996'
		Notice: /Stage[main]/Ntp::Install/Package[ntp]/ensure: created
		Notice: /Stage[main]/Ntp::Config/File[/etc/ntp.conf]/content: 
		--- /etc/ntp.conf       2019-11-27 16:47:41.000000000 +0000
		+++ /tmp/puppet-file20221005-572-vd56vb 2022-10-05 18:03:52.763950667 +0000
		@@ -1,58 +1,30 @@
		-# For more information about this file, see the man pages
		-# ntp.conf(5), ntp_acc(5), ntp_auth(5), ntp_clock(5), ntp_misc(5), ntp_mon(5).
		+# ntp.conf: Managed by puppet.
		+#
		+# Enable next tinker options:
		+# panic - keep ntpd from panicking in the event of a large clock skew
		+# when a VM guest is suspended and resumed;
		+# stepout - allow ntpd change offset faster
		+tinker panic 0
		+disable monitor
		 
		-driftfile /var/lib/ntp/drift
		+statsdir /var/log/ntpstats
		 
		 # Permit time synchronization with our time source, but do not
		 # permit the source to query or modify the service on this system.
		-restrict default nomodify notrap nopeer noquery
		-
		-# Permit all access over the loopback interface.  This could
		-# be tightened as well, but to do so would effect some of
		-# the administrative functions.
		-restrict 127.0.0.1 
		-restrict ::1
		-
		-# Hosts on local network are less restricted.
		-#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap
		-
		-# Use public servers from the pool.ntp.org project.
		-# Please consider joining the pool (http://www.pool.ntp.org/join.html).
		-server 0.centos.pool.ntp.org iburst
		-server 1.centos.pool.ntp.org iburst
		-server 2.centos.pool.ntp.org iburst
		-server 3.centos.pool.ntp.org iburst
		-
		-#broadcast 192.168.1.255 autokey       # broadcast server
		-#broadcastclient                       # broadcast client
		-#broadcast 224.0.1.1 autokey           # multicast server
		-#multicastclient 224.0.1.1             # multicast client
		-#manycastserver 239.255.254.254                # manycast server
		-#manycastclient 239.255.254.254 autokey # manycast client
		-
		-# Enable public key cryptography.
		-#crypto
		-
		-includefile /etc/ntp/crypto/pw
		+restrict default kod nomodify notrap nopeer noquery
		+restrict -6 default kod nomodify notrap nopeer noquery
		+restrict 127.0.0.1
		+restrict -6 ::1
		+
		+# Set up servers for ntpd with next options:
		+# server - IP address or DNS name of upstream NTP server
		+# burst - send a burst of eight packets instead of the usual one.
		+# iburst - allow send sync packages faster if upstream unavailable
		+# prefer - select preferrable server
		+# minpoll - set minimal update frequency
		+# maxpoll - set maximal update frequency
		+# noselect - do not sync with this server
		+server server 1.africa.pool.ntp.org iburst
		 
		-# Key file containing the keys and key identifiers used when operating
		-# with symmetric key cryptography. 
		-keys /etc/ntp/keys
		-
		-# Specify the key identifiers which are trusted.
		-#trustedkey 4 8 42
		-
		-# Specify the key identifier to use with the ntpdc utility.
		-#requestkey 8
		-
		-# Specify the key identifier to use with the ntpq utility.
		-#controlkey 8
		-
		-# Enable writing of statistics records.
		-#statistics clockstats cryptostats loopstats peerstats
		-
		-# Disable the monitoring facility to prevent amplification attacks using ntpdc
		-# monlist command when default restrict does not include the noquery flag. See
		-# CVE-2013-5211 for more details.
		-# Note: Monitoring will not be disabled with the limited restriction flag.
		-disable monitor
		+# Driftfile.
		+driftfile /var/lib/ntp/drift

		Info: Computing checksum on file /etc/ntp.conf
		Info: /Stage[main]/Ntp::Config/File[/etc/ntp.conf]: Filebucketed /etc/ntp.conf to puppet with sum dc9e5754ad2bb6f6c32b954c04431d0a
		Notice: /Stage[main]/Ntp::Config/File[/etc/ntp.conf]/content: content changed '{md5}dc9e5754ad2bb6f6c32b954c04431d0a' to '{md5}3ead0e8724aba2f2dccd54df6fb0bed6'
		Notice: /Stage[main]/Ntp::Config/File[/etc/ntp/step-tickers]/content: 
		--- /etc/ntp/step-tickers       2019-11-27 16:47:41.000000000 +0000
		+++ /tmp/puppet-file20221005-572-1oe06ym        2022-10-05 18:03:52.859957523 +0000
		@@ -1,3 +1,3 @@
		 # List of NTP servers used by the ntpdate service.
		 
		-0.centos.pool.ntp.org
		+server 1.africa.pool.ntp.org iburst																																				<-----------------------------

		Info: Computing checksum on file /etc/ntp/step-tickers
		Info: /Stage[main]/Ntp::Config/File[/etc/ntp/step-tickers]: Filebucketed /etc/ntp/step-tickers to puppet with sum 9b77b3b3eb41daf0b9abb8ed01c5499b
		Notice: /Stage[main]/Ntp::Config/File[/etc/ntp/step-tickers]/content: content changed '{md5}9b77b3b3eb41daf0b9abb8ed01c5499b' to '{md5}f4c913e757420b9f50c2fbd38715cbc5'
		Info: Class[Ntp::Config]: Scheduling refresh of Class[Ntp::Service]
		Info: Class[Ntp::Service]: Scheduling refresh of Service[ntp]
		Notice: /Stage[main]/Ntp::Service/Service[ntp]/ensure: ensure changed 'stopped' to 'running'								<---------------------------------
		Info: /Stage[main]/Ntp::Service/Service[ntp]: Unscheduling refresh on Service[ntp]
		Notice: Applied catalog in 19.25 seconds

7. Validate the task by checking resource service ntpd
[root@stapp01 ~]# puppet resource service ntpd
		service { 'ntpd':
		  ensure   => 'running',
		  enable   => 'true',
		  provider => 'systemd',
		}

[root@stapp01 ~]# exit
		logout

[tony@stapp01 ~]$ exit
		logout
		Connection to stapp01 closed.

8. Repeat the tasks for App server 2 and app server 3.		

--------------------------------------------------------------------------------------------------------------------------------
Task 86: 07/Oct/2022

Using Ansible Conditionals


The Nautilus DevOps team had a discussion about, how they can train different team members to use Ansible for different automation tasks. There are numerous ways to perform a particular task using Ansible, but we want to utilize each aspect that Ansible offers. The team wants to utilise Ansible's conditionals to perform the following task:

An inventory file is already placed under /home/thor/ansible directory on jump host, with all the Stratos DC app servers included.

Create a playbook /home/thor/ansible/playbook.yml and make sure to use Ansible's when conditionals statements to perform the below given tasks.

    Copy blog.txt file present under /usr/src/devops directory on jump host to App Server 1 under /opt/devops directory. Its user and group owner must be user tony and its permissions must be 0777 .

    Copy story.txt file present under /usr/src/devops directory on jump host to App Server 2 under /opt/devops directory. Its user and group owner must be user steve and its permissions must be 0777 .

    Copy media.txt file present under /usr/src/devops directory on jump host to App Server 3 under /opt/devops directory. Its user and group owner must be user banner and its permissions must be 0777 .

NOTE: You can use ansible_nodename variable from gathered facts with when condition. Additionally, please make sure you are running the play for all hosts i.e use - hosts: all.

Note: Validation will try to run the playbook using command ansible-playbook -i inventory playbook.yml, so please make sure the playbook works this way without passing any extra arguments.

1. Check the inventory file and run ansible to verify.
thor@jump_host ~$ cd /home/thor/ansible/

thor@jump_host ~/ansible$ ls -ahl
		total 16K
		drwxr-xr-x 2 thor thor 4.0K Oct  7 10:25 .
		drwxr----- 1 thor thor 4.0K Oct  7 10:25 ..
		-rw-r--r-- 1 thor thor   36 Oct  7 10:25 ansible.cfg
		-rw-r--r-- 1 thor thor  237 Oct  7 10:25 inventory

thor@jump_host ~/ansible$ cat inventory 
		stapp01 ansible_host=172.16.238.10 ansible_ssh_pass=Ir0nM@n ansible_user=tony
		stapp02 ansible_host=172.16.238.11 ansible_ssh_pass=Am3ric@ ansible_user=steve
		stapp03 ansible_host=172.16.238.12 ansible_ssh_pass=BigGr33n ansible_user=banner

thor@jump_host ~/ansible$ ansible -i inventory all -a "ls -ahl /opt/devops"
		stapp03 | CHANGED | rc=0 >>
		total 8.0K
		drwxr-xr-x 2 root root 4.0K Oct  7 10:25 .
		drwxr-xr-x 1 root root 4.0K Oct  7 10:25 ..
		stapp01 | CHANGED | rc=0 >>
		total 8.0K
		drwxr-xr-x 2 root root 4.0K Oct  7 10:25 .
		drwxr-xr-x 1 root root 4.0K Oct  7 10:25 ..
		stapp02 | CHANGED | rc=0 >>
		total 8.0K
		drwxr-xr-x 2 root root 4.0K Oct  7 10:25 .
		drwxr-xr-x 1 root root 4.0K Oct  7 10:25 ..

2. Verify source files
thor@jump_host ~/ansible$ ls -ahl /usr/src/devops
		total 20K
		drwxr-xr-x 2 root root 4.0K Oct  7 10:26 .
		drwxr-xr-x 1 root root 4.0K Oct  7 10:25 ..
		-rw-r--r-- 1 root root   35 Oct  7 10:25 blog.txt
		-rw-r--r-- 1 root root   22 Oct  7 10:25 media.txt
		-rw-r--r-- 1 root root   27 Oct  7 10:25 story.txt

3. Create playbook as per given requirement
thor@jump_host ~/ansible$ pwd
		/home/thor/ansible

thor@jump_host ~/ansible$ vi playbook.yml
 
thor@jump_host ~/ansible$ cat playbook.yml 
		- name: Copy text files to Appservers

		  hosts: all

		  become: yes

		  tasks:

		    - name: Copy blog.txt to stapp01

		      ansible.builtin.copy:

		        src: /usr/src/devops/blog.txt

		        dest: /opt/devops/

		        owner: tony

		        group: tony

		        mode: "0777"

		      when: inventory_hostname == "stapp01"

		    - name: Copy story.txt to stapp02

		      ansible.builtin.copy:

		        src: /usr/src/devops/story.txt

		        dest: /opt/devops/

		        owner: steve

		        group: steve

		        mode: "0777"

		      when: inventory_hostname == "stapp02"

		    - name: Copy media.txt to stapp03

		      ansible.builtin.copy:

		        src: /usr/src/devops/media.txt

		        dest: /opt/devops/

		        owner: banner

		        group: banner

		        mode: "0777"

		      when: inventory_hostname == "stapp03"

4. Run/execute the playbook
thor@jump_host ~/ansible$ ansible-playbook -i inventory playbook.yml 

		PLAY [Copy text files to Appservers] ********************************************************************************************************

		TASK [Gathering Facts] **********************************************************************************************************************
		ok: [stapp03]
		ok: [stapp02]
		ok: [stapp01]

		TASK [Copy blog.txt to stapp01] *************************************************************************************************************
		skipping: [stapp02]
		skipping: [stapp03]
		changed: [stapp01]

		TASK [Copy story.txt to stapp02] ************************************************************************************************************
		skipping: [stapp03]
		skipping: [stapp01]
		changed: [stapp02]

		TASK [Copy media.txt to stapp03] ************************************************************************************************************
		skipping: [stapp01]
		skipping: [stapp02]
		changed: [stapp03]

		PLAY RECAP **********************************************************************************************************************************
		stapp01                    : ok=2    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0   
		stapp02                    : ok=2    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0   
		stapp03                    : ok=2    changed=1    unreachable=0    failed=0    skipped=2    rescued=0    ignored=0   

5. Verify the changes on App servers
thor@jump_host ~/ansible$ ansible -i inventory all -a "ls -ahl /opt/devops"
		stapp03 | CHANGED | rc=0 >>
		total 12K
		drwxr-xr-x 2 root   root   4.0K Oct  7 10:31 .
		drwxr-xr-x 1 root   root   4.0K Oct  7 10:25 ..
		-rwxrwxrwx 1 banner banner   22 Oct  7 10:31 media.txt
		stapp02 | CHANGED | rc=0 >>
		total 12K
		drwxr-xr-x 2 root  root  4.0K Oct  7 10:31 .
		drwxr-xr-x 1 root  root  4.0K Oct  7 10:25 ..
		-rwxrwxrwx 1 steve steve   27 Oct  7 10:31 story.txt
		stapp01 | CHANGED | rc=0 >>
		total 12K
		drwxr-xr-x 2 root root 4.0K Oct  7 10:31 .
		drwxr-xr-x 1 root root 4.0K Oct  7 10:25 ..
		-rwxrwxrwx 1 tony tony   35 Oct  7 10:31 blog.txt

--------------------------------------------------------------------------------------------------------------------------------
Task 87: 08/Oct/2022

 Deploy Node App on Kubernetes

The Nautilus development team has completed development of one of the node applications, which they are planning to deploy on a Kubernetes cluster. They recently had a meeting with the DevOps team to share their requirements. Based on that, the DevOps team has listed out the exact requirements to deploy the app. Find below more details:

    Create a deployment using gcr.io/kodekloud/centos-ssh-enabled:node image, replica count must be 2.

    Create a service to expose this app, the service type must be NodePort, targetPort must be 8080 and nodePort should be 30012.

    Make sure all the pods are in Running state after the deployment.

    You can check the application by clicking on NodeApp button on top bar.

You can use any labels as per your choice.

Note: The kubectl on jump_host has been configured to work with the kubernetes cluster.


1. Check kubectl utility configuration and working on jump_host
thor@jump_host ~$ kubectl get all
		NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
		service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   51m

thor@jump_host ~$ kubectl get namespace
		NAME                 STATUS   AGE
		default              Active   55m
		kube-node-lease      Active   55m
		kube-public          Active   55m
		kube-system          Active   55m
		local-path-storage   Active   54m
 
thor@jump_host ~$ kubectl get pods
		No resources found in default namespace.

2. Create /tmp/node.yaml file as per requirement
thor@jump_host ~$ vi /tmp/node.yaml
 
thor@jump_host ~$ cat /tmp/node.yaml 
		apiVersion: v1
		kind: Service
		metadata:
		  name: node-service
		spec:
		  type: NodePort
		  selector:
		    app: node-app
		  ports:
		    - port: 80
		      targetPort: 8080
		      nodePort: 30012
		---
		apiVersion: apps/v1
		kind: Deployment
		metadata:
		  name: node-deployment
		spec:
		  replicas: 2
		  selector:
		    matchLabels:
		      app: node-app
		  template:
		    metadata:
		      labels:
		        app: node-app
		    spec:
		      containers:
		        - name: node-container-datacenter
		          image: gcr.io/kodekloud/centos-ssh-enabled:node
		          ports:
		            - containerPort: 80

3. Create deployment and service.
thor@jump_host ~$ kubectl create -f /tmp/node.yaml 
		service/node-service created
		deployment.apps/node-deployment created

4. Verify deployment and service creation, Wait for deployment and pods  to running status
thor@jump_host ~$ kubectl get all
		NAME                                   READY   STATUS              RESTARTS   AGE
		pod/node-deployment-5ffff8bcb4-ns2bc   0/1     ContainerCreating   0          28s
		pod/node-deployment-5ffff8bcb4-vhp7z   0/1     ContainerCreating   0          28s

		NAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
		service/kubernetes     ClusterIP   10.96.0.1      <none>        443/TCP        61m
		service/node-service   NodePort    10.96.18.249   <none>        80:30012/TCP   28s

		NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
		deployment.apps/node-deployment   0/2     2            0           28s

		NAME                                         DESIRED   CURRENT   READY   AGE
		replicaset.apps/node-deployment-5ffff8bcb4   2         2         0       28s

thor@jump_host ~$ kubectl get deploy
		NAME              READY   UP-TO-DATE   AVAILABLE   AGE
		node-deployment   0/2     2            0           52s

thor@jump_host ~$ kubectl get deploy
		NAME              READY   UP-TO-DATE   AVAILABLE   AGE
		node-deployment   0/2     2            0           59s

thor@jump_host ~$ kubectl get pods
		NAME                               READY   STATUS    RESTARTS   AGE
		node-deployment-5ffff8bcb4-ns2bc   1/1     Running   0          65s
		node-deployment-5ffff8bcb4-vhp7z   1/1     Running   0          65s

thor@jump_host ~$ kubectl get deploy
		NAME              READY   UP-TO-DATE   AVAILABLE   AGE
		node-deployment   2/2     2            2           71s

--------------------------------------------------------------------------------------------------------------------------------
Task 88 Onwards : Kodekloud Cheatsheat SrDevOps Task Commands.txt

---------------------------------------------------------------------------------------------------------------------------------